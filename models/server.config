{
    "host": "0.0.0.0",
    "port": 8000,
    "models": [
        {
            "model": "/var/model/qwen2_500m/qwen2-0_5b-instruct-q5_k_m.gguf",
            "model_alias": "Qwen2-0.5b-instruct",
            "chat_format": "chatml",
            "n_gpu_layers": -1,
            "offload_kqv": true,
            "n_threads": 12,
            "n_batch": 512,
            "n_ctx": 2048
        },
#     n_gpu_layers=n_gpu_layers,
#     n_batch=n_batch,
#     n_ctx=2048,
#     f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls
#     callback_manager=callback_manager,
#     verbose=False,        
        {
            "model": "/var/model/llama3_8b/Meta-Llama-3-8B-Instruct.Q5_0.gguf",
            "model_alias": "Llama-3-8b-Instruct",
            "chat_format": "chatml",
            "n_gpu_layers": -1,
            "offload_kqv": true,
            "n_threads": 12,
            "n_batch": 512,
            "n_ctx": 2048
        },
        {
            "model": "/var/model/llama3_8b/ggml-model-Q4_K_M.gguf",
            "model_alias": "llama3_8b_ggml",
            "chat_format": "chatml",
            "n_gpu_layers": -1,
            "offload_kqv": true,
            "n_threads": 12,
            "n_batch": 512,
            "n_ctx": 2048,
            "f16_kv":True,
            "callback_manager":callback_manager,
            "verbose":False
        },
        {
            "model": "/var/model/llama-2-7b-chat/llama-2-7b-chat.ggmlv3.q4_0.bin",
            "model_alias": "llama-2-7b-chat",
            "chat_format": "chatml",
            "n_gpu_layers": -1,
            "offload_kqv": true,
            "n_threads": 12,
            "n_batch": 512,
            "n_ctx": 2048
            "f16_kv":True,
            "callback_manager":callback_manager,
            "verbose":False
        },
        {
            "model": "/var/model/em_nomic_ai/nomic-embed-text-v1.5.Q5_K_M.gguf",
            "model_alias": "nomic_embedding",
            "embedding":true
        }
   ]
}