{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain dependencies\n",
    "import streamlit as st\n",
    "import os\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader # Importing PDF loader from Langchain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # Importing text splitter from Langchain\n",
    "from langchain.embeddings import OpenAIEmbeddings # Importing OpenAI embeddings from Langchain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document # Importing Document schema from Langchain\n",
    "from langchain.vectorstores.chroma import Chroma # Importing Chroma vector store from Langchain\n",
    "from dotenv import load_dotenv # Importing dotenv to get API key from .env file\n",
    "from langchain.chat_models import ChatOpenAI # Import OpenAI LLM\n",
    "import os # Importing os module for operating system functionalities\n",
    "import shutil # Importing shutil module for high-level file operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Page Configuration for Streamlit\n",
    "st.set_page_config(\n",
    "    page_title = \"Local RAG llm\",\n",
    "    layout='wide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Traditional System of Medicine  \n",
      "  \n",
      "Pharmacognosy has been basically evolved as an applied science \n",
      "pertaining to the study of all types of drugs of natural origin. However, its \n",
      "subject matter is directed towards the modern allopathic medicine. During \n",
      "the course of developments, many civiliza tions have raised and perished \n",
      "but the systems of medicines developed by them in various parts of the \n",
      "world are still practised, and are also popular as the alternative systems of \n",
      "medicine. These are the alternative systems in the sense that modern \n",
      "allopat hic system has been globally acclaimed as the principal system of \n",
      "medicine, and so all the other systems prevalent and practised in various \n",
      "parts of the world are supposed to be alternative systems. The philosophy \n",
      "and the basic principles of these so calle d alternative systems might differ \n",
      "significantly from each other, but the fact cannot be denied that these \n",
      "systems have served the humanity for the treatment and management of \n",
      "diseases and also for maintenance of good health. About 80 percent of the \n",
      "world population still rely and use the medicines of these traditional \n",
      "systems.  \n",
      "  \n",
      "Traditional Chinese medicine in China, Unani  system in Greece, Ayurvedic  \n",
      "system in India, Amachi  in Tibet or more recently Homoeopathy  in Germany \n",
      "are these systems of medicine whic h were once practised only in the \n",
      "respective areas or subcontinents of the world, are now popularly practised \n",
      "all over the world. The World Health Organization (WHO) is already taking \n",
      "much interest in indigenous systems of medicine and coming forward to ' metadata={'source': '../data/Traditional_System_of_Medicine.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "# Directory to your pdf files:\n",
    "DATA_PATH = \"../data\"\n",
    "def load_documents():\n",
    "  \"\"\"\n",
    "  Load PDF documents from the specified directory using PyPDFDirectoryLoader.\n",
    "  Returns:\n",
    "  List of Document objects: Loaded PDF documents represented as Langchain\n",
    "                                                          Document objects.\n",
    "  \"\"\"\n",
    "  # Initialize PDF loader with specified directory\n",
    "  document_loader = PyPDFDirectoryLoader(DATA_PATH) \n",
    "  # Load PDF documents and return them as a list of Document objects\n",
    "  return document_loader.load() \n",
    "\n",
    "documents = load_documents() # Call the function\n",
    "# Inspect the contents of the first document as well as metadata\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(documents: list[Document]):\n",
    "  \"\"\"\n",
    "  Split the text content of the given list of Document objects into smaller chunks.\n",
    "  Args:\n",
    "    documents (list[Document]): List of Document objects containing text content to split.\n",
    "  Returns:\n",
    "    list[Document]: List of Document objects representing the split text chunks.\n",
    "  \"\"\"\n",
    "  # Initialize text splitter with specified parameters\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300, # Size of each chunk in characters\n",
    "    chunk_overlap=100, # Overlap between consecutive chunks\n",
    "    length_function=len, # Function to compute the length of the text\n",
    "    add_start_index=True, # Flag to add start index to each chunk\n",
    "  )\n",
    "\n",
    "  # Split documents into smaller chunks using text splitter\n",
    "  chunks = text_splitter.split_documents(documents)\n",
    "  print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "  # Print example of page content and metadata for a chunk\n",
    "  document = chunks[0]\n",
    "  print(document.page_content)\n",
    "  print(document.metadata)\n",
    "\n",
    "  return chunks # Return the list of split text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# service_context=ServiceContext.from_defaults(llm=OpenAI(temperature=0, model=\"gpt-3.5-turbo\", api_key=\"sk-proj-O7P7AxMfq3M13eMHz5ZxT3BlbkFJzngJzu9d4UFdBJlMjp2x\"))\n",
    "# print(service_context.llm.complete(\"Hello!\"))\n",
    "\n",
    "\n",
    "# Path to the directory to save Chroma database\n",
    "CHROMA_PATH = \"chroma\"\n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "  \"\"\"\n",
    "  Save the given list of Document objects to a Chroma database.\n",
    "  Args:\n",
    "  chunks (list[Document]): List of Document objects representing text chunks to save.\n",
    "  Returns:\n",
    "  None\n",
    "  \"\"\"\n",
    "\n",
    "  # Clear out the existing database directory if it exists\n",
    "  if os.path.exists(CHROMA_PATH):\n",
    "    shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "  # Create a new Chroma database from the documents using OpenAI embeddings\n",
    "  db = Chroma.from_documents(\n",
    "    chunks,\n",
    "    OpenAIEmbeddings(),\n",
    "    persist_directory=CHROMA_PATH\n",
    "  )\n",
    "\n",
    "  # Persist the database to disk\n",
    "  db.persist()\n",
    "  print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 42 documents into 544 chunks.\n",
      "Traditional System of Medicine  \n",
      "  \n",
      "Pharmacognosy has been basically evolved as an applied science \n",
      "pertaining to the study of all types of drugs of natural origin. However, its \n",
      "subject matter is directed towards the modern allopathic medicine. During\n",
      "{'source': '../data/Traditional_System_of_Medicine.pdf', 'page': 0, 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "def generate_data_store():\n",
    "  \"\"\"\n",
    "  Function to generate vector database in chroma from documents.\n",
    "  \"\"\"\n",
    "  documents = load_documents() # Load documents from a source\n",
    "  chunks = split_text(documents) # Split documents into manageable chunks\n",
    "  #save_to_chroma(chunks) # Save the processed data to a data store\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "# Generate the data store\n",
    "generate_data_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    " - -\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find matching results.\n",
      "Without additional context, it is difficult to provide a specific answer to this question. However, some potential negative side effects of traditional medicine could include adverse reactions to herbal ingredients, lack of regulation leading to inconsistent quality and potency of products, and potential interactions with other medications.\n"
     ]
    }
   ],
   "source": [
    "def query_rag(query_text):\n",
    "  \"\"\"\n",
    "  Query a Retrieval-Augmented Generation (RAG) system using Chroma database and OpenAI.\n",
    "  Args:\n",
    "    - query_text (str): The text to query the RAG system with.\n",
    "  Returns:\n",
    "    - formatted_response (str): Formatted response including the generated text and sources.\n",
    "    - response_text (str): The generated response text.\n",
    "  \"\"\"\n",
    "  # YOU MUST - Use same embedding function as before\n",
    "  embedding_function = OpenAIEmbeddings()\n",
    "\n",
    "  # Prepare the database\n",
    "  db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "  \n",
    "  # Retrieving the context from the DB using similarity search\n",
    "  results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "\n",
    "  # Check if there are any matching results or if the relevance score is too low\n",
    "  if len(results) == 0 or results[0][1] < 0.7:\n",
    "    print(f\"Unable to find matching results.\")\n",
    "\n",
    "  # Combine context from matching documents\n",
    "  context_text = \"\\n\\n - -\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    " \n",
    "  # Create prompt template using context and query text\n",
    "  prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "  prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "  \n",
    "  # Initialize OpenAI chat model\n",
    "  model = ChatOpenAI()\n",
    "\n",
    "  # Generate response text based on the prompt\n",
    "  response_text = model.predict(prompt)\n",
    " \n",
    "   # Get sources of the matching documents\n",
    "  sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    " \n",
    "  # Format and return response including generated text and sources\n",
    "  formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "  return formatted_response, response_text\n",
    "\n",
    "# Let's call our function we have defined\n",
    "formatted_response, response_text = query_rag(query_text)\n",
    "# and finally, inspect our final response!\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The RAG is working"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
