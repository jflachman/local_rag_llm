# prototype work

## Prototypes

0. [Setup the environment in WSL ubuntu](0-build-env/README.md)
1. [RAG-LLM workflow](1-rag-llm-workflow/README.md)


## LLM Server Approach (examples)

1. Llama-ccp-python - [How to Run LLMs in a Docker Container](examples/1-llama-cpp/README.md) LLM Server
2. [Llama-cpp-python Docker CUDA](examples/2-llama-cpp-cuda/README.md) LLM Server
3. llm-docker - [LLM Everywhere: Docker for Local and Hugging Face Hosting](examples/3-llm-docker/README.md) LLM Server
4. anything-llm - [How to use Dockerized Anything LLM](examples/4-anything-llama/README.md) LLM Server
5. [download-llm](examples/5-download-llm/README.md) huggingface hub
6. [Retrieval Augmented Generation (RAG) with Llama Index and Open-Source Models](examples/6-rag-llm-opensourceLLMs/README.md)
7. [LlamaIndex-RAG-WSL-CUDA](examples/7-LlamaIndex-RAG-WSL-CUDA/README.md)
- [Models](models/README.md) LLM Server
  - Downloaded [models](../../models/README.md)

### Server approach links
LLama-cpp-python

- https://llama-cpp-python.readthedocs.io/en/latest/
- https://github.com/abetlen/llama-cpp-python
- https://github.com/abetlen/llama-cpp-python/tree/main/docker

## RAG-LLM approaches

- youtube
    - https://www.youtube.com/watch?v=f-AXdiCyiT8
    - https://www.youtube.com/watch?v=hH4WkgILUD4
- github
    - https://github.com/krishnaik06/Llamindex-Projects/
    - https://github.com/marklysze/LlamaIndex-RAG-WSL-CUDA

