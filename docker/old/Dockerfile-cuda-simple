# This will build in two stages
# 1. The CUDA 'devel' base image will be used to build llama-cpp-python with CUDA support (~13GB image)
# 2. The CUDA 'runtime' base image will then be used and the llama-cpp build from the devel image will
#       be copied to the new runtime image (5GB final image)

# **********************************************************************************************
# First Stage.  Build from CUDA development image
# **********************************************************************************************
FROM nvidia/cuda:12.1.1-devel-ubuntu22.04 AS builder

RUN apt-get update && apt-get upgrade -y \
    && apt-get install -y git build-essential \
    python3 \
    python3-pip \
    gcc \
    wget \
    ocl-icd-opencl-dev \
    opencl-headers \
    clinfo \
    libclblast-dev \
    libopenblas-dev \
    && mkdir -p /etc/OpenCL/vendors \
    && echo "libnvidia-opencl.so.1" > /etc/OpenCL/vendors/nvidia.icd

# setting build related env vars
ENV CUDA_DOCKER_ARCH=all
ENV LLAMA_CUBLAS=1
ENV LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/compat:$LD_LIBRARY_PATH

# Install depencencies
RUN python3 -m pip install --user --upgrade pip \
    pytest \
    cmake \
    scikit-build \
    setuptools \
    fastapi \
    uvicorn \
    sse-starlette \
    pydantic-settings \
    starlette-context

# Install llama-cpp-python (build with cuda)
RUN CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install --user llama-cpp-python==0.2.77 \
    --extra-index-url https://github.com/abetlen/llama-cpp-python/releases/download/v0.2.77-cu124/llama_cpp_python-0.2.77-cp310-cp310-linux_x86_64.whl \
    --verbose;



# **********************************************************************************************
# Second Stage: Build from CUDA runtime image
# This will use the new runtime base image and then copy 
#   llama-cpp build to this new container.
# The runtime image is much smaller than the devel image (<50%)
# **********************************************************************************************
FROM nvidia/cuda:12.1.1-runtime-ubuntu22.04

# Set environment variable for CUDA library path
RUN export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH

# Install apt packages
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/* \
    && mkdir -p /etc/OpenCL/vendors \
    && echo "libnvidia-opencl.so.1" > /etc/OpenCL/vendors/nvidia.icd

# Install python packages
RUN python3 -m pip install --upgrade pip \
    # pytest \
    # cmake \
    # scikit-build \
    # setuptools \
    fastapi \
    uvicorn \
    sse-starlette \
    pydantic-settings \
    starlette-context \
    diskcache \
    numpy

# Copy llama-cpp libraries compiled with cuda from build local path
COPY --from=builder root/.local/lib/python3.10/site-packages/llama_cpp /usr/local/lib/python3.10/dist-packages/llama_cpp
COPY --from=builder root/.local/lib/python3.10/site-packages/lib /usr/local/lib/python3.10/dist-packages/lib
COPY --from=builder root/.local/lib/python3.10/site-packages/include /usr/local/lib/python3.10/dist-packages/include

# Copy and setup entrypoint.sh
RUN mkdir /app
WORKDIR /app

COPY entrypoint.sh /app/entrypoint.sh
RUN chmod +x /app/entrypoint.sh

ENV HOST=0.0.0.0
ENV PORT=8000

# Set the entrypoint
#ENTRYPOINT ["/app/entrypoint.sh"]

# Expose the default port
EXPOSE 8000


# ----------------------------------------------------------------------------
# To run the container
# ----------------------------------------------------------------------------
# docker run --gpus all -d -p 8200:8000 --name my-llama-container my-llama-cpp-python-app server.config
# 
# Old Method
CMD ["python3", "-m", "llama_cpp.server"]
