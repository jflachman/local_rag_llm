{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation of Chroma DB\n",
    "\n",
    "**Credit to ChromaDB usage guide**:  This Investigation was built using ChromaDB user guide.  Much of the content (description and code) is copied directly from the [Chroma Usage Guide](https://docs.trychroma.com/guides).  This investigation allows me follow a good guide and make modifications and test functionality as needed.\n",
    "\n",
    "- https://docs.trychroma.com/guides\n",
    "- https://docs.trychroma.com/deployment/auth#static-api-token-authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiating a persistent Chroma Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: '/mnt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z7/8qp96g0d3r3b0tsph5j2f5m80000gn/T/ipykernel_63904/207716217.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# os.environ['STORAGE_PATH'] = db_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dev/lib/python3.10/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dev/lib/python3.10/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dev/lib/python3.10/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dev/lib/python3.10/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dev/lib/python3.10/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dev/lib/python3.10/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dev/lib/python3.10/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dev/lib/python3.10/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dev/lib/python3.10/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/mnt'"
     ]
    }
   ],
   "source": [
    "db_path = '/mnt/c/ML/DU/local_rag_llm/prototype/jeff/3-vectorDB/db'\n",
    "\n",
    "if not os.path.exists(db_path):\n",
    "    os.makedirs(db_path)\n",
    "\n",
    "# os.environ['STORAGE_PATH'] = db_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "FastAPI.__init__() missing 1 required positional argument: 'system'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfastapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastAPI \u001b[38;5;28;01mas\u001b[39;00m ChromaFastAPI\n\u001b[1;32m----> 5\u001b[0m chroma_api_impl \u001b[38;5;241m=\u001b[39m ChromaFastAPI()\n\u001b[0;32m      6\u001b[0m chroma_db \u001b[38;5;241m=\u001b[39m chromadb(api_impl\u001b[38;5;241m=\u001b[39mchroma_api_impl)\n\u001b[0;32m      8\u001b[0m client \u001b[38;5;241m=\u001b[39m chromadb\u001b[38;5;241m.\u001b[39mPersistentClient(path\u001b[38;5;241m=\u001b[39mdb_path)\n",
      "\u001b[1;31mTypeError\u001b[0m: FastAPI.__init__() missing 1 required positional argument: 'system'"
     ]
    }
   ],
   "source": [
    "from chromadb.api.fastapi import FastAPI as ChromaFastAPI\n",
    "\n",
    "\n",
    "\n",
    "chroma_api_impl = ChromaFastAPI()\n",
    "chroma_db = chromadb(api_impl=chroma_api_impl)\n",
    "\n",
    "client = chromadb.PersistentClient(path=db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The client object has a few useful convenience methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# returns a nanosecond heartbeat. Useful for making sure the client remains connected.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m client\u001b[38;5;241m.\u001b[39mheartbeat()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "# returns a nanosecond heartbeat. Useful for making sure the client remains connected.\n",
    "client.heartbeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empties and completely resets the database. WARNING This is destructive and not reversible.\n",
    "\n",
    "#client.reset() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Chroma in client/server mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prerequisites \n",
    "\n",
    "Note: instead of running in python, we will use a docker container.  Run the following docker command.\n",
    "\n",
    "- expose the server on port `8200`\n",
    "\n",
    "`docker run -d --name chromadb -v C:/ML/DU/local_rag_llm/db:/chroma/chroma -p 8200:8000 -e IS_PERSISTENT=TRUE -e ANONYMIZED_TELEMETRY=TRUE chromadb/chroma:latest`\n",
    "\n",
    "Once running, access the chromaDB API documentation at:  http://localhost:8200\n",
    "\n",
    "**Note:** Chroma also provides an async HTTP client.  For more details: https://docs.trychroma.com/guides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker options:\n",
    "\n",
    "You can also create a `.chroma_env` file setting the required environment variables and pass it to the Docker container with the `--env-file` flag when running the container.  This will be useful when adding authentication.\n",
    "\n",
    "`docker run -d --name chromadb -v C:/ML/DU/local_rag_llm/db:/chroma/chroma -p 8200:8000 --env-file ./.chroma_env chromadb/chroma:latest`\n",
    "\n",
    "where `.chroma_env` file contains:\n",
    "- IS_PERSISTENT=TRUE\n",
    "- ANONYMIZED_TELEMETRY=TRUE\n",
    "\n",
    "docker flags\n",
    "- `-v` mounts local directory `C:/ML/DU/local_rag_llm/db` to container directory `/chroma/chroma`\n",
    "- `-p` exposes container port `8000` to localhost port `8200`\n",
    "- `-d` runs container disconnected (returns to terminal prompt)\n",
    "- `name` defines the name for the container.  when omitted docker assigns a random name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Python HTTP-only client\n",
    "\n",
    "If you are running Chroma in client-server mode, you may not need the full Chroma library. Instead, you can use the lightweight client-only library. In this case, you can install the chromadb-client package. This package is a lightweight HTTP client for the server with a minimal dependency footprint.\n",
    "\n",
    "`pip install chromadb-client` instead of `pip intall chromadb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma's API will run in client-server mode with just this change.\n",
    "\n",
    "# NOTE: Requires server to be running on port 8200 before running this command.\n",
    "\n",
    "chroma_client = chromadb.HttpClient(host='localhost', port=8200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1721915287858657712"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try the heartbeat for the server.\n",
    "chroma_client.heartbeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Collections\n",
    "\n",
    "Chroma lets you manage collections of embeddings, using the collection primitive.\n",
    "\n",
    "Chroma uses collection names in the url, so there are a few restrictions on naming them:\n",
    "\n",
    "- The length of the name must be between 3 and 63 characters.\n",
    "- The name must start and end with a lowercase letter or a digit, and it can contain dots, dashes, and underscores in between.\n",
    "- The name must not contain two consecutive dots.\n",
    "- The name must not be a valid IP address.\n",
    "\n",
    "Chroma collections are created with a `name` and an optional `embedding function`. If you supply an embedding function, you must supply it every time you get the collection.\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "The embedding function takes text as input, and performs tokenization and embedding. If no embedding function is supplied, Chroma will use sentence transformer `all-MiniLM-L6-v2 model` to create embeddings. This embedding model can create sentence and document embeddings that can be used for a wide variety of tasks. This embedding function runs locally on your machine, and may require you download the model files (this will happen automatically).\n",
    "\n",
    "See: https://docs.trychroma.com/guides/embeddings to use other sentence transformer embeddings or create a custom embedding.\n",
    "\n",
    "Embedding functions can be linked to a collection and used whenever you call `add`, `update`, `upsert` or `query`. You can also use them directly which can be handy for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The sentence_transformers python package is not installed. Please install it with `pip install sentence_transformers`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kerek\\anaconda3\\Lib\\site-packages\\chromadb\\utils\\embedding_functions\\sentence_transformer_embedding_function.py:32\u001b[0m, in \u001b[0;36mSentenceTransformerEmbeddingFunction.__init__\u001b[1;34m(self, model_name, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m embedding_functions\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Use a different sentence transformer: all-mpnet-base-v2\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m sentence_transformer_ef \u001b[38;5;241m=\u001b[39m embedding_functions\u001b[38;5;241m.\u001b[39mSentenceTransformerEmbeddingFunction(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall-mpnet-base-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kerek\\anaconda3\\Lib\\site-packages\\chromadb\\utils\\embedding_functions\\sentence_transformer_embedding_function.py:34\u001b[0m, in \u001b[0;36mSentenceTransformerEmbeddingFunction.__init__\u001b[1;34m(self, model_name, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 34\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     35\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe sentence_transformers python package is not installed. Please install it with `pip install sentence_transformers`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m         )\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels[model_name] \u001b[38;5;241m=\u001b[39m SentenceTransformer(\n\u001b[0;32m     38\u001b[0m         model_name, device\u001b[38;5;241m=\u001b[39mdevice, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     39\u001b[0m     )\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels[model_name]\n",
      "\u001b[1;31mValueError\u001b[0m: The sentence_transformers python package is not installed. Please install it with `pip install sentence_transformers`"
     ]
    }
   ],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Use a different sentence transformer: all-mpnet-base-v2\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a collection\n",
    "#client.delete_collection(name=\"my_collection\")\n",
    "collection = chroma_client.create_collection(name=\"my_collection\", embedding_function=sentence_transformer_ef)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a collection\n",
    "collection = chroma_client.get_collection(name=\"my_collection\", embedding_function=sentence_transformer_ef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collections have a some useful methods\n",
    "\n",
    "Existing collections can be retrieved by name with `.get_collection`, and deleted with `.delete_collection`. You can also use `.get_or_create_collection` to get a collection if it exists, or create it if it doesn't.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.get_or_create_collection(name=\"test\") # Get a collection object from an existing collection, by name. If it doesn't exist, create it.\n",
    "collection = chroma_client.get_collection(name=\"test\") # Get a collection object from an existing collection, by name. Will raise an exception if it's not found.\n",
    "#chroma_client.delete_collection(name=\"my_collection\") # Delete a collection and all associated embeddings, documents, and metadata. ⚠️ This is destructive and not reversible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other useful Collection methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See this used at the last two cells of this notebook\n",
    "col_list = collection.peek() # returns a list of the first 10 items in the collection\n",
    "col_num = collection.count() # returns the number of items in the collection\n",
    "# collection.modify(name=\"new_name\") # Rename the collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Existing collections** can be retrieved by name with `.get_collection`, and deleted with `.delete_collection`. You can also use `.get_or_create_collection` to get a collection if it exists, or create it if it doesn't.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve collection\n",
    "collection_A = chroma_client.get_or_create_collection(name=\"test\") # Get a collection object from an existing collection, by name. If it doesn't exist, create it.\n",
    "collection_A = chroma_client.get_collection(name=\"test\") # Get a collection object from an existing collection, by name. Will raise an exception if it's not found.\n",
    "#chroma_client.delete_collection(name=\"my_collection\") # Delete a collection and all associated embeddings, documents, and metadata. ⚠️ This is destructive and not reversible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "- Read in a list of text document\n",
    "- Add them to the collection\n",
    "- Check the collection has 6 entries\n",
    "- Query the collection for 2 best fit documents\n",
    "- Retrieve those documents\n",
    "- Print the results and note the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are repeating this test, the first reset the database by uncommenting the following lines\n",
    "\n",
    "#chroma_client.delete_collection(name=\"my_collection\", embedding_function=sentence_transformer_ef)\n",
    "#chroma_client.create_collection(name=\"my_collection\", embedding_function=sentence_transformer_ef)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read list of documents from directory\n",
    "\n",
    "- Read a list of text documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['applying RL to build a binanace trading bot.txt',\n",
       " 'reinforcement learning DQN part 1.txt',\n",
       " 'reinforcement learning and introduction part 4.txt',\n",
       " 'reinforcement learning and introduction part 3.txt',\n",
       " 'reinforcement learning and introduction part 2.txt',\n",
       " 'reinforcement learning and introduction part 1.txt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read list of .txt files from directory\n",
    "files = []\n",
    "for x in os.listdir('data'):\n",
    "    if x.endswith(\".txt\"):\n",
    "        files.append(x)\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy files contents to a list\n",
    "content_list = []   # file contents\n",
    "id_list = []        # IDs, text of your choice\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "    with open('data/'+file, \"r\") as f:\n",
    "        content_list.append( f.read() )\n",
    "    id_list.append( 'id' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Our objective is to make a trading bot that trade cryptocurrency using state-of the-art reinforcment learning. To create our RL agents we will use the following technologies:\\n\\nPython\\nReinforcment Learning\\nOPenAI gym\\nBinance\\nYou don’t need any background in ML to understand the following articles, knowledge of Python will be enough. However, if a part is unclear do not hesitate to contact me.\\n\\nHere are the different parts of the creation of our bot, each part will be one article:\\n\\nWe will use Binance data to generate our data that we will customize for our need. All you need is a Binance account, you can create one by clicking here. It is free and one of best trading platform to find cryptocurrency data.\\n\\nAll of the code for this article will be available on my GitHub.\\n\\nSteps for generating data:\\n\\nAdd our Binance API keys\\nPulling data from Binance\\nStandardize our data as we wish\\nSave it in a csv file\\nAdd our Binance API keys\\nAfter cloning the repository, we will need to add our binance keys. We can generate them on the Binance site. Then we just have to add them to keys.py.\\n\\napiKey = “”\\nsecretKey = “”\\nAfter the adding:\\n\\napiKey = “glebfxHdBtDR3RWt3hVRtHmvQaYduT3WodIRdKCOg51qwQsRWbn3aeS298GINGaM”\\nsecretKey = “NSBJOldWu9xXARDCcJRdQ5C0uVth3Qx5bY4y9YyKJubvz7HPcbJL3Racao9xEo1o”\\nThat’s it ! We are connected to the API.\\n\\nPulling data from Binance\\nFor all the steps below we will works on main.py.\\nThe first step is to import a bunch of things that you may be familiar with, we will explain them after:\\n\\nfrom binance.client import Client\\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\\nfrom matplotlib import pyplot\\nimport pandas as pd\\nimport keys\\nimport os\\nIn our case we want BTCUSDT (Bitcoin/Tether) but this script can create data for any pair available on Binance. We want the data since 2 year ago, but we can change this if wanted. The data will be for every minutes so it’s approximately 1 million rows of data.\\n\\n#Changes these values to obtain the desired csv\\npair = os.getenv(“PAIR”, “BTCUSDT”)\\nsince = os.getenv(“SINCE”, “2 year ago UTC”)\\nTo connect to our Binance account we use Client(apikey, secretkey), then we will have our dataframe df that we could work on.\\n\\n#Connect to the Binance client\\nclient = Client(keys.apiKey, keys.secretKey)\\n# Get the data from Binance\\ndf = client.get_historical_klines(pair, Client.KLINE_INTERVAL_1MINUTE, since)\\nWe want to normalize our data (make him between 0 and 1). However we still want to know the “Real Open” and the “Real CLose” it will be usefull to evaluate our bot.\\n\\n# Store the open and close values in a pandas dataframe\\nreal_values = []\\nfor i in df:\\n    real_values.append([i[1], i[3]])\\nreal_df = pd.DataFrame(real_values, columns=[“Real open”, “Real close”])\\nNormalizing data is easy with sklearn, after this all our data are between 0 and 1, exept for the Real Open and the Real Close of course.\\n\\n# Normalize the data\\nnormalizer = MinMaxScaler().fit(df)\\ndf = normalizer.transform(df)\\nWe then transform our data to a pandas array and drop useless columns “Open Time”, “Close Time” and “Ignore”.\\n\\n# Transform the data to a pandas array\\ndf = pd.DataFrame(df,\\ncolumns=[\\n“Open time”, “Open”, “High”, “Low”, “Close”, “Volume”,\\n“Close time”, “Quote asset volume”, “Number of trades”,\\n“Taker buy base asset volume”,\\n“Taker buy quote asset volume”, “Ignore”\\n])\\n# Drop useless columns\\ndf.drop([“Open time”, “Close time”, “Ignore”], axis=1, inplace=True)\\nPlot the data ! It will give us a good idea if our generation is successful.\\n\\n#Plot the data\\ndf.plot()\\nWe then add our 2 dataframes. We also need to drop the last timesteps, because it is possibly not finished.\\n\\n# Add the real open and the real close to df\\ndf = pd.concat([df, real_df], axis=1)\\n# Remove the last timestep\\ndf = df[:-1]\\nOur csv file will be in our folder datafile and at the end we will show the plot.\\n\\n# Add to the csv file\\ndf.to_csv(“datafile/” + pair + “.csv”)\\n#Show the plot\\npyplot.show()\\nTest our script\\nRunning can take a bit of time, for 2 years of data I needed 15 minutes.\\n\\nRunning main.py give us a plot:\\n\\n\\nBut above all it gives us a precious csv file in the folder datafile that we will use for our trading agent.\\n\\nOpenAI’s gym is by far the best packages to create a custom reinforcement learning environment. It comes with some pre-built environnments, but it also allow us to create complex custom environments. An environment contains all the necessary functionality to run an agent and allow it to learn.\\n\\nOur goal is to recreate Binance with the same fees, the same data, and let our agent learn from these. If you do not have a Binance account you can create one by clicking here. A common mistake is to make environments who do not match reality and are to easy for our agents to learn, it’s what we will try to avoid here.\\n\\nIt’s really hard to make money on the stock market/cryptocurrency market. If our bot make +10% per month, it’s probably not sustainable or just pure luck. Being able to trade and keeping our money with paying our fees is already really impressive, let’s do our best !\\n\\nAll of the code for this article is available on my GitHub.\\n\\nThe custom environment\\nFirst let import what we will need for our env, we will explain them after:\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport gym\\nimport random\\nfrom gym import spaces\\nimport static\\nA custom environment is a class that inherits from gym.env.\\n\\nclass CryptoEnv(gym.Env):\\ndef __init__(self, df, title=None):\\nself.df = df\\nself.reward_range = (-static.MAX_ACCOUNT_BALANCE,\\nstatic.MAX_ACCOUNT_BALANCE)\\nself.total_fees = 0\\nself.total_volume_traded = 0\\nself.crypto_held = 0\\nself.bnb_usdt_held = static.BNBUSDTHELD\\nself.bnb_usdt_held_start = static.BNBUSDTHELD\\nself.episode = 1\\n# Graph to render\\nself.graph_reward = []\\nself.graph_profit = []\\nself.graph_benchmark = []\\n# Action space from -1 to 1, -1 is short, 1 is buy\\nself.action_space = spaces.Box(low=-1,\\nhigh=1,\\nshape=(1, ),\\ndtype=np.float16)\\n# Observation space contains only the actual price for the     moment\\nself.observation_space = spaces.Box(low=0,\\nhigh=1,\\nshape=(10, 5),\\ndtype=np.float16)\\ndf : The dataframe that we have created in the past article\\n\\nreward_range : Not really usefull but needed, let’s make it be between 2 huge numbers (static is the file where we stored all of our constants). Let’s make it between -10 millions and +10 millions.\\n\\ntotal_fees : Keep track of the total fees paid\\n\\ntotal_volumes_traded : Keep track of the the total trading volume\\n\\ncrypto_held : Keep track of the crypto held (Bitcoin in our case)\\n\\nbnb_usdt_held, bnb_usdt_start : To track our USDT ( We can easily change the pair)\\n\\nepisode : The number of our current episode (start at 1)\\n\\ngraph_reward, graph_profit, graph_benchmark are used to render the result.\\n\\naction_space : We set our action space between -1 and 1. 1 means using 100% of our USDT to buy BTC, 0 means doing nothing, -1 means selling all of our BTC for USDT.\\n\\nThe method reset()\\ndef reset(self):\\nself.balance = static.INITIAL_ACCOUNT_BALANCE\\nself.net_worth = static.INITIAL_ACCOUNT_BALANCE + static.BNBUSDTHELD\\nself.max_net_worth = static.INITIAL_ACCOUNT_BALANCE\\nstatic.BNBUSDTHELD\\nself.total_fees = 0\\nself.total_volume_traded = 0\\nself.crypto_held = 0\\nself.bnb_usdt_held = static.BNBUSDTHELD\\nself.episode_reward = 0\\n# Set the current step to a random point within the data frame\\n# Weights of the current step follow the square function\\nstart = list(range(4, len(self.df.loc[:, ‘Open’].values) —    static.MAX_STEPS)) + self.df.index[0]\\nweights = [i for i in start]\\nself.current_step = random.choices(start, weights)[0]\\nself.start_step = self.current_step\\nreturn self._next_observation()\\nWe decide to start our current_step to a random point in our dataframe. But not totally random, we choose it so that the older the data the least it is choosed at a starting point. Make sense right ? What append yesterday has more impact then what append 2 years ago.\\n\\nThe method _next_observation_()\\ndef _next_observation(self):\\n# Get the data for the last 5 timestep\\nframe = np.array([\\nself.df.loc[self.current_step — 4:self.current_step, ‘Open’],\\nself.df.loc[self.current_step — 4:self.current_step, ‘High’],\\nself.df.loc[self.current_step — 4:self.current_step, ‘Low’],\\nself.df.loc[self.current_step — 4:self.current_step, ‘Close’],\\nself.df.loc[self.current_step — 4:self.current_step, ‘Volume’],\\nself.df.loc[self.current_step -\\n4:self.current_step, ‘Quote asset volume’],\\nself.df.loc[self.current_step -\\n4:self.current_step, ‘Number of trades’],\\nself.df.loc[self.current_step -\\n4:self.current_step, ‘Taker buy base asset volume’],\\nself.df.loc[self.current_step -\\n4:self.current_step, ‘Taker buy quote asset volume’]\\n])\\n# We append additional data\\nobs = np.append(frame, [[self.balance /  static.MAX_ACCOUNT_BALANCE,\\nself.net_worth / self.max_net_worth,\\nself.crypto_held / static.MAX_CRYPTO,\\nself.bnb_usdt_held / self.bnb_usdt_held_start,\\n0]],\\naxis=0)\\nreturn obs\\nWe take here the data we want our agent to know before making the decision to buy or sell. We decide to give the 5 last timeframes, so our agent will know the Open of the last 5 timesteps (he will also know other things like the Close, the Volume,…). We also pass him our current balance, net worth and the amount of crypto held.\\n\\n\\nThe method _take_action()\\ndef _take_action(self, action):\\n# Set the current price to a random price between open and close\\ncurrent_price = random.uniform(\\nself.df.loc[self.current_step, ‘Real open’],\\nself.df.loc[self.current_step, ‘Real close’])\\nif action[0] > 0:\\n# Buy\\ncrypto_bought = self.balance * action[0] / current_price\\nself.bnb_usdt_held -= crypto_bought * current_price *  static.MAKER_FEE\\nself.total_fees += crypto_bought * current_price * static.MAKER_FEE\\nself.total_volume_traded += crypto_bought * current_price\\nself.balance -= crypto_bought * current_price\\nself.crypto_held += crypto_bought\\nif action[0] < 0:\\n# Sell\\ncrypto_sold = -self.crypto_held * action[0]\\nself.bnb_usdt_held -= crypto_sold * current_price * static.TAKER_FEE\\nself.total_fees += crypto_sold * current_price * static.TAKER_FEE\\nself.total_volume_traded += crypto_sold * current_price\\nself.balance += crypto_sold * current_price\\nself.crypto_held -= crypto_sold\\nself.net_worth = self.balance + self.crypto_held * current_price + self.bnb_usdt_held\\nif self.net_worth > self.max_net_worth:\\nself.max_net_worth = self.net_worth\\nThis method make us buy or sell depending on the action taken and calculate the new net_worth.\\n\\nThe metod step()\\ndef step(self, action, end=True):\\n# Execute one time step within the environment\\nself._take_action(action)\\nself.current_step += 1\\n# Calculus of the reward\\nprofit = self.net_worth — (static.INITIAL_ACCOUNT_BALANCE +\\nstatic.BNBUSDTHELD)\\nprofit_percent = profit / (static.INITIAL_ACCOUNT_BALANCE +\\nstatic.BNBUSDTHELD) * 100\\nbenchmark_profit = (self.df.loc[self.current_step, ‘Real open’]   / self.df.loc[self.start_step, ‘Real open’] - 1) * 100\\ndiff = profit_percent — benchmark_profit\\nreward = np.sign(diff) * (diff)**2\\n# A single episode can last a maximum of MAX_STEPS steps\\nif self.current_step >= static.MAX_STEPS + self.start_step:\\nend = True\\nelse:\\nend = False\\ndone = self.net_worth <= 0 or self.bnb_usdt_held <= 0 or end\\nif done and end:\\nself.episode_reward = reward\\nself._render_episode()\\nself.graph_profit.append(profit_percent)\\nself.graph_benchmark.append(benchmark_profit)\\nself.graph_reward.append(reward)\\nself.episode += 1\\nobs = self._next_observation()\\n# {} needed because gym wants 4 args\\nreturn obs, reward, done, {}\\nThis method calculate our reward. The choice of the formula is crucial for our bot, it depends to the profit we have made (easily understandable) but it also depends on the benchmark profit. It is made that way because it is easy for our agent to make +1% when the BTC make +10% and it is hard to keep our money when BTC goes -10%. We have to reward the agent when he choose the best solution not when he make money.\\n\\nRender the choice of our agent\\nWe have make 2 method that render, one render a summary of our balance, crypto held and profit for each step and one render at the end of each episode. We also plot a graph to have a a better visualisation.\\n\\nConclusion\\nWe have made a environment close to the Binance site, we did not forget fees that can be change on the static.py file. The reward function can be tested and changed if we find a better one but this one will do the work for the moment.\\n\\nOur objective is to train an agent by making him on the binance environment created in the last article. Let’s see if he can make money trading Bitcoin. All of the code for this article is available on my GitHub.\\n\\nIn this article we will be using OpenAI’s gym and PPO agent from the stable-baseline library. We already have a env.py which contains our RL environment, a static.py file which contains all of our constants like fees paid and a .csv file which contains the past Bitcoin data that we have created in this article.\\n\\nWe will create a file main.py that we will run and that will train our agent.\\n\\nThe main.py file\\nFirst let import what we will need for our env, we will explain them after:\\n\\nfrom stable_baselines.common.policies import MlpPolicy\\nfrom stable_baselines.common.vec_env import DummyVecEnv\\nfrom stable_baselines import PPO2\\nfrom env import CryptoEnv\\nimport pandas as pd\\nimport os\\nThen we read the data that we have put on a data folder:\\n\\ndf = pd.read_csv(‘data/BTCUSDT.csv’, index_col=0)\\nWe have imported our environment created (CryptoEnv), the way to instantiate it is not really instinctive:\\n\\nenv = DummyVecEnv([lambda: CryptoEnv(df)])\\nWe create our agent that will try his best to trade on our environment. We can change parameters like gamma or the learning rate to have better results.\\n\\n# Instanciate the agent\\nmodel = PPO2(MlpPolicy, env, gamma=1, learning_rate=0.01, verbose=0)\\nWe then train the agent during 5000000 timesteps:\\n\\n# Train the agent\\ntotal_timesteps = int(os.getenv(‘TOTAL_TIMESTEPS’, 500000))\\nmodel.learn(total_timesteps)\\nAnd let’s render if we succeed to increase our reward over time:\\n\\n# Render the graph of rewards\\nenv.render(graph=True)\\nAfter training we need to check if he can predict the market:\\n\\n# Trained agent performence\\nobs = env.reset()\\nenv.render()\\nfor i in range(100000):\\naction, _states = model.predict(obs)\\nobs, rewards, done, info = env.step(action)\\nenv.render(print_step=True)\\nRun main.py !\\nLet’s see the result for a short training:\\n\\n\\nSo after a few training, what i see is that the agent is doing very poorly at first because he is trading WAY TOO MUCH and he is paying the trading fees each time.\\n\\nAfter ~50 episodes he understand that trading comes with a cost. After ~1000 episodes he barely trade like he know that he will just loose money most of the time.\\n\\nIt’s a great news ! That means we have succeeded to create an environment close to the real one. Surpassing the 0% profit is the real challenge, for the moment we have only recreated the environment.',\n",
       " 'Hi and welcome to the intro series on Reinforcement Learning, today’s topic will be about the DQN algorithm!\\n\\n\\nThis blog post is part of a longer series about Reinforcement Learning (RL). If you are completely unfamiliar with RL, I suggest you read my previous blog posts first.\\n\\nPreviously we talked about policy gradient algorithms. Today we will have a look at a different family of RL algorithms: Q-learning algorithms. And more specifically, we will focus on the vanilla DQN-algorithm.\\n\\nThe topics for today’s blog post are:\\n\\nHistorical significance of DQN\\nWhat is Q-Learning?\\nDQN Explained\\nQ-Learning VS. Policy Gradients\\nThere will also be a part 2 for today’s blog post, which will include a basic implementation of DQN.\\n\\nNote: If you are reading this on a smartphone browser, you might not be able to view the subscripts. You can download the Medium app to mitigate this.\\n\\nHistorical significance of DQN\\nThe algorithm we will discuss today is called DQN. You might wonder why we would discuss such an old algorithm at this point, and you’d be right about the fact that DQN is in fact quite old by now (the paper was published in 2013, an age ago in the software industry!). But despite its age, DQN carries some historical significance as the original paper by Mnih et al. was the first paper to successfully combine deep learning (the use of deep neural networks) with reinforcement learning. Furthermore, some of the ideas from this paper are still valuable today.\\n\\nThe paper specifically showed how a reinforcement learning agent can learn how to play Atari games, directly from pixels. In other words, the algorithm directly takes images as input, just like a human would see them, and outputs which actions to take to successfully play these games.\\n\\n\\nAlthough the idea of Q-learning was not new at the time, Q-learning hadn’t been directly applied before to high-dimensional inputs (like images). Of course, the algorithm was not perfect yet. DQN had many similar problems as the ones we have today, such as sample inefficiency, but it heralded a new age for reinforcement learning, the age of Deep Reinforcement Learning.\\n\\n\\nIn the picture above, you can see a performance comparison of various versions of DQN that have come out since the original. Even to this day, these algorithms remain a popular choice for model-free, off-policy RL.\\n\\nWhat is Q-Learning?\\nBefore we dive into the specifics of DQN, let’s first talk about Q-learning in general.\\n\\nIn the previous blog posts we focussed on policy gradient algorithms. Policy gradient algorithms try to optimise the policy/behaviour of an agent by directly changing the policy. This, however, is not the case for Q-learning algorithms. Q-learning algorithms try to exploit the fact that: if we know the Q-function, we can construct an optimal policy by selecting the action at every state that has the highest Q-value.\\n\\nLet’s back up for a bit and go over some of the terminology. We are still operating in the typical RL framework: we have an agent taking actions in an environment at every discrete timestep. The agent starts in a certain state (or receives an observation of the state). After taking an action, the agent receives a new observation, a (possibly negative) reward, and the agent will be in a new state or receives a new observation.\\n\\n\\nThe goal of reinforcement learning is to learn a policy (a behaviour) that maximizes the agent’s expected cumulative reward. Or in other words, we want the agent to behave in a way such that the reward it receives over time is maximal.\\n\\nIn previous blog posts, we have talked about the value function:\\n\\n\\nThe value function tells us how valuable it is to be in state s when following policy π. It is equal to the summed reward of the trajectory that is expected to follow from state s.\\n\\n\\nA closely related function, is the Q-function:\\n\\n\\nThe Q-function tells us how valuable it is to be in state s and take action a while following policy π.\\n\\n\\nNow let’s say that we denote the Q-function for an optimal policy (a policy that maximises the cumulative reward) as Q*. The optimal Q-function needs to adhere to one important property, the Bellman equation:\\n\\n\\nThe Bellman equation tells us that, for the optimal policy, the value of taking action a while in state s, is equal to the immediate reward r, plus the value of taking the best possible action in the next state (multiplied by discount factor gamma). In other words, the Bellman equation is a recursive formula, the value of taking an action is equal to the reward you get for that action plus following the optimal policy from then on.\\n\\nThe idea behind Q-learning is that we can learn or approximate the optimal Q-function by iteratively updating our estimate of it based on the Bellman equation.\\n\\nIf you haven’t quite caught on to the idea yet, don’t worry, things will probably become more clear in the next section. There we will look at this idea from a more practical point of view.\\n\\nDQN Explained\\nSo, our goal is to find a good approximation of the Q-function. If we can do this, we can make our agent behave optimally by selecting the action that produces the highest Q-value (gives the highest reward) at every state.\\n\\nA rough outline of the algorithm would look like this:\\n\\nInitialize Q-function approximation\\nRepeat:\\n    Collect experience\\n    Update Q-function approximation\\nAs you can tell, we repeatedly switch between collecting new experience and updating our approximation of the q-function. We switch because crucially the states and action we will see, also depend on the policy/behaviour of our agent. We may only encounter certain states after we have already learned certain strategies.\\n\\nQ-function representation\\nIn order to make an estimate of the Q-function, we will need to represent it somehow. In earlier versions of Q-learning, people tried to keep track of a table that contained the exact reward for every state and action that the agent took. In practice this is often infeasible, especially when our state is highly dimensional such as when we use images for representing the state (remember, images are just matrices/tensors with pixel intensity values). One of the innovations the original DQN paper brought, was using a neural network to approximate the Q-function.\\n\\n\\nIn theory the neural network should take a state and an action as input and output the corresponding Q-value. In practice, we will only use the state as input and output a Q-value for each possible action. This makes the learning process a bit easier and saves some computation costs.\\n\\nInitially the neural network weights will be initialised with random values, over time we update the values for a more accurate estimate based on the collected experience.\\n\\nReplay memory\\nAs is mentioned above, we continuously collect new experience. An advantage of DQN is that it is an off-policy algorithm, meaning that it can learn from collected experience, even if that experience was collected with an older version of the policy.\\n\\n\\nCollecting experience happens by letting our agent operate in the environment and storing so-called transitions in a replay buffer/replay memory. A transition refers to the combination of a state, action, next state, reward and an indication whether the episode was over after this transition.\\n\\nDuring the learning process the buffer will be filled with transitions, while we update our q-function estimate based on the already collected transitions. Over time, older transitions will be replaced with new ones once the maximum capacity of the memory is reached.\\n\\nSome variations of DQN use prioritised replay memory, where transitions that are more significant for achieving high reward are more often sampled from the memory.\\n\\nQ-function update\\nFor updating our q-function estimate, we make use of the Bellman equation. We make a prediction of the state from our transition and for the next state in our transition. We then calculate the gradient based on the difference between our Q-value estimate for state sₜ and the received reward plus the maximum Q-value prediction of the next state sₜ₊₁(multiplied by a discount factor).\\n\\n\\nNote that, in order to do the gradient update for the q-function, we need to mask out the predicted values for the actions that weren’t actually taken in the sampled transition.\\n\\nFor example, if we fetch a (non-terminal) transition from our replay memory that contains the following values: state s₇, action a₃, next state s₈ and reward 10. Our Q-value estimate for (s₇, a₃) might be 20 and the maximum Q-value estimate (the highest value for any of the actions) for s₈ may be 8. If we use the mean-squared error, the loss would be 400–324. We get 400 by taking the second power of the estimate for s₇ and a₃ (20²) and we get 324 by taking the second power of the estimate for the highest Q-value for s₈ plus the actual observed reward ((8+10)²).\\n\\nThe loss we just calculated is only valid for action a₃ and not for the other Q-values produced by our neural network, so we want to mask out those other values/gradients, such that we don’t influence the variables in the neural network used to estimate the Q-values of the other actions.\\n\\nEpsilon-greedy policy\\nWe are now close to having all the ingredients needed for our algorithm. One thing that is still missing, is a way for deciding which actions our agent should take during the process of collecting experience. This is the classical exploration-exploitation trade-off. On one hand, we can let our agent take random actions to discover new strategies (exploration), but if we only act randomly, we don’t use the knowledge we have already gained from previous experience. On the other hand, if we only use what we have already learned in the past (exploitation), we won’t discover any new strategies. Hence, the trade-off.\\n\\nSome algorithms deal with this trade-off in a natural way. For example, some policy gradient algorithms output a distribution over actions and from time to time actions get selected that are not optimal according to the algorithm. DQN on the other hand, deals with this trade-off very explicitly by using a so-called epsilon-greedy strategy. Despite the fancy name, the idea is rather simple. We will define a value epsilon ϵ. Every time we need to choose an action, we will generate a new value. If that value is greater than epsilon ϵ, we will select the action that is optimal according to our Q-function (the action with the highest Q-value should give the highest reward). If the generated value is smaller than epsilon ϵ on the other hand, we will select a random value.\\n\\nDuring the course of the training, we can decrease the value of epsilon ϵ, such that our algorithm will do more exploitation over time.\\n\\nPutting it all together\\nAlright, with all this knowledge in mind, I can now present the algorithm as it was described by the original paper. Let’s take a look and go over it.\\n\\n\\nIn the first lines, we initialise our empty replay memory with capacity N and an initially random Q-function. The capacity N refers to the maximum number of transitions that will be stored in this memory.\\n\\nWe then start from an initial state (referred to as a “sequence” in the algorithm). The original authors also talk about “preprocessed sequences”, the preprocessed part refers to the fact that some states/sequences might need some preprocessing before they are fed into the neural network. For example, in the case of a game, they stack several images of the game together, because a single image is not enough to determine how certain objects are moving (see image).\\n\\n\\nNext, we perform an action based on the epsilon-greedy strategy (either random or based on the Q-function estimate), and we store all the information we get from taking that action. The stored information contains the state, the action taken, the reward and the next state.\\n\\nWe then go on to sample some of the transitions from our replay memory and use the formula described above to calculate the loss. Note that for a terminal state (a transition where the episode is ended), solely the reward is used as a target instead.\\n\\nWe then perform a gradient step (update the weights of our neural network) and we repeat this until we are satisfied with the results. That’s it! You now have a solid understanding of the basics of DQN.\\n\\nQ-Learning VS. Policy Gradients\\nAs a short final note, I want to comment a bit on the difference between Q-learning and the algorithms we previously mentioned (Policy Gradient algorithms).\\n\\nPolicy Gradient algorithms are sometimes regarded as being more stable. They are principled because we are directly optimising the policy. This is not the case for Q-learning, in the case of Q-learning we are constructing a policy by selecting an action for every state based on our estimate of the Q-function.\\n\\nThe advantage of Q-learning is that it can be used in an off-policy way. Since we store transitions in a buffer, we can later on reuse transitions for which the actions haven’t necessarily been generated by the latest policy. This makes Q-learning more sample-efficient, since we need to interact less with the environment.\\n\\nBoth methods come with their own trade-offs, and some of the methods we will explore later on, try to achieve the best of both worlds. Stay tuned if you want to learn more about this.\\n\\nConclusion\\nCongratulations for making it all the way to the end of this article! You should now have a solid understanding of Q-learning and DQN. This knowledge will be valuable when we explore even more advanced algorithms in the future. In the next part of this article, I will present an implementation of DQN. The implementation will be built further upon the educative RL framework we are developing for this blog (see the REINFORCE-implementation).',\n",
       " 'Welcome back to the final part of this introduction series to Reinforcement Learning!\\n\\n\\nIn this final part, we are going to make our hands dirty and implement the REINFORCE algorithm that we talked about in part 3. After this you will have a working algorithm that you can use to solve some simple tasks. On top of that, you should have a proper understanding of Policy Gradient algorithms, which you can use to tackle more complex problems and algorithms.\\n\\nThe content of today looks like this:\\n\\nDevelopment Environment Setup\\nThe RL Environment Abstraction\\nImplementing REINFORCE\\nConclusion\\nLet’s dive in!\\n\\nDevelopment Environment Setup\\nThe first thing every developer needs is a development environment. I don’t want to enforce any specific setup or tools, but in case you are a beginner, I will just walk you through to the tools I personally use.\\n\\nIDE (Code editor)\\nMy choice of code editor is Visual Studio Code. It is a lightweight editor which you can use for multiple programming languages. The editor comes out of the box with very minimal tooling, but allows you to add more functionality through extensions.\\n\\nDownload VS Code here\\n\\n\\nPython\\nThe programming language we will be using is Python, a high-level programming language. By high-level we mean that the language takes care of a lot of things for you like memory management. It is also an interpreted and dynamically typed language. You can simply download and install Python like any other application. Personally I use version 3.8 for maximum compatibility with several libraries, but for this tutorial you can just go on and install the latest available version.\\n\\nDownload Python here\\n\\nVirtual environment setup (Optional, recommended)\\nFor this tutorial we will be using some libraries (code that we import to use in our own project). You typically install these libraries through a package manager. In the case of Python, the default package manager is called pip. We will use pip to install some packages. It is however a good idea to create a so-called virtual environment. A virtual environment isolates the packages you installed from other (virtual) environments. Having this isolation is often a good idea, because it allows you to install multiple versions of a package at the same time (but in different environments).\\n\\nThere are multiple benefits of creating a virtual environment and also multiple ways to do it. For now we will just look at one particular way. We start by opening a terminal. Navigate to the folder where you intend the code to be and type the following commands:\\n\\n# For Windows\\npy -m venv env\\n# For Mac OS/Linux\\npython3 -m venv env\\nThis tells Python (version 3.X) to create a new virtual environment named env. Next up, we will need to activate this environment by using:\\n\\n# For Windows\\n\\\\env\\\\Scripts\\\\activate\\n# For Mac OS/Linux\\nsource env/bin/activate\\nThat’s it! Commands you enter will from now on be executed in your virtual environment. To deactivate the environment, simply type deactivate. To activate the environment again, simply type the command mentioned above. Do make sure you are in the same folder as where you have created the environment (your project folder).\\n\\nPyTorch\\nIn the next step of our setup, we will install a deep learning framework. There are many frameworks available, but the industry standards at the time of writing are either TensorFlow or PyTorch. We will be using the latter one although nothing prevents you from opting for TensorFlow (or other frameworks) instead.\\n\\n\\nIn previous posts we talked about neural networks, gradients etc. PyTorch will make abstractions of a lot of these things for us and make our life easier by providing ready to use implementations for these concepts.\\n\\nIn order to install PyTorch, you will need to open a terminal and enter the commands provided to you on this website:\\n\\nhttps://pytorch.org/get-started/locally/\\n\\nYou should select Stable, your operating system, Pip, Python and Default.\\n\\nAdditional packages\\nSome smaller additional pip packages we will use are:\\n\\nrandomname\\ntensorboard\\nYou can just use “pip install nameofpackage” to install these.\\n\\nThe RL Environment Abstraction\\nIn order to train our RL algorithm to solve “any” problem, we are going to make an abstraction of the environment. For this we will use the popular framework called Gym by OpenAI:\\n\\nhttps://github.com/openai/gym\\n\\nYou can install the Gym framework like any other pip package from the link above. Crucially, Gym provides us with several abstractions that we can apply to any environment and problem:\\n\\nYou can create an environment with gym.make(“environment-name”) function\\nAfter creating the environment, you can use the env.step(action) to take an action and go to the next state. This function will provide you with the observation of the next state, the reward granted, a boolean indicating whether the episode ended, and some optional metadata\\nThe env.reset() function will reset the environment to a starting state (and return the values of this starting state)\\nFinally, the env.close() function will clean up any resources allocated by the environment\\nWe will go even one step further than using vanilla Gym. We will write our own Environment class, which contains some additional variables and helper functions, which will mostly help us later on when implementing more complex algorithms.\\n\\nImplementing Reinforce\\nWe will now start writing some code. First we will need a class that can represent our policy (Neural network).\\n\\nNeural Network\\n\\nnet/variable_type.py\\nWe start by defining a small enumeration. We will use this Enum to distinguish between the different types neural networks we could expect.\\n\\nDiscrete represents a discrete output, or more precisely a distribution of probabilities over a fixed number of dimensions.\\nContinuous represents a continuous value.\\nGaussian represents a gaussian distribution output, meaning we output a mean and standard deviation.\\n\\nnet/dnn.py\\nOur next class represents our neural network. We start with a fairly complex constructor (the __init__ function). Even though there is a lot of code, all this constructor does is construct a neural network, given some configuration. The most crucial parameters are dim_in, dim_out and layer_sizes. They represent the input dimension size, output dimension size and the number of neurons in the hidden layers, respectfully.\\n\\nNext we have a forward function, which is typical in PyTorch. This function does a forward pass through our neural network (see the resources here if you want to learn more about how PyTorch handles neural networks).\\n\\nLastly we have a sample function. This function performs a forward pass through the network, constructs a distribution (categorical or gaussian) and then takes a sample from it. Optionally you can pass a boolean called deterministic, in which case either the mean of the gaussian will be returned, or the index of the dimension where the categorical distribution is highest.\\n\\nEnvironment\\nAs mentioned earlier, we will also create our own abstraction of an environment, on top of the Gym abstraction. We do this to make experimentation a bit easier and such that we have to write less code to try out a different environment. Later on this abstraction will also help us with implementing some more complicated algorithms that require us to overwrite some functions (for example in case we need a custom reward function).\\n\\n\\ncommon/step.py\\nWe start with a simple Step class, which represents a step in the environment. Essentially the Step class is just a data container class.\\n\\n\\ncommon/trajectory.py\\nOur Trajectory class holds an array of Steps. Besides this, it contains some helper functions to convert the values these steps contain to Tensors. A Tensor is a data object from PyTorch which will make our computations more efficient. It also contains a helper function to compute the score of a trajectory and the length.\\n\\n\\nenvironments/action_type.py\\nWe define an ActionType enum, to make a distinction between environments that require a discrete action space or a continuous action space.\\n\\n\\nenvironments/environment.py\\nThe next class is our actual environment abstraction. We define some variables like the dimensions of the state space (note: currently only vectors are supported), the dimensions of the action space and the type of actions, along with the usual Gym abstractions. We also allow the user to pass some functions as parameters. They allow a user to override the reward-function or how to determine when an episode is over.\\n\\nThe two most important functions are create_trajectory and step. The former will output a trajectory in the environment, the latter takes a single step in the environment. Both might be useful under certain circumstances.\\n\\nIf you want to use this class in tandem with one of the environments provided by OpenAI Gym, you can do so with the following class, which inherits from the Environment class.\\n\\n\\nenvironments/gym_environment.py\\nIn order to use this with one of the environments, for example the Pendulum-environment, you can do it similar to this example.\\n\\n\\nenvironments/pendulum.py\\nBaseline calculation\\nAs we have seen in the previous blogpost, we will also need to calculate the reward-to-go. I’ve made this code part of a file called “advantages.py”, since this value should in theory be similar to the advantage and we might want to reuse it later for other algorithms.\\n\\n\\ncommon/advantages.py\\nThe file also mentions a buffer, which we will later use for other algorithms, but you can safely ignore it for now. The reward-to-go calculation takes a trajectory and a discount_factor as input. You might want to tweak the discount factor depending on how (un)important future rewards are for the problem you are solving.\\n\\nLogging\\nTo make sure that we see some output and can track how our RL agent is doing, we will write a class that performs some evaluation.\\n\\n\\nloggers/model_tracker.py\\nAs you can see, we use the Tensorboard library in this class. This will allow us to track the progress of our algorithm through a nice dashboard with some graphs.\\n\\nThe crux of the logger is in the _evaluate function. Here we make use of the Environment class to create several trajectories. In order to create those trajectories, we pass in a parameter called get_action. The get_action parameter is a function that takes a state as input, and produces an action as output. This may seem very generic, and it is, but this is exactly the way we want it to allow maximum flexibility in what we want to evaluate.\\n\\nTypically the get_action parameter will be a function that uses our RL agent to determine what the next action (given the state) should be.\\n\\nReinforce algorithm\\nWe then arrive at the implementation of the actual algorithm. In this version we only sample one trajectory before every gradient update. It is definitely not wrong to sample multiple trajectories before doing a gradient update.\\n\\n\\nreinforce/reinforce.py\\nThe most important part of this file is in the train function. First we initialise our environment and logger. We then sample a trajectory, for which we calculate the log probabilities of the taken actions, as well as the reward-to-go.\\n\\nAs you can see, we use some to_tensor functions, which are converting our array outputs to PyTorch Tensors for speedy calculations. In addition we call the to_device function which allows us to do some of the calculations on our graphics card instead of the CPU. For our purposes, these are just implementation details.\\n\\nNext, we calculate the loss (target) by multiplying the log probabilities of the actions by the reward-to-go, as we have seen in the previous blog post. By default, PyTorch accumulates gradients, so we first call the zero_grad function to reset them. We then calculate the gradient of our loss with the backward function and update the weights with the step function.\\n\\nIn addition, this class contains some functions to save and load a trained model, and to sample an action from the model or sample an action deterministically.\\n\\nPutting it all together\\n\\nrun_reinforce.py\\nWe now have all the ingredients to run our algorithm. I created a small main function that instantiates our Reinforce algorithm and passes in a custom environment called EasyTraversalDiscrete. We train the model using the default number of iterations and finally run our model!\\n\\nFor completeness, I will show what the directory structure of our project will look like. As you can see the project contains some extra files that we haven’t discussed yet, for now you can see these as a teaser for the following tutorials.\\n\\n\\nConclusion\\nIf you made it all the way here, congratulations, you have implemented your first RL algorithm and are probably well equipped to do your own research to explore more methods and algorithms.\\n\\nThe last part of this tutorial was probably not so easy to follow though, I have exclusively included images such that you would also think about what we are doing in the code, instead of blindly copying the code. If you have any questions though, feel free to leave a comment or to reach out to me personally.\\n\\nLastly, the files in this project are ordered in such a way, such that they allow for extending the code to other algorithms as well. This will be it for now however, so thank you for following this first series and see you in the next one!',\n",
       " 'Welcome back to the third part of this introduction series to Reinforcement Learning!\\n\\n\\nIn part 2 we explained some of the basic concepts of RL. We will be using these concepts today to discuss an RL algorithm: the REINFORCE algorithm. REINFORCE is conceptually simple, but it gives you a solid basis for understanding and implementing more advanced algorithms. Specifically, REINFORCE is part of the Policy Gradient family of algorithms.\\n\\nThe blog post will be ordered like this:\\n\\nThe REINFORCE Algorithm\\nPseudo-code\\nThe Credit Assignment Problem\\nOn-policy vs. Off-policy RL\\nPolicy Gradient Baselines\\nNote: If you are reading this on a smartphone browser, you might not be able to view the subscripts. You can download the Medium app to mitigate this.\\n\\nThe REINFORCE Algorithm\\nREINFORCE is a policy gradient algorithm. Remember that the policy was a function that defines the behaviour of our agent. We will define our policy as a parametrised function:\\n\\n\\nHere, π is our policy with parameters θ. The policy takes a state sₜ as input and produces an action distribution as output (the probabilities that our agent would perform a certain action aₜ). In this case we talk about a stochastic policy, since our policy outputs probabilities instead of an action directly.\\n\\nPolicy gradient algorithms attempt to improve the policy directly, by changing the parameters of the policy function, such that the policy produces better results.\\n\\n\\nThis is also what the REINFORCE algorithm is doing. The main idea is this: We start with a random policy (so our agent will take random actions). Next, we let our agent operate in the environment according to this policy. This will produce a trajectory (a series of states and actions that the agent took). If the agent got a high reward during this trajectory, we are going to update our policy such that the trajectory our policy produced is more likely to happen next time. Vice versa, if our agent performed poorly, we are going to make the selected trajectories less likely to be selected. We will repeat this process until we (hopefully) get to a good policy.\\n\\nPseudo-code\\nThe process we just described, would look like this:\\n\\ninitialise policy P with random parameters θ\\nrepeat until agent performs well:\\n    generate trajectory τ (s₀, a₀, s₁, a₁, s₂, ...) with reward R \\n    for each state sₜ in trajectory τ:\\n        calculate reward-to-go Gₜ\\n    calculate policy gradient L(θ) using G\\n    update policy parameters θ using L(θ)\\nLet’s break this pseudo-code down into more digestable bits.\\n\\ninitialise policy P with parameters θ\\nWe will first initialise our policy P and our parameters θ.\\n\\nIn the second part of this series, we mentioned that we will represent our policy by a neural network. However, this doesn’t necessarily need to be the case, for policy gradient algorithms we can use any differentiable learning method. A differentiable learning method, in simpler terms, means that we know a way of calculating how to update the parameters of the policy. In this blog post, I will still opt for a neural network.\\n\\n\\nA neural network takes some input in the first layer and produces some output at the end. The connections (weights) between the neurons are our parameters. They influence the output by strengthening or weakening the signal produced by the previous layer. Initially these connections will be random. So our policy will initially give us random actions.\\n\\nrepeat until agent performs well:\\nThe next line in our pseudo-code we can somewhat ignore for now. It just means that we will repeat the next steps until we are happy with how our agent is performing.\\n\\ngenerate trajectory τ (s₀, a₀, s₁, a₁, s₂, ...) with reward R\\nHere we generate a trajectory τ. This means we will now let the agent interact with the environment. We will start in a starting state s₀ (determined by the environment) and then let our agent act according to our policy P with the parameters θ at that point in time. So we will take state s₀, provide it as input to our neural network (our policy) and get a distribution of actions out of it. We can then select an action a₀ by sampling from that distribution. We execute the selected action and arrive at a new state s₁. We repeat this process until we reach a terminal state, the end of the episode (determined by the environment). That is how we get trajectory τ.\\n\\n\\nIf you wonder how we can get a distribution from a neural network, then consider that many distributions are defined by just a few parameters. Take a Gaussian (normal) distribution for example, a normal distribution is defined by a mean and a standard deviation. Our network can take a state as input and produce two scalars (a mean and standard deviation) as output. From this we can construct our distribution and sample an action from it.\\n\\nNote that we don’t necessarily need to make the neural net to output both a mean and a standard deviation. It is fairly common practice to assume a certain standard deviation and let the neural network only predict a mean value.\\n\\nfor each state sₜ in trajectory τ:\\n    calculate reward-to-go Gₜ\\nLet’s look at the next two lines together, because the first line simply means we will repeat the process for every timestep of our trajectory. The line that follows introduces a new concept, the reward-to-go. The reward-to-go is the reward obtained during the trajectory from a certain state onwards.\\n\\n\\nSo for the first state of the trajectory for example, this accounts for the total reward of the trajectory. We will however also take a discount factor into account. A discount factor makes us value rewards in the future less than how we value immediate rewards. This gives us the following formula for the reward-to-go:\\n\\n\\nγ is the discount factor, which has a value between 0 and 1. Rₜ is the reward at timestep t. If we care a lot about future rewards, we could give γ a value of 0.99 for example. If we mostly care about immediate rewards, we could give it a lower value, like 0.7.\\n\\ncalculate policy gradient L(θ) using G\\nWe have arrived at our policy gradient. You might be wondering what a gradient actually is. A gradient is a mathematical way of determining how a change in the parameters affects the outcome of a function. If you happen to remember derivatives from high school, the idea is somewhat the same. Except, we will be dealing with a function with multiple variables.\\n\\n\\nWe want to know how we should update our parameters θ of our policy P, such that the trajectories which have a high reward-to-go Gₜ associated with them, are more likely to happen and vice versa.\\n\\nYou can imagine the policy, the neural network, like a big machine with a lot of knobs to turn. We want to calculate how we should turn those knobs, in order to make the good trajectories more likely to happen. The method that tells us how we should turn those knobs, is the gradient.\\n\\nLet’s say we have a function J that tells us what return (total reward of a trajectory) we can expect for a given policy:\\n\\n\\nR is the return for a trajectory τ. The formula tells us that for a policy π the expected return is equal to the return of the trajectory we expect to follow, given policy π. We want to maximize the value of J(π). So we will try to calculate the gradient of J. This brings us to the next formula:\\n\\n\\nThe ∇ symbol here stands for the gradient. It will tell us how to change the values of θ to increase the value of J. Specifically, the gradient gives us an array of values (one for each parameter in θ). You can think of these values as a direction, pointing towards the place where the outcome of function J would be higher. If we add those values to our parameters θ, the value of J would maximally increase.\\n\\nUnfortunately, we cannot do any calculus with the above formula directly. We will derive an equivalent formula which we can actually use to do our calculations. The full mathematical derivation would take us a bit too far, but if you are interested in a clear explanation on how to derive the gradient, I invite you to read this webpage by OpenAI: Deriving the simplest policy gradient\\n\\nThe calculation of our policy gradient would look like this:\\n\\n\\nThis formula tells us how to calculate the gradient of J. The right-hand side starts with an Expectation-symbol (E). The expectation represents the trajectory τ we expect to get when following policy π (the average trajectory if you will). We cannot exactly calculate which trajectory would be the expected trajectory, because this would require us to calculate all possible trajectories for our policy. Luckily, we can sample one or more trajectories and these should lie close to the expected trajectory.\\n\\nAfter sampling, we apply the formula between brackets. π(aₜ ∣ sₜ) represents the probability of action aₜ under policy π given state sₜ. We will however use the log of this probability. Taking the log will make the gradient calculation process a bit more stable as it allows us to sum the gradients for each timestep instead of having to multiply them. Multiplication might make the gradients very small or big, causing numerical instability. You can check the derivation link to see how taking the log is actually equivalent.\\n\\nFinally, we calculate the gradient for this function, giving us a direction. We multiply this direction with Gₜ, the reward-to-go. If Gₜ was high, we step in the direction of the gradient, making action aₜ more probable. If Gₜ was low, we step in the opposite direction (or we take a relatively smaller step compared to a trajectory with high Gₜ).\\n\\nThe gradient calculation itself is usually taken care of by the programming library we use (for example Tensorflow or Pytorch). If you are interested in how to do the calculation of the policy gradient by hand, both for discrete action spaces and continuous action spaces, then I recommend this excellent post that goes a little deeper into the mathematics: RL Policy Gradients explained by Jonathan Hui\\n\\nupdate policy parameters θ using L(θ)\\nWe arrive at the final line of our algorithm. In this step, we will use the gradient we have just calculated to update the weights of our neural network (the policy).\\n\\nOur gradient is a vector, containing a value for each parameter in θ. So now we will simply update our weights in the following way:\\n\\n\\nIf you are still somewhat confused about how a neural network works and how we can update these weights, I recommend this video explaining backpropagation. Backpropagation is the algorithm that allows calculating and applying these gradients.\\n\\n\\nNote that in the video the narrator talks about a loss function. Strictly speaking we are not computing the gradient of a loss function, even though the calculation is somewhat similar. We are calculating a policy gradient. A loss function (as shown in the video) is a direct indicator of how well your neural network is doing on learning a dataset. A high value for the loss function means your network is making a lot of mistakes. This does not hold true for the policy gradient, there is no indication of how successful our agent is. Another difference is that we are not trying to minimize our policy gradient, but we are trying to maximize it (gradient ascent instead of descent). An easy trick to achieve this, is by performing gradient descent on the negative of our policy gradient.\\n\\nThe Credit Assignment Problem\\nYou might be looking at the explanation above and wonder “how does REINFORCE filter out the good actions from the bad actions?”. Our agent could take a very bad action, but in the end still end up with a score that was relatively good. In such case, we would reinforce that behaviour, making the bad action more likely to happen. Vice versa, our agent might take a very good action that didn’t lead to a good score. In that case the behaviour that led to the good action would be discouraged.\\n\\nThe problem that we are describing is the so-called credit assignment problem: Our algorithm cannot determine which actions were good actions or bad actions throughout the course of a trajectory. Does this mean that REINFORCE won’t work? No. What will happen is that during the good trajectory, on average, there should be more good actions than bad actions. Vice versa for bad trajectories. So after we have gone through enough iterations, our model will start averaging out these trajectories and should eventually grow a bias towards the good actions.\\n\\n\\nA consequence of the credit assignment problem is that our algorithm is more sample-inefficient. We will need to evaluate more trajectories to start filtering out bad behaviours from good ones. There is a lot of research that tries to tackle specifically this problem. As an example, researchers have attempted to use a separate neural network that learns to infer immediate rewards from actions in trajectories. (Link to paper)\\n\\nOn-policy vs. Off-policy RL\\nAnother characteristic of the REINFORCE algorithm is that it’s an on-policy algorithm. On-policy means that the policy gets updated, based on the experience collected from that same policy. This is exactly what happens in REINFORCE. Each iteration, our agent behaves according to the latest version of the policy. Afterwards we update the policy based on what we learned from these interactions.\\n\\nThis doesn’t necessarily need to be the case. Off-policy RL algorithms may update their policy based on what they learned from previous or other policies. Take a look at the image below for an example.\\n\\n\\nIn the on-policy case the agent is constantly interacting with the environment, and as soon as new experience has been collected, the policy gets updated and the agent behaves accordingly.\\n\\nIn the off-policy case we have an agent (or multiple agents) acting according to a policy, we call this policy the behaviour-policy. The experience they collect gets saved in a buffer. We then use that collected experience to train a policy for action selection. So in this case, our buffer can contain experience from older versions of our policy, or even a completely different policy. The methodology we just described is often seen in Q-learning algorithms.\\n\\nOff-policy methods have the potential to be more sample-effective, since the agent might need to interact less with the environment. They are also less prone to get stuck in local minima. The reason for this is that in the on-policy case, the agent might have created such a strong bias towards a certain behaviour, that small deviations from this behaviour are no longer good enough to discover new (and improved) strategies. This is why more sophisticated on-policy methods often try to incorporate a mechanism that encourages the agent to do more exploration.\\n\\nIn the most extreme case of off-policy RL, we only let our agent interact with the environment once. So the experience we collected thus far is similar to a limited dataset of example interactions. This is called offline RL and it is one of the more prominent research directions for RL in the last few years. The reason for the popularity of offline RL is simply because of its practicality and applicability in the real world. Offline RL is very hard however, because the agent that collected the training data was behaving according to an unknown policy. If you want to learn more about this offline RL, I invite you to check out the incredible work done by Sergey Levine et al.: Offline RL paper\\n\\nPolicy Gradient Baselines\\nWe need to talk about one more thing before we can end this part of the blog. You see, we can improve our version of the REINFORCE algorithm in one more subtle way.\\n\\nIn our current version of the algorithm, we apply gradients in such a way that trajectories become more or less likely, proportional with the return (cumulative reward) that these trajectories gave us. How much more or less we make a trajectory is all relative. In mathematical terms, the magnitude of our gradient is completely dependent on the return of a trajectory.\\n\\n\\nIn the picture above, you can see we get three scores: 100, 0 and 100. So what our algorithm will do, is update the policy accordingly. This might sound desirable at first, but actually it is not. Because the last trajectory is similar to the first trajectory and once again we make it more likely. After a while, this might induce such a big change in behaviour that it might have an averse effect on the average outcome of the game. If we update our policy parameters too much in a specific direction, they might start to show unwanted behaviours.\\n\\nSo what can we do about this? Intuitively, we want our gradient to update not just according to the score, but we want to update it when our agent did better or worse than expected. This is something we can achieve by introducing a baseline.\\n\\n\\nThe formula shown here is exactly the same as our policy gradient, with one difference: We subtracted V(sₜ) from our reward-to-go Gₜ. Remember that the value function is an estimator for the value of being in a certain state and behaving according to a certain policy. So V is a good baselines in this case, because by subtracting V from Gₜ, we will now only update our policy “if we did better or worse than expected”. In practice we don’t know the exact value of V. So usually during the course of our algorithm we additionally train a neural network that estimates V.\\n\\nSo why do baselines actually improve the training process? Mathematically speaking, adding an appropriate baselines will reduce the variance of our samples. In simple terms, our policy will have a clearer learning signal, which will make the training faster and more stable. We will talk more about baselines in a later post.\\n\\nConclusion\\nWe have once again explored a lot of topics in today’s post. Congratulations on making it this far! You should now have a solid understanding of basic policy gradient algorithms. Additionally, this post will prepare you to get your hands dirty in our next post. If this post seemed very theoretical to you, or you learn more quickly through code, then stick around for the final post of this intro series!',\n",
       " 'Welcome back to part 2 of the intro to reinforcement learning series!\\n\\n\\nIn part 1 we described what reinforcement learning (RL) is, what you can do with it and some of the challenges of RL. This time around, we will explain some of the main concepts of RL and introduce some terminology and notation. This will be useful for the next part, where we discuss a simple RL algorithm.\\n\\nSpecifically we will talk about:\\n\\nStates\\nActions\\nReward functions\\nEnvironment dynamics (Markov Decision Processes)\\nPolicies\\nTrajectories and return\\nValue function & Q-function\\nAs always, if you’re already familiar with the content, feel free to head on to the next part.\\n\\nNote: If you are reading this on a smartphone browser, you might not be able to view the subscripts. You can download the Medium app to mitigate this.\\n\\nThe RL framework\\nLet’s start with a quick recap. The RL framework involves an agent that tries to solve a task in a certain environment. At every timestep t, the agent needs to choose an action a. After this action it might receive a reward r and we get a new observation of its state s. The new state can be determined both by the action of the agent and also by the environment the agent is operating in. In RL we want to maximize the future accumulated reward.\\n\\nFor the sake of explanation, we will use the game of Pong to further explain these concepts.\\n\\n\\nPong is an easy game to explain: There are two players, represented by the green and the red paddles in the image above. In our example, we will consider the player who operates the green paddle to be our agent. The goal of Pong is to bounce back the ball, past the other players’ paddle. When either player manages to bounce the ball past the other player, they get a point and the ball starts moving from the centre of the field again. The player with the highest score wins.\\n\\nHere is a 1 hour long gameplay video if you are still confused.\\n\\nHow do the different terms of the RL framework apply when we want to train an RL agent to play Pong?\\n\\nStates\\nLet’s start with the state. A state is usually a vector, matrix or other tensor. If you are not familiar with these terms, don’t worry, they are essentially various ways of organising numbers. You can check the image below as an example.\\n\\n\\nThe state should describe the relevant information we need in order to decide which action to take at timestep t. We will denote the state at timestep t as sₜ .\\n\\nIn the case of Pong, a good description of the state can be a vector (an array of numbers) containing the position of the players’ paddle, the position of the ball and the angular velocities of the ball. This would be enough for our agent to decide whether it’s best to move the paddle up or down. Because from this information it can check its own position, the position of the ball and where the ball is moving towards.\\n\\nNote that it wouldn’t suffice to only encode the position of the ball and the player, because we also need some information that indicates in which direction the ball is moving.\\n\\nAnother option would be to directly use the frames/pixels of the game to represent the state. The state is then a matrix or tensor representing the light intensities/colours of the pixels. This will be harder for the AI algorithm to learn, but it is definitely possible. It will be harder, because now it not only needs to learn how to play the game, but it also needs to learn how to extract the relevant information from these pixels. It will need to learn that for example the blue pixels in the middle represent a ball and it will need to learn that the location of this ball is relevant in solving this problem.\\n\\nNote: We also encounter the same problem as the one we encountered when we used a state vector, namely: one static image is not enough to show in which direction the ball is moving. As a solution, the state is often represented as a stack of the last few frames.\\n\\n\\nAs a final remark, even though we are using the term “state”, the term “observation” is more appropriate. The difference is that a state is assumed to give a full description of the state of the world, while the term observation refers to a (possibly partial) observation of the state. For our purposes, we will use these terms interchangeably.\\n\\nActions\\nIn Pong we want our agent to be able to move the paddle up and down.\\n\\n\\nThere are various ways of representing this action, but first there is an important distinction to make about actions in RL in general. We will make a distinction between discrete action spaces and continuous action spaces.\\n\\nIn the case of a discrete action space, our actions are discrete, this means that they can take on only a certain set of values. For example in the game of Pong, we could define our actions to be “Up”, “Down” and “No move”. This means that our paddle can move up, down or stand still at every timestep, but only by a predefined amount. Moving up could mean that we move up by 10 pixels every time for example.\\n\\n\\nTo represent these three actions, we could use a vector (row of numbers), where each dimension (column) of the vector indicates which action to take. Normally the values in the vector are still continuous and represent “a categorical distribution” of which action the agent should take. You can disregard this for now and just look at this as “the dimension in the vector where the number is 1, is the action our agent will take”.\\n\\n\\nIn the case of a continuous action space, our actions are continuous, meaning that they can take on any value. So if we look at the game Pong, now our action might not just represent the paddle moving up our down, but we might also specify by how much the paddle should move up or down. In this case, a vector with a single dimension (a scalar) will suffice. The scalar can have a positive value for moving up, 0 for not moving and a negative value for moving down.\\n\\nReward function\\nThe reward function tells us after every action how much reward the agent got for taking that action. Or more precise: the reward function takes a state Sₜ and an action Aₜ as input at every timestep t, and outputs a reward Rₜ . The reward Rₜ is a scalar (number) that indicates how well the agent did.\\n\\n\\nIn Pong, we could opt for giving a reward of 1 every time the agent manages to make the ball go past the other players’ paddle, 0 when nobody scores, and -1 when he misses the ball causing the opponent to score.\\n\\nFormally, the reward function has the following signature:\\n\\n\\nWhere S represents the set of states, A the set of actions and ℝ the set of real numbers.\\n\\nYou should be careful when choosing how you define the reward the agent receives, as this can highly affect its behaviour. For example, it can produce unforeseen consequences or if your agent sees too little rewards (sparse reward problem), it might fail to learn anything at all.\\n\\nEnvironment dynamics\\nThe environment dynamics answer the following question: given a state and an action, what will be the next state? Typically, we don’t need to model these environment dynamics ourselves. Our RL algorithm will need to figure them out on its own. Even more so, for many problems the environment dynamics are unknown.\\n\\nWe will briefly talk about the idea of environment dynamics nonetheless, because the only requirement for us to be able to apply RL to a problem, is that we are in theory able to define the problem as a Markov Decision Process (MDP).\\n\\nA Markov-what you ask? You will see this term come up in a lot of the literature around RL and the reason for this is that MDP’s are a way of formalising problem statements. They will also help us explain some of the algorithms we will use later on. Let’s talk about MDP’s for a moment.\\n\\nMarkov Decision Process\\nAn MDP is a discrete-time stochastic control process that has the Markov property. This is a mouth-full, but it’s not so complicated. Let’s break it down.\\n\\nWe are describing a process in which we have a set of states. These are the states that our agent can be in during the process of solving a problem. The process is modelled in discrete time, which roughly means that everything that happens is represented as a separate point in time. In other words, you cannot be “in between” two states, either you are in a state or you are not. A point in time is then represented by a number and the next point in time would be that number +1.\\n\\nWe also define a transition distribution:\\n\\n\\nA transition means we are going from one state to another. So this formula tells us the probability of going to a next state sₜ₊₁, given our current state sₜ and taking an action aₜ. That is the “stochastic” part of our definition, stochastic simply means that we are dealing with probabilities. So sometimes our agent might decide to take an action, but because of the environment, there might not be a guarantee that he ends up in the state the agent intended.\\n\\nLast but not least, our control process needs to have the Markov property. Markov refers to its inventor, Andrej Markov. The property tells us that any next state only depends on the current state. When a process has the Markov property, the next state does not depend on any other states except for the current one. So our agent might have seen a million states before, but still it won’t matter to determine the next state, only the current state does.\\n\\nThe diagram below shows an example of such a process. The circles containing S₀-S₂ represent states, while the pink circles represent actions that can be taken in those states. S₀ is green, because it represents a starting state. Most processes also have terminal states (states that mark the end of the process), but they are not drawn here.\\n\\n\\nIf you are in state S₂ at timestep t for example, you can take actions A₀ or A₁. If you opt for action A₀, then you have a 26% chance of ending up in state S₀ at time t+1, while you have a 74% chance of ending back up in state S₂ at t+1. Notice that it doesn’t matter which states we have previously visited to determine our next state.\\n\\nGreat, that’s all there’s to it, now you know what an MDP is! Now what would an MDP look like for the game Pong? We have already discussed what the states and actions of our agent can be. The states describe the positions of the paddles and the ball, as well as the speed of the ball. The actions we can take are move up or down.\\n\\nIn the case of Pong we are dealing with an environment that is fully observed, we have access to all information in this game. This is not always true, sometimes our environment can only be partially observed. Think for example of a game of cards, where you cannot see the cards your opponent is holding.\\n\\n\\nOur environment, as we currently described it, is still stochastic however. The reason for this is mostly due to our opponent. Say that our paddle is at position X1 while our opponent is at position X2. Even though we decide how our paddle will move and the physics of the moving ball might be deterministic, our opponents’ paddle might move randomly or probabilistically.\\n\\nPolicies\\nAlright, we have a lot of terms to cover in today’s post, next up are policies. When we talk about a policy, we are referring to the “behaviour” of our agent. Given a state, what should be the next action the agent takes? This is the crux of our algorithm.\\n\\nSimilar to what we did for actions, we will distinguish between deterministic policies and stochastic policies. We can write a deterministic policy as:\\n\\n\\nIn this formula, aₜ is an action at timestep t, sₜ is the state at timestep t, and μ is our policy function with parameters θ. You can simply look at this formula as a mathematical function that receives some input (the state) and then outputs a corresponding action for the agent to take. The parameters θ influence the output of the function, so we want to change these such that our agent behaves in a way that optimally solves the problem. We will see in the next blog post how we do that, don’t worry about them for now.\\n\\nA stochastic policy on the other hand, would look like this:\\n\\n\\nThis formula is very similar to the deterministic policy. aₜ is an action at timestep t, sₜ is the state at timestep t, and π is our policy function with parameters θ. It is a convention to use π for stochastic policies. The difference with the deterministic policy, is that we select action aₜ with a certain probability, but it could very well be the case that the next time we are in a similar state, we select a different action instead.\\n\\nNeural networks\\nYou might wonder, these policies look nice, but what function do we use to model them?\\n\\nIn practice, we will often use a neural network (NN)for these functions. Remember, we are trying to create a self-learning system after all. We will very briefly explain what a neural network is and how it works.\\n\\n\\nA neural network (NN) is a learning structure loosely based on the human brain. It consists of neurons (the cyan dots in the picture) and synapses (the blue wires between the blue dots). When you look at the NN, you can see that it has various “columns” of blue dots, we will call those layers. The first layer (blue dots with green edge) we call the “input layer”. The final layer (a single neuron with blue edge) we call the output layer.\\n\\nA NN works by taking some numbers as input, starting at the -you guessed it- input layer. This input is similar to an electrical signal arriving at the brain, and passing through it. The synapses (connections) reinforce or weaken the signal. Then, the signal from various neurons gets combined, right before it arrives at a new neuron in the next layer. If the signal is strong enough, the neuron will “fire” and propagate the signal further, if it is not strong enough, it will not propagate the signal. This process continues, until the signal arrives at the output layer and thus gives us an output. So in our case, the state will be the input to the network, and the number (or numbers) it outputs will be the action of our agent.\\n\\nInitially the NN is wired randomly, so the signal will also be strengthened and weakened randomly, and thus produce a random output/action. The trick is now for us to change the connections in such a way that the neural network produces an output that is closer to the one we were expecting. You can look at the NN as a big machine with a lot of knobs, and now we need to figure out how to turn the knobs until it produces satisfactory output. We can do this using mathematics, through an algorithm called backpropagation. But we will omit details for now.\\n\\nRemember the parameters we were referring to when talking about policies? In the case of a neural network, they are in fact referring to these connections! Changing the connections is our way of changing the output the policy produces.\\n\\nWe have covered a lot of ground already and we are nearly ready to complete this article full of definitions and notations. I promise you, they will all be useful later on when we get to the actual coding of these algorithms!\\n\\nTrajectories and returns\\nTrajectories are easy to understand once you know about states and actions. A trajectory is simply referring to a certain sequence of states and actions:\\n\\n\\n\\nAt the end of a trajectory, our agent will have accumulated a certain amount of rewards. This accumulated reward is what we call the return.\\n\\nWe distinguish between finite-horizon and infinite-horizon returns. A finite horizon means that our trajectory will consist only of a limited amount of timesteps, while an infinite-horizon means we will calculate the return for infinite timesteps.\\n\\n\\nThis is the formula for the finite-horizon return. We have a trajectory τ for which we calculate the return R(τ). In the formula, we take the sum of every rₜ for timestep 0 until horizon T.\\n\\n\\nThe formula for the infinite-horizon return is quite similar, except we are now summing over a theoretical infinite number of timesteps and we introduced a new variable γ. The γ (gamma) is the discount factor, its value is always between 0 and 1 (inclusive). The lower the discount factor, the less valuable future rewards will be.\\n\\nThe discount factor is both mathematically convenient and it has an intuitive interpretation as well. It is convenient because for problems with infinite timesteps, we know that it will converge to a finite value. On the other hand, it is also intuitive to think that it’s usually better to get rewards rather sooner than later. It is for example better to get cash now than later, because due to inflation the cash might be worth less later.\\n\\nValue function and Q-function\\nIn RL we are looking to find a policy (a behaviour) for our agent such that it will receive the highest possible cumulative reward. In order to discover this policy, there are some other useful functions we can define, like the Value function. The value function looks like this:\\n\\n\\nYou can read this formula as: “The value of being in state s, while acting under policy π, is equal to the reward we get of the trajectory τ that we expect to follow from policy π, given that state s is the starting state.” Or in short, it roughly tells us how good it is to be in a certain state. The “E”-symbol in this formula stands for “expectation”. It is the trajectory we expect to get from policy π, the average trajectory if you will.\\n\\n\\nAs an example, have a look at the above image. In the top image, our agent is very far from the ball, while in the bottom image, our agent is pretty close to the ball. If our agent has a policy that makes him move towards the ball, then it is highly likely that the value of state Y is higher than the value of state X. The reason is that in state X, our agent might not be able to make it to the ball in time, thus losing him the point.\\n\\nAnother function that turns out to be very useful is the Q-function. It looks like this:\\n\\n\\nAs you can tell, it looks rather similar to the value function, and it is in fact similar, with one subtle difference. The function says :“The value of being in state s and taking action a, while acting under policy π, is equal to the reward we get of the trajectory τ that we expect to follow from policy π, given that state s is the starting state and the first action we take is a.” Or in short, it tells us how good is it to be in a state and to take a certain action.\\n\\n\\nBoth of these functions are often used in RL algorithms. They have whole families of algorithms associated with them. For example, Q-learning algorithms are based on exploiting the fact that if you know the Q-value for all possible actions in a state, then you can decide which action is best to take. You will get an optimal policy simply by picking the action that has the highest value for the Q-function.\\n\\nUnfortunately, in most cases we can’t know the Value function or Q-function exactly, so in practice we will estimate/learn them using learning algorithms (like a neural network).\\n\\nConclusion\\nThis was a long post full of terminology, but you made it! Having talked about all these concepts, will give you a solid basis to start implementing your own algorithm. In the next part we will talk about the REINFORCE algorithm. The REINFORCE algorithm is conceptually simple, but lies at the heart of the policy gradient-family of algorithms. Thank you for reading and see you in the next part!',\n",
       " 'Hi and welcome to the first part of a series on Reinforcement Learning.\\n\\n\\nIf you somehow ended up here without having heard of Reinforcement Learning (RL) before, then let me summarize it as follows: “RL is a general framework for training an artificial intelligence model to solve a certain task or goal” … or in layman’s terms, we make AI do cool things!\\n\\nThe goal of this blog series is to learn about RL and simultaneously explore some of the more recent research later on. We will start from the very basics and work our way towards more advanced topics. Even if you have almost no prior programming and/or mathematics knowledge, you should be able to follow along pretty smoothly.\\n\\nThe first mini-series will be split into four parts:\\n\\nPart 1: What is Reinforcement learning?\\nPart 2: RL terminology and formal concepts\\nPart 3: The REINFORCE algorithm\\nPart 4: Implementing the REINFORCE algorithm\\nAt the same time, this mini-series will be the introduction to future posts with increasing complexity. Feel free to skip to the next part if you are already familiar with the content.\\n\\nNote: If you are reading this on a smartphone browser, you might not be able to view the subscripts. You can download the Medium app to mitigate this.\\n\\nThat sounds cool! … but what can I do with RL?\\nReinforcement learning is a framework to learn any task. In theory, RL can solve any problem that is phrased as a Markov Decision Process. We will explain what that means later on. For now, let’s have a look at some successful applications. If you enjoy these examples, please be sure to also check out the work from the original authors.\\n\\nGeneral\\nOne of the videos I love showing is the hide and seek video from OpenAI. The video is a nice example of how RL can help us with finding novel solutions to problems, without explicitly programming tactics or solution methods.\\n\\n\\nGames\\nOne of the early successes of Deep RL (which is combining RL with neural networks), was the ability to learn how to play Atari games, straight from pixels. Later on, researchers took it upon themselves to not only evaluate RL on the (relatively) simple Atari games, but rather evaluate it on the hardest competitive games out there. The assumption here is, that if RL can solve these complex games, it can also generalize to challenging real-world settings. As an example, this is Deepmind’s AlphaStar taking on a pro-gamer in the game StarCraft 2.\\n\\n\\nRobotics\\nSolving tasks in simulations and video games is one thing, but what about real life? Another popular field where RL is often applied (or at least holds great promise), is robotics. Robotics are significantly harder than simulations for various reasons. Think for example about the time it takes to repeatedly make a robot try out a certain action. Or think about the safety requirements involved for robotics. In the example below, you can see how the ANYmal robot from the Robotics System Lab in Zürich learned to recover from a fall.\\n\\n\\nReal world examples\\nRL can be applied to many other domains than the ones I just mentioned. Advertising, finance, healthcare, … just to name a few. As a final example, I present a goal-oriented chatbot, trained to negotiate about sales (source: https://siddharthverma314.github.io/research/chai-acl-2022/ ).\\n\\n\\nRL: The basics\\nA description of the RL framework is as follows: We have an agent that tries to solve a task in a certain environment. The concept of agent should be taken very broadly here, an agent can be a robot, a chatbot, a virtual character, etc. . At every timestep t, the agent needs to choose an action a. After this action it might receive a reward r and we get a new observation of its state s. The new state can be determined both by the action of the agent and also by the environment the agent is operating in.\\n\\nThe RL problem is trying to maximize the cumulative reward the agent gets over time.\\n\\n\\nImagine our agent is a monkey and the task we want the monkey to solve, is to pick up as many bananas as possible. At every timestep, the monkey needs to decide to take an action. The actions could be to step towards the tree, grab something, climb, … Perhaps the reward at every timestep can be defined as the number of bananas the monkey got at that timestep. After every action, the monkey will also be in a new state. Maybe we define the state of the monkey as its position in the world. So when the monkey takes a step, the state at the next timestep would be the coordinates of the monkey at the next timestep. We are now searching for the optimal behavior, the best sequence of actions the monkey can take, to maximize the cumulative number of bananas it will get.\\n\\nHow does RL fit in the bigger picture?\\nYou might look at this framework and think: “Hey, isn’t that exactly what people are already studying in the field of …?”. And in fact, you might be right.\\n\\nIn other domains like engineering or mathematics, people have often been studying the same problems with different names and methods. Or in fields like neuroscience and psychology, some similarities can be found in the way our brain “rewards” us by releasing dopamine.\\n\\nThe likely reason for this intersection of domains, is that reinforcement learning is the study of a fundamental problem. It is essentially the science of decision taking. In these series, we will be looking at it from the umbrella of computer science and machine learning.\\n\\n\\nThis general applicability is also what makes RL so interesting to me personally. RL is one of the potential technologies that could get us closer to general AI: an AI system that can solve any task, in contrast to a narrow set of tasks. There are also other technologies (e.g. big language models, graph neural networks, …) that are making some strides in this regard, but the problem statement of RL in particular seems to be the most ambitious.\\n\\nAnother way of situating RL, is by looking at how it compares to other learning paradigms. Within the field of machine learning, people often distinguish between Supervised learning, Unsupervised learning and Reinforcement learning.\\n\\n\\nWhen it comes to Supervised learning, we are essentially trying to learn a function, a mapping from X to Y. Our dataset consists of samples X and during training we provide the AI system with labels Y of what these samples should map to. Our AI model is successful when it correctly predicts a label y given an (unseen) sample x.\\n\\nAs an example, say we have a dataset consisting of cat and dog images. Each sample (image) has a label, stating whether the image contains a “dog” or a “cat”. The goal of our supervised model, is now to learn how to map a dog-image to the label “dog” and a cat-image to the label “cat”. After it has learned this mapping, the hope is that this AI model can repeat this process for new images that it has not seen before.\\n\\nIn an Unsupervised learning context, we no longer provide any labels Y. So the task for the AI system now becomes learning some general statistics about the dataset. We could for example give an AI the task to generate a new sample, similar to the ones seen in the dataset. In this case the model would be considered successful if it manages to correctly learn the interesting characteristics of a dataset.\\n\\nReinforcement Learning is rather different from the previous paradigms. In the case of RL, we consider an agent that is actively interacting with an environment. Through its interactions, it is possible that it may influence the environment that it operates in. The “dataset” we need to consider here, are the actions our agent took and the accumulated rewards it got by taking those actions. An added difficulty here is that our dataset is non-static. Say that our agent acts in a certain way, we can then collect some data of the actions our agent took and we can try to optimize these (e.g. do more of the actions that led to a successful result). But as a result of this optimization, we have now changed the behavior of this agent, and thus we will need to collect new data to see how well our agent fares now.\\n\\nIf RL is so great, then why isn’t everyone using RL?\\nAfter reading all this, you might be wondering why people aren’t using RL to solve all imaginable problems. The truth is that even though the field has made a lot of advancements in the last few years, there are still a few fundamental problems to be solved. Progress is being made all the time, but to give you an idea of what you might encounter, I’ll list a few common ones.\\n\\nSample (in-)efficiency\\nIt is generally known that RL is very sample-inefficient. We regard a “sample” as an interaction with the environment. RL needs a lot of samples/interactions to be able to solve a task. In this sense, RL is very inefficient compared to humans, for example, it doesn’t take a human dozens of hours to learn how to play an Atari game.\\n\\nThis sample-efficiency can in part be explained by the fact that humans can leverage a lot of their previous knowledge (priors) when they encounter a new task. A human can for example reuse some of the knowledge and skills of previous games and/or concepts they already acquired from other experiences throughout their life. An RL-agent in contrast, starts the learning process without any assumptions.\\n\\nAnother thing to mention is that leveraging knowledge from previous tasks is also an active research topic. I’ll just put one example (out of many) here to give you an idea.\\n\\nDeepmind Gato\\nGoogle Jump-Start RL\\n\\nThe exploration-exploitation trade-off\\nWhile the previous problem sounds more like an engineering effort (it’s not), the exploration-exploitation trade-off seems more fundamental. Whenever we train an RL-agent, the agent will need some time to explore, it needs to take some actions that it hasn’t taken before, in order to discover how to solve the problem. On the other hand, we can’t let the agent always take random actions, because these random actions might lead to nothing. Sometimes we want the agent to leverage what it has already learned to try and optimize further. This is the exploration-exploitation trade-off, we want an automated way to strike a good balance between letting the agent explore and taking actions for which it already knows what they will lead to.\\n\\nFor a lot problems, it is quite possible that the agent gets stuck in a local optimum.\\n\\n\\nThe exploration-exploitation trade-off sounds very much tractable at first, but it turns out to be one of the hardest problems for RL to solve. To give you an idea about how hard this problem is: The problem was originally considered by the scientists of the allied forces, but was suggested to be dropped over to Germany, because it was deemed so intractable that they wanted the German scientists to also waste their time on it.\\n\\nNo silver bullet for this problem has been found and there are many people working on various solutions. I will leave a link here to a previously proposed solution called “curiosity-driven exploration” which I found particularly interesting.\\n\\nCuriosity-driven exploration by Self-supervised Prediction\\n\\nThe sparse-reward problem\\nAnother rather fundamental problem, is the so called Sparse-reward problem. As the name implies, this problem occurs when our RL-agent receives so little rewards, that it actually gets no feedback on how it should improve.\\n\\nImagine for example this mountain car. The agent needs to move the car left and right, such that it gets enough momentum to reach the top. Initially though, the agent doesn’t know that it needs to move the car back and forth to reach the top. If we only give our agent a reward (a positive feedback signal) when we have reached the flag, it might not ever get a positive feedback signal, simply because it might never reach the flag by taking random actions (exploration).\\n\\n\\nSomething commonly done to counteract this problem, is by “reward shaping”. We will modify the reward such that the agent gets more feedback signals to learn from. In case of the mountain car, we could for example also give the agent a positive reward based on the speed or altitude it achieves. However, reward shaping is not a scalable solution. Luckily other solutions are being sought after.\\n\\nI’m leaving another example here which I think is an interesting (but not generally applicable approach) for counteracting sparse rewards.\\n\\nHindsight Experience Replay\\n\\nThe aforementioned problems are just some prominent examples, but there are in fact many more problems that are being looked into by some of the brightest minds out there. The good news is that regular and steady progress is being made.\\n\\nConclusion\\nPhew, that was a lot of information in a very short period, but you made it through! This first introduction should give you a good idea of what RL is all about.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id0', 'id1', 'id2', 'id3', 'id4', 'id5']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Documents to a collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashwinikumar/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:13<00:00, 6.29MiB/s]\n"
     ]
    }
   ],
   "source": [
    "# Add documents to the collection\n",
    "collection.add(\n",
    "    documents=content_list,\n",
    "    metadatas=[\n",
    "        {\"source\": files[0]},   # This holds the file name (source name) for the text being added to the DB\n",
    "        {\"source\": files[1]},\n",
    "        {\"source\": files[2]},\n",
    "        {\"source\": files[3]},\n",
    "        {\"source\": files[4]},\n",
    "        {\"source\": files[5]}\n",
    "    ],\n",
    "    ids=id_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify there are 6 items in the collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['id1', 'id5']], 'distances': [[1.4940412090843298, 1.6182181858429838]], 'embeddings': None, 'metadatas': [[{'source': 'reinforcement learning DQN part 1.txt'}, {'source': 'reinforcement learning and introduction part 1.txt'}]], 'documents': [['Hi and welcome to the intro series on Reinforcement Learning, today’s topic will be about the DQN algorithm!\\n\\n\\nThis blog post is part of a longer series about Reinforcement Learning (RL). If you are completely unfamiliar with RL, I suggest you read my previous blog posts first.\\n\\nPreviously we talked about policy gradient algorithms. Today we will have a look at a different family of RL algorithms: Q-learning algorithms. And more specifically, we will focus on the vanilla DQN-algorithm.\\n\\nThe topics for today’s blog post are:\\n\\nHistorical significance of DQN\\nWhat is Q-Learning?\\nDQN Explained\\nQ-Learning VS. Policy Gradients\\nThere will also be a part 2 for today’s blog post, which will include a basic implementation of DQN.\\n\\nNote: If you are reading this on a smartphone browser, you might not be able to view the subscripts. You can download the Medium app to mitigate this.\\n\\nHistorical significance of DQN\\nThe algorithm we will discuss today is called DQN. You might wonder why we would discuss such an old algorithm at this point, and you’d be right about the fact that DQN is in fact quite old by now (the paper was published in 2013, an age ago in the software industry!). But despite its age, DQN carries some historical significance as the original paper by Mnih et al. was the first paper to successfully combine deep learning (the use of deep neural networks) with reinforcement learning. Furthermore, some of the ideas from this paper are still valuable today.\\n\\nThe paper specifically showed how a reinforcement learning agent can learn how to play Atari games, directly from pixels. In other words, the algorithm directly takes images as input, just like a human would see them, and outputs which actions to take to successfully play these games.\\n\\n\\nAlthough the idea of Q-learning was not new at the time, Q-learning hadn’t been directly applied before to high-dimensional inputs (like images). Of course, the algorithm was not perfect yet. DQN had many similar problems as the ones we have today, such as sample inefficiency, but it heralded a new age for reinforcement learning, the age of Deep Reinforcement Learning.\\n\\n\\nIn the picture above, you can see a performance comparison of various versions of DQN that have come out since the original. Even to this day, these algorithms remain a popular choice for model-free, off-policy RL.\\n\\nWhat is Q-Learning?\\nBefore we dive into the specifics of DQN, let’s first talk about Q-learning in general.\\n\\nIn the previous blog posts we focussed on policy gradient algorithms. Policy gradient algorithms try to optimise the policy/behaviour of an agent by directly changing the policy. This, however, is not the case for Q-learning algorithms. Q-learning algorithms try to exploit the fact that: if we know the Q-function, we can construct an optimal policy by selecting the action at every state that has the highest Q-value.\\n\\nLet’s back up for a bit and go over some of the terminology. We are still operating in the typical RL framework: we have an agent taking actions in an environment at every discrete timestep. The agent starts in a certain state (or receives an observation of the state). After taking an action, the agent receives a new observation, a (possibly negative) reward, and the agent will be in a new state or receives a new observation.\\n\\n\\nThe goal of reinforcement learning is to learn a policy (a behaviour) that maximizes the agent’s expected cumulative reward. Or in other words, we want the agent to behave in a way such that the reward it receives over time is maximal.\\n\\nIn previous blog posts, we have talked about the value function:\\n\\n\\nThe value function tells us how valuable it is to be in state s when following policy π. It is equal to the summed reward of the trajectory that is expected to follow from state s.\\n\\n\\nA closely related function, is the Q-function:\\n\\n\\nThe Q-function tells us how valuable it is to be in state s and take action a while following policy π.\\n\\n\\nNow let’s say that we denote the Q-function for an optimal policy (a policy that maximises the cumulative reward) as Q*. The optimal Q-function needs to adhere to one important property, the Bellman equation:\\n\\n\\nThe Bellman equation tells us that, for the optimal policy, the value of taking action a while in state s, is equal to the immediate reward r, plus the value of taking the best possible action in the next state (multiplied by discount factor gamma). In other words, the Bellman equation is a recursive formula, the value of taking an action is equal to the reward you get for that action plus following the optimal policy from then on.\\n\\nThe idea behind Q-learning is that we can learn or approximate the optimal Q-function by iteratively updating our estimate of it based on the Bellman equation.\\n\\nIf you haven’t quite caught on to the idea yet, don’t worry, things will probably become more clear in the next section. There we will look at this idea from a more practical point of view.\\n\\nDQN Explained\\nSo, our goal is to find a good approximation of the Q-function. If we can do this, we can make our agent behave optimally by selecting the action that produces the highest Q-value (gives the highest reward) at every state.\\n\\nA rough outline of the algorithm would look like this:\\n\\nInitialize Q-function approximation\\nRepeat:\\n    Collect experience\\n    Update Q-function approximation\\nAs you can tell, we repeatedly switch between collecting new experience and updating our approximation of the q-function. We switch because crucially the states and action we will see, also depend on the policy/behaviour of our agent. We may only encounter certain states after we have already learned certain strategies.\\n\\nQ-function representation\\nIn order to make an estimate of the Q-function, we will need to represent it somehow. In earlier versions of Q-learning, people tried to keep track of a table that contained the exact reward for every state and action that the agent took. In practice this is often infeasible, especially when our state is highly dimensional such as when we use images for representing the state (remember, images are just matrices/tensors with pixel intensity values). One of the innovations the original DQN paper brought, was using a neural network to approximate the Q-function.\\n\\n\\nIn theory the neural network should take a state and an action as input and output the corresponding Q-value. In practice, we will only use the state as input and output a Q-value for each possible action. This makes the learning process a bit easier and saves some computation costs.\\n\\nInitially the neural network weights will be initialised with random values, over time we update the values for a more accurate estimate based on the collected experience.\\n\\nReplay memory\\nAs is mentioned above, we continuously collect new experience. An advantage of DQN is that it is an off-policy algorithm, meaning that it can learn from collected experience, even if that experience was collected with an older version of the policy.\\n\\n\\nCollecting experience happens by letting our agent operate in the environment and storing so-called transitions in a replay buffer/replay memory. A transition refers to the combination of a state, action, next state, reward and an indication whether the episode was over after this transition.\\n\\nDuring the learning process the buffer will be filled with transitions, while we update our q-function estimate based on the already collected transitions. Over time, older transitions will be replaced with new ones once the maximum capacity of the memory is reached.\\n\\nSome variations of DQN use prioritised replay memory, where transitions that are more significant for achieving high reward are more often sampled from the memory.\\n\\nQ-function update\\nFor updating our q-function estimate, we make use of the Bellman equation. We make a prediction of the state from our transition and for the next state in our transition. We then calculate the gradient based on the difference between our Q-value estimate for state sₜ and the received reward plus the maximum Q-value prediction of the next state sₜ₊₁(multiplied by a discount factor).\\n\\n\\nNote that, in order to do the gradient update for the q-function, we need to mask out the predicted values for the actions that weren’t actually taken in the sampled transition.\\n\\nFor example, if we fetch a (non-terminal) transition from our replay memory that contains the following values: state s₇, action a₃, next state s₈ and reward 10. Our Q-value estimate for (s₇, a₃) might be 20 and the maximum Q-value estimate (the highest value for any of the actions) for s₈ may be 8. If we use the mean-squared error, the loss would be 400–324. We get 400 by taking the second power of the estimate for s₇ and a₃ (20²) and we get 324 by taking the second power of the estimate for the highest Q-value for s₈ plus the actual observed reward ((8+10)²).\\n\\nThe loss we just calculated is only valid for action a₃ and not for the other Q-values produced by our neural network, so we want to mask out those other values/gradients, such that we don’t influence the variables in the neural network used to estimate the Q-values of the other actions.\\n\\nEpsilon-greedy policy\\nWe are now close to having all the ingredients needed for our algorithm. One thing that is still missing, is a way for deciding which actions our agent should take during the process of collecting experience. This is the classical exploration-exploitation trade-off. On one hand, we can let our agent take random actions to discover new strategies (exploration), but if we only act randomly, we don’t use the knowledge we have already gained from previous experience. On the other hand, if we only use what we have already learned in the past (exploitation), we won’t discover any new strategies. Hence, the trade-off.\\n\\nSome algorithms deal with this trade-off in a natural way. For example, some policy gradient algorithms output a distribution over actions and from time to time actions get selected that are not optimal according to the algorithm. DQN on the other hand, deals with this trade-off very explicitly by using a so-called epsilon-greedy strategy. Despite the fancy name, the idea is rather simple. We will define a value epsilon ϵ. Every time we need to choose an action, we will generate a new value. If that value is greater than epsilon ϵ, we will select the action that is optimal according to our Q-function (the action with the highest Q-value should give the highest reward). If the generated value is smaller than epsilon ϵ on the other hand, we will select a random value.\\n\\nDuring the course of the training, we can decrease the value of epsilon ϵ, such that our algorithm will do more exploitation over time.\\n\\nPutting it all together\\nAlright, with all this knowledge in mind, I can now present the algorithm as it was described by the original paper. Let’s take a look and go over it.\\n\\n\\nIn the first lines, we initialise our empty replay memory with capacity N and an initially random Q-function. The capacity N refers to the maximum number of transitions that will be stored in this memory.\\n\\nWe then start from an initial state (referred to as a “sequence” in the algorithm). The original authors also talk about “preprocessed sequences”, the preprocessed part refers to the fact that some states/sequences might need some preprocessing before they are fed into the neural network. For example, in the case of a game, they stack several images of the game together, because a single image is not enough to determine how certain objects are moving (see image).\\n\\n\\nNext, we perform an action based on the epsilon-greedy strategy (either random or based on the Q-function estimate), and we store all the information we get from taking that action. The stored information contains the state, the action taken, the reward and the next state.\\n\\nWe then go on to sample some of the transitions from our replay memory and use the formula described above to calculate the loss. Note that for a terminal state (a transition where the episode is ended), solely the reward is used as a target instead.\\n\\nWe then perform a gradient step (update the weights of our neural network) and we repeat this until we are satisfied with the results. That’s it! You now have a solid understanding of the basics of DQN.\\n\\nQ-Learning VS. Policy Gradients\\nAs a short final note, I want to comment a bit on the difference between Q-learning and the algorithms we previously mentioned (Policy Gradient algorithms).\\n\\nPolicy Gradient algorithms are sometimes regarded as being more stable. They are principled because we are directly optimising the policy. This is not the case for Q-learning, in the case of Q-learning we are constructing a policy by selecting an action for every state based on our estimate of the Q-function.\\n\\nThe advantage of Q-learning is that it can be used in an off-policy way. Since we store transitions in a buffer, we can later on reuse transitions for which the actions haven’t necessarily been generated by the latest policy. This makes Q-learning more sample-efficient, since we need to interact less with the environment.\\n\\nBoth methods come with their own trade-offs, and some of the methods we will explore later on, try to achieve the best of both worlds. Stay tuned if you want to learn more about this.\\n\\nConclusion\\nCongratulations for making it all the way to the end of this article! You should now have a solid understanding of Q-learning and DQN. This knowledge will be valuable when we explore even more advanced algorithms in the future. In the next part of this article, I will present an implementation of DQN. The implementation will be built further upon the educative RL framework we are developing for this blog (see the REINFORCE-implementation).', 'Hi and welcome to the first part of a series on Reinforcement Learning.\\n\\n\\nIf you somehow ended up here without having heard of Reinforcement Learning (RL) before, then let me summarize it as follows: “RL is a general framework for training an artificial intelligence model to solve a certain task or goal” … or in layman’s terms, we make AI do cool things!\\n\\nThe goal of this blog series is to learn about RL and simultaneously explore some of the more recent research later on. We will start from the very basics and work our way towards more advanced topics. Even if you have almost no prior programming and/or mathematics knowledge, you should be able to follow along pretty smoothly.\\n\\nThe first mini-series will be split into four parts:\\n\\nPart 1: What is Reinforcement learning?\\nPart 2: RL terminology and formal concepts\\nPart 3: The REINFORCE algorithm\\nPart 4: Implementing the REINFORCE algorithm\\nAt the same time, this mini-series will be the introduction to future posts with increasing complexity. Feel free to skip to the next part if you are already familiar with the content.\\n\\nNote: If you are reading this on a smartphone browser, you might not be able to view the subscripts. You can download the Medium app to mitigate this.\\n\\nThat sounds cool! … but what can I do with RL?\\nReinforcement learning is a framework to learn any task. In theory, RL can solve any problem that is phrased as a Markov Decision Process. We will explain what that means later on. For now, let’s have a look at some successful applications. If you enjoy these examples, please be sure to also check out the work from the original authors.\\n\\nGeneral\\nOne of the videos I love showing is the hide and seek video from OpenAI. The video is a nice example of how RL can help us with finding novel solutions to problems, without explicitly programming tactics or solution methods.\\n\\n\\nGames\\nOne of the early successes of Deep RL (which is combining RL with neural networks), was the ability to learn how to play Atari games, straight from pixels. Later on, researchers took it upon themselves to not only evaluate RL on the (relatively) simple Atari games, but rather evaluate it on the hardest competitive games out there. The assumption here is, that if RL can solve these complex games, it can also generalize to challenging real-world settings. As an example, this is Deepmind’s AlphaStar taking on a pro-gamer in the game StarCraft 2.\\n\\n\\nRobotics\\nSolving tasks in simulations and video games is one thing, but what about real life? Another popular field where RL is often applied (or at least holds great promise), is robotics. Robotics are significantly harder than simulations for various reasons. Think for example about the time it takes to repeatedly make a robot try out a certain action. Or think about the safety requirements involved for robotics. In the example below, you can see how the ANYmal robot from the Robotics System Lab in Zürich learned to recover from a fall.\\n\\n\\nReal world examples\\nRL can be applied to many other domains than the ones I just mentioned. Advertising, finance, healthcare, … just to name a few. As a final example, I present a goal-oriented chatbot, trained to negotiate about sales (source: https://siddharthverma314.github.io/research/chai-acl-2022/ ).\\n\\n\\nRL: The basics\\nA description of the RL framework is as follows: We have an agent that tries to solve a task in a certain environment. The concept of agent should be taken very broadly here, an agent can be a robot, a chatbot, a virtual character, etc. . At every timestep t, the agent needs to choose an action a. After this action it might receive a reward r and we get a new observation of its state s. The new state can be determined both by the action of the agent and also by the environment the agent is operating in.\\n\\nThe RL problem is trying to maximize the cumulative reward the agent gets over time.\\n\\n\\nImagine our agent is a monkey and the task we want the monkey to solve, is to pick up as many bananas as possible. At every timestep, the monkey needs to decide to take an action. The actions could be to step towards the tree, grab something, climb, … Perhaps the reward at every timestep can be defined as the number of bananas the monkey got at that timestep. After every action, the monkey will also be in a new state. Maybe we define the state of the monkey as its position in the world. So when the monkey takes a step, the state at the next timestep would be the coordinates of the monkey at the next timestep. We are now searching for the optimal behavior, the best sequence of actions the monkey can take, to maximize the cumulative number of bananas it will get.\\n\\nHow does RL fit in the bigger picture?\\nYou might look at this framework and think: “Hey, isn’t that exactly what people are already studying in the field of …?”. And in fact, you might be right.\\n\\nIn other domains like engineering or mathematics, people have often been studying the same problems with different names and methods. Or in fields like neuroscience and psychology, some similarities can be found in the way our brain “rewards” us by releasing dopamine.\\n\\nThe likely reason for this intersection of domains, is that reinforcement learning is the study of a fundamental problem. It is essentially the science of decision taking. In these series, we will be looking at it from the umbrella of computer science and machine learning.\\n\\n\\nThis general applicability is also what makes RL so interesting to me personally. RL is one of the potential technologies that could get us closer to general AI: an AI system that can solve any task, in contrast to a narrow set of tasks. There are also other technologies (e.g. big language models, graph neural networks, …) that are making some strides in this regard, but the problem statement of RL in particular seems to be the most ambitious.\\n\\nAnother way of situating RL, is by looking at how it compares to other learning paradigms. Within the field of machine learning, people often distinguish between Supervised learning, Unsupervised learning and Reinforcement learning.\\n\\n\\nWhen it comes to Supervised learning, we are essentially trying to learn a function, a mapping from X to Y. Our dataset consists of samples X and during training we provide the AI system with labels Y of what these samples should map to. Our AI model is successful when it correctly predicts a label y given an (unseen) sample x.\\n\\nAs an example, say we have a dataset consisting of cat and dog images. Each sample (image) has a label, stating whether the image contains a “dog” or a “cat”. The goal of our supervised model, is now to learn how to map a dog-image to the label “dog” and a cat-image to the label “cat”. After it has learned this mapping, the hope is that this AI model can repeat this process for new images that it has not seen before.\\n\\nIn an Unsupervised learning context, we no longer provide any labels Y. So the task for the AI system now becomes learning some general statistics about the dataset. We could for example give an AI the task to generate a new sample, similar to the ones seen in the dataset. In this case the model would be considered successful if it manages to correctly learn the interesting characteristics of a dataset.\\n\\nReinforcement Learning is rather different from the previous paradigms. In the case of RL, we consider an agent that is actively interacting with an environment. Through its interactions, it is possible that it may influence the environment that it operates in. The “dataset” we need to consider here, are the actions our agent took and the accumulated rewards it got by taking those actions. An added difficulty here is that our dataset is non-static. Say that our agent acts in a certain way, we can then collect some data of the actions our agent took and we can try to optimize these (e.g. do more of the actions that led to a successful result). But as a result of this optimization, we have now changed the behavior of this agent, and thus we will need to collect new data to see how well our agent fares now.\\n\\nIf RL is so great, then why isn’t everyone using RL?\\nAfter reading all this, you might be wondering why people aren’t using RL to solve all imaginable problems. The truth is that even though the field has made a lot of advancements in the last few years, there are still a few fundamental problems to be solved. Progress is being made all the time, but to give you an idea of what you might encounter, I’ll list a few common ones.\\n\\nSample (in-)efficiency\\nIt is generally known that RL is very sample-inefficient. We regard a “sample” as an interaction with the environment. RL needs a lot of samples/interactions to be able to solve a task. In this sense, RL is very inefficient compared to humans, for example, it doesn’t take a human dozens of hours to learn how to play an Atari game.\\n\\nThis sample-efficiency can in part be explained by the fact that humans can leverage a lot of their previous knowledge (priors) when they encounter a new task. A human can for example reuse some of the knowledge and skills of previous games and/or concepts they already acquired from other experiences throughout their life. An RL-agent in contrast, starts the learning process without any assumptions.\\n\\nAnother thing to mention is that leveraging knowledge from previous tasks is also an active research topic. I’ll just put one example (out of many) here to give you an idea.\\n\\nDeepmind Gato\\nGoogle Jump-Start RL\\n\\nThe exploration-exploitation trade-off\\nWhile the previous problem sounds more like an engineering effort (it’s not), the exploration-exploitation trade-off seems more fundamental. Whenever we train an RL-agent, the agent will need some time to explore, it needs to take some actions that it hasn’t taken before, in order to discover how to solve the problem. On the other hand, we can’t let the agent always take random actions, because these random actions might lead to nothing. Sometimes we want the agent to leverage what it has already learned to try and optimize further. This is the exploration-exploitation trade-off, we want an automated way to strike a good balance between letting the agent explore and taking actions for which it already knows what they will lead to.\\n\\nFor a lot problems, it is quite possible that the agent gets stuck in a local optimum.\\n\\n\\nThe exploration-exploitation trade-off sounds very much tractable at first, but it turns out to be one of the hardest problems for RL to solve. To give you an idea about how hard this problem is: The problem was originally considered by the scientists of the allied forces, but was suggested to be dropped over to Germany, because it was deemed so intractable that they wanted the German scientists to also waste their time on it.\\n\\nNo silver bullet for this problem has been found and there are many people working on various solutions. I will leave a link here to a previously proposed solution called “curiosity-driven exploration” which I found particularly interesting.\\n\\nCuriosity-driven exploration by Self-supervised Prediction\\n\\nThe sparse-reward problem\\nAnother rather fundamental problem, is the so called Sparse-reward problem. As the name implies, this problem occurs when our RL-agent receives so little rewards, that it actually gets no feedback on how it should improve.\\n\\nImagine for example this mountain car. The agent needs to move the car left and right, such that it gets enough momentum to reach the top. Initially though, the agent doesn’t know that it needs to move the car back and forth to reach the top. If we only give our agent a reward (a positive feedback signal) when we have reached the flag, it might not ever get a positive feedback signal, simply because it might never reach the flag by taking random actions (exploration).\\n\\n\\nSomething commonly done to counteract this problem, is by “reward shaping”. We will modify the reward such that the agent gets more feedback signals to learn from. In case of the mountain car, we could for example also give the agent a positive reward based on the speed or altitude it achieves. However, reward shaping is not a scalable solution. Luckily other solutions are being sought after.\\n\\nI’m leaving another example here which I think is an interesting (but not generally applicable approach) for counteracting sparse rewards.\\n\\nHindsight Experience Replay\\n\\nThe aforementioned problems are just some prominent examples, but there are in fact many more problems that are being looked into by some of the brightest minds out there. The good news is that regular and steady progress is being made.\\n\\nConclusion\\nPhew, that was a lot of information in a very short period, but you made it through! This first introduction should give you a good idea of what RL is all about.']], 'uris': None, 'data': None}\n"
     ]
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\n",
    "        \"This is a query about machine learning and data science\"\n",
    "    ],\n",
    "    n_results=2\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the result.  Note the data structure is a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': None,\n",
      " 'distances': [[1.4940412090843298, 1.6182181858429838]],\n",
      " 'documents': [['Hi and welcome to the intro series on Reinforcement Learning, '\n",
      "                'today’s topic will be about the DQN algorithm!\\n'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'This blog post is part of a longer series about Reinforcement '\n",
      "                'Learning (RL). If you are completely unfamiliar with RL, I '\n",
      "                'suggest you read my previous blog posts first.\\n'\n",
      "                '\\n'\n",
      "                'Previously we talked about policy gradient algorithms. Today '\n",
      "                'we will have a look at a different family of RL algorithms: '\n",
      "                'Q-learning algorithms. And more specifically, we will focus '\n",
      "                'on the vanilla DQN-algorithm.\\n'\n",
      "                '\\n'\n",
      "                'The topics for today’s blog post are:\\n'\n",
      "                '\\n'\n",
      "                'Historical significance of DQN\\n'\n",
      "                'What is Q-Learning?\\n'\n",
      "                'DQN Explained\\n'\n",
      "                'Q-Learning VS. Policy Gradients\\n'\n",
      "                'There will also be a part 2 for today’s blog post, which will '\n",
      "                'include a basic implementation of DQN.\\n'\n",
      "                '\\n'\n",
      "                'Note: If you are reading this on a smartphone browser, you '\n",
      "                'might not be able to view the subscripts. You can download '\n",
      "                'the Medium app to mitigate this.\\n'\n",
      "                '\\n'\n",
      "                'Historical significance of DQN\\n'\n",
      "                'The algorithm we will discuss today is called DQN. You might '\n",
      "                'wonder why we would discuss such an old algorithm at this '\n",
      "                'point, and you’d be right about the fact that DQN is in fact '\n",
      "                'quite old by now (the paper was published in 2013, an age ago '\n",
      "                'in the software industry!). But despite its age, DQN carries '\n",
      "                'some historical significance as the original paper by Mnih et '\n",
      "                'al. was the first paper to successfully combine deep learning '\n",
      "                '(the use of deep neural networks) with reinforcement '\n",
      "                'learning. Furthermore, some of the ideas from this paper are '\n",
      "                'still valuable today.\\n'\n",
      "                '\\n'\n",
      "                'The paper specifically showed how a reinforcement learning '\n",
      "                'agent can learn how to play Atari games, directly from '\n",
      "                'pixels. In other words, the algorithm directly takes images '\n",
      "                'as input, just like a human would see them, and outputs which '\n",
      "                'actions to take to successfully play these games.\\n'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'Although the idea of Q-learning was not new at the time, '\n",
      "                'Q-learning hadn’t been directly applied before to '\n",
      "                'high-dimensional inputs (like images). Of course, the '\n",
      "                'algorithm was not perfect yet. DQN had many similar problems '\n",
      "                'as the ones we have today, such as sample inefficiency, but '\n",
      "                'it heralded a new age for reinforcement learning, the age of '\n",
      "                'Deep Reinforcement Learning.\\n'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'In the picture above, you can see a performance comparison of '\n",
      "                'various versions of DQN that have come out since the '\n",
      "                'original. Even to this day, these algorithms remain a popular '\n",
      "                'choice for model-free, off-policy RL.\\n'\n",
      "                '\\n'\n",
      "                'What is Q-Learning?\\n'\n",
      "                'Before we dive into the specifics of DQN, let’s first talk '\n",
      "                'about Q-learning in general.\\n'\n",
      "                '\\n'\n",
      "                'In the previous blog posts we focussed on policy gradient '\n",
      "                'algorithms. Policy gradient algorithms try to optimise the '\n",
      "                'policy/behaviour of an agent by directly changing the policy. '\n",
      "                'This, however, is not the case for Q-learning algorithms. '\n",
      "                'Q-learning algorithms try to exploit the fact that: if we '\n",
      "                'know the Q-function, we can construct an optimal policy by '\n",
      "                'selecting the action at every state that has the highest '\n",
      "                'Q-value.\\n'\n",
      "                '\\n'\n",
      "                'Let’s back up for a bit and go over some of the terminology. '\n",
      "                'We are still operating in the typical RL framework: we have '\n",
      "                'an agent taking actions in an environment at every discrete '\n",
      "                'timestep. The agent starts in a certain state (or receives an '\n",
      "                'observation of the state). After taking an action, the agent '\n",
      "                'receives a new observation, a (possibly negative) reward, and '\n",
      "                'the agent will be in a new state or receives a new '\n",
      "                'observation.\\n'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'The goal of reinforcement learning is to learn a policy (a '\n",
      "                'behaviour) that maximizes the agent’s expected cumulative '\n",
      "                'reward. Or in other words, we want the agent to behave in a '\n",
      "                'way such that the reward it receives over time is maximal.\\n'\n",
      "                '\\n'\n",
      "                'In previous blog posts, we have talked about the value '\n",
      "                'function:\\n'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'The value function tells us how valuable it is to be in state '\n",
      "                's when following policy π. It is equal to the summed reward '\n",
      "                'of the trajectory that is expected to follow from state s.\\n'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'A closely related function, is the Q-function:\\n'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'The Q-function tells us how valuable it is to be in state s '\n",
      "                'and take action a while following policy π.\\n'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'Now let’s say that we denote the Q-function for an optimal '\n",
      "                'policy (a policy that maximises the cumulative reward) as Q*. '\n",
      "                'The optimal Q-function needs to adhere to one important '\n",
      "                'property, the Bellman equation:\\n'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'The Bellman equation tells us that, for the optimal policy, '\n",
      "                'the value of taking action a while in state s, is equal to '\n",
      "                'the immediate reward r, plus the value of taking the best '\n",
      "                'possible action in the next state (multiplied by discount '\n",
      "                'factor gamma). In other words, the Bellman equation is a '\n",
      "                'recursive formula, the value of taking an action is equal to '\n",
      "                'the reward you get for that action plus following the optimal '\n",
      "                'policy from then on.\\n'\n",
      "                '\\n'\n",
      "                'The idea behind Q-learning is that we can learn or '\n",
      "                'approximate the optimal Q-function by iteratively updating '\n",
      "                'our estimate of it based on the Bellman equation.\\n'\n",
      "                '\\n'\n",
      "                'If you haven’t quite caught on to the idea yet, don’t worry, '\n",
      "                'things will probably become more clear in the next section. '\n",
      "                'There we will look at this idea from a more practical point '\n",
      "                'of view.\\n'\n",
      "                '\\n'\n",
      "                'DQN Explained\\n'\n",
      "                'So, our goal is to find a good approximation of the '\n",
      "                'Q-function. If we can do this, we can make our agent behave '\n",
      "                'optimally by selecting the action that produces the highest '\n",
      "                'Q-value (gives the highest reward) at every state.\\n'\n",
      "                '\\n'\n",
      "                'A rough outline of the algorithm would look like this:\\n'\n",
      "                '\\n'\n",
      "                'Initialize Q-function approximation\\n'\n",
      "                'Repeat:\\n'\n",
      "                '    Collect experience\\n'\n",
      "                '    Update Q-function approximation\\n'\n",
      "                'As you can tell, we repeatedly switch between collecting new '\n",
      "                'experience and updating our approximation of the q-function. '\n",
      "                'We switch because crucially the states and action we will '\n",
      "                'see, also depend on the policy/behaviour of our agent. We may '\n",
      "                'only encounter certain states after we have already learned '\n",
      "                'certain strategies.\\n'\n",
      "                '\\n'\n",
      "                'Q-function representation\\n'\n",
      "                'In order to make an estimate of the Q-function, we will need '\n",
      "                'to represent it somehow. In earlier versions of Q-learning, '\n",
      "                'people tried to keep track of a table that contained the '\n",
      "                'exact reward for every state and action that the agent took. '\n",
      "                'In practice this is often infeasible, especially when our '\n",
      "                'state is highly dimensional such as when we use images for '\n",
      "                'representing the state (remember, images are just '\n",
      "                'matrices/tensors with pixel intensity values). One of the '\n",
      "                'innovations the original DQN paper brought, was using a '\n",
      "                'neural network to approximate the Q-function.\\n'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'In theory the neural network should take a state and an '\n",
      "                'action as input and output the corresponding Q-value. In '\n",
      "                'practice, we will only use the state as input and output a '\n",
      "                'Q-value for each possible action. This makes the learning '\n",
      "                'process a bit easier and saves some computation costs.\\n'\n",
      "                '\\n'\n",
      "                'Initially the neural network weights will be initialised with '\n",
      "                'random values, over time we update the values for a more '\n",
      "                'accurate estimate based on the collected experience.\\n'\n",
      "                '\\n'\n",
      "                'Replay memory\\n'\n",
      "                'As is mentioned above, we continuously collect new '\n",
      "                'experience. An advantage of DQN is that it is an off-policy '\n",
      "                'algorithm, meaning that it can learn from collected '\n",
      "                'experience, even if that experience was collected with an '\n",
      "                'older version of the policy.\\n'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'Collecting experience happens by letting our agent operate in '\n",
      "                'the environment and storing so-called transitions in a replay '\n",
      "                'buffer/replay memory. A transition refers to the combination '\n",
      "                'of a state, action, next state, reward and an indication '\n",
      "                'whether the episode was over after this transition.\\n'\n",
      "                '\\n'\n",
      "                'During the learning process the buffer will be filled with '\n",
      "                'transitions, while we update our q-function estimate based on '\n",
      "                'the already collected transitions. Over time, older '\n",
      "                'transitions will be replaced with new ones once the maximum '\n",
      "                'capacity of the memory is reached.\\n'\n",
      "                '\\n'\n",
      "                'Some variations of DQN use prioritised replay memory, where '\n",
      "                'transitions that are more significant for achieving high '\n",
      "                'reward are more often sampled from the memory.\\n'\n",
      "                '\\n'\n",
      "                'Q-function update\\n'\n",
      "                'For updating our q-function estimate, we make use of the '\n",
      "                'Bellman equation. We make a prediction of the state from our '\n",
      "                'transition and for the next state in our transition. We then '\n",
      "                'calculate the gradient based on the difference between our '\n",
      "                'Q-value estimate for state sₜ and the received reward plus '\n",
      "                'the maximum Q-value prediction of the next state '\n",
      "                'sₜ₊₁(multiplied by a discount factor).\\n'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'Note that, in order to do the gradient update for the '\n",
      "                'q-function, we need to mask out the predicted values for the '\n",
      "                'actions that weren’t actually taken in the sampled '\n",
      "                'transition.\\n'\n",
      "                '\\n'\n",
      "                'For example, if we fetch a (non-terminal) transition from our '\n",
      "                'replay memory that contains the following values: state s₇, '\n",
      "                'action a₃, next state s₈ and reward 10. Our Q-value estimate '\n",
      "                'for (s₇, a₃) might be 20 and the maximum Q-value estimate '\n",
      "                '(the highest value for any of the actions) for s₈ may be 8. '\n",
      "                'If we use the mean-squared error, the loss would be 400–324. '\n",
      "                'We get 400 by taking the second power of the estimate for s₇ '\n",
      "                'and a₃ (20²) and we get 324 by taking the second power of the '\n",
      "                'estimate for the highest Q-value for s₈ plus the actual '\n",
      "                'observed reward ((8+10)²).\\n'\n",
      "                '\\n'\n",
      "                'The loss we just calculated is only valid for action a₃ and '\n",
      "                'not for the other Q-values produced by our neural network, so '\n",
      "                'we want to mask out those other values/gradients, such that '\n",
      "                'we don’t influence the variables in the neural network used '\n",
      "                'to estimate the Q-values of the other actions.\\n'\n",
      "                '\\n'\n",
      "                'Epsilon-greedy policy\\n'\n",
      "                'We are now close to having all the ingredients needed for our '\n",
      "                'algorithm. One thing that is still missing, is a way for '\n",
      "                'deciding which actions our agent should take during the '\n",
      "                'process of collecting experience. This is the classical '\n",
      "                'exploration-exploitation trade-off. On one hand, we can let '\n",
      "                'our agent take random actions to discover new strategies '\n",
      "                '(exploration), but if we only act randomly, we don’t use the '\n",
      "                'knowledge we have already gained from previous experience. On '\n",
      "                'the other hand, if we only use what we have already learned '\n",
      "                'in the past (exploitation), we won’t discover any new '\n",
      "                'strategies. Hence, the trade-off.\\n'\n",
      "                '\\n'\n",
      "                'Some algorithms deal with this trade-off in a natural way. '\n",
      "                'For example, some policy gradient algorithms output a '\n",
      "                'distribution over actions and from time to time actions get '\n",
      "                'selected that are not optimal according to the algorithm. DQN '\n",
      "                'on the other hand, deals with this trade-off very explicitly '\n",
      "                'by using a so-called epsilon-greedy strategy. Despite the '\n",
      "                'fancy name, the idea is rather simple. We will define a value '\n",
      "                'epsilon ϵ. Every time we need to choose an action, we will '\n",
      "                'generate a new value. If that value is greater than epsilon '\n",
      "                'ϵ, we will select the action that is optimal according to our '\n",
      "                'Q-function (the action with the highest Q-value should give '\n",
      "                'the highest reward). If the generated value is smaller than '\n",
      "                'epsilon ϵ on the other hand, we will select a random value.\\n'\n",
      "                '\\n'\n",
      "                'During the course of the training, we can decrease the value '\n",
      "                'of epsilon ϵ, such that our algorithm will do more '\n",
      "                'exploitation over time.\\n'\n",
      "                '\\n'\n",
      "                'Putting it all together\\n'\n",
      "                'Alright, with all this knowledge in mind, I can now present '\n",
      "                'the algorithm as it was described by the original paper. '\n",
      "                'Let’s take a look and go over it.\\n'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'In the first lines, we initialise our empty replay memory '\n",
      "                'with capacity N and an initially random Q-function. The '\n",
      "                'capacity N refers to the maximum number of transitions that '\n",
      "                'will be stored in this memory.\\n'\n",
      "                '\\n'\n",
      "                'We then start from an initial state (referred to as a '\n",
      "                '“sequence” in the algorithm). The original authors also talk '\n",
      "                'about “preprocessed sequences”, the preprocessed part refers '\n",
      "                'to the fact that some states/sequences might need some '\n",
      "                'preprocessing before they are fed into the neural network. '\n",
      "                'For example, in the case of a game, they stack several images '\n",
      "                'of the game together, because a single image is not enough to '\n",
      "                'determine how certain objects are moving (see image).\\n'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'Next, we perform an action based on the epsilon-greedy '\n",
      "                'strategy (either random or based on the Q-function estimate), '\n",
      "                'and we store all the information we get from taking that '\n",
      "                'action. The stored information contains the state, the action '\n",
      "                'taken, the reward and the next state.\\n'\n",
      "                '\\n'\n",
      "                'We then go on to sample some of the transitions from our '\n",
      "                'replay memory and use the formula described above to '\n",
      "                'calculate the loss. Note that for a terminal state (a '\n",
      "                'transition where the episode is ended), solely the reward is '\n",
      "                'used as a target instead.\\n'\n",
      "                '\\n'\n",
      "                'We then perform a gradient step (update the weights of our '\n",
      "                'neural network) and we repeat this until we are satisfied '\n",
      "                'with the results. That’s it! You now have a solid '\n",
      "                'understanding of the basics of DQN.\\n'\n",
      "                '\\n'\n",
      "                'Q-Learning VS. Policy Gradients\\n'\n",
      "                'As a short final note, I want to comment a bit on the '\n",
      "                'difference between Q-learning and the algorithms we '\n",
      "                'previously mentioned (Policy Gradient algorithms).\\n'\n",
      "                '\\n'\n",
      "                'Policy Gradient algorithms are sometimes regarded as being '\n",
      "                'more stable. They are principled because we are directly '\n",
      "                'optimising the policy. This is not the case for Q-learning, '\n",
      "                'in the case of Q-learning we are constructing a policy by '\n",
      "                'selecting an action for every state based on our estimate of '\n",
      "                'the Q-function.\\n'\n",
      "                '\\n'\n",
      "                'The advantage of Q-learning is that it can be used in an '\n",
      "                'off-policy way. Since we store transitions in a buffer, we '\n",
      "                'can later on reuse transitions for which the actions haven’t '\n",
      "                'necessarily been generated by the latest policy. This makes '\n",
      "                'Q-learning more sample-efficient, since we need to interact '\n",
      "                'less with the environment.\\n'\n",
      "                '\\n'\n",
      "                'Both methods come with their own trade-offs, and some of the '\n",
      "                'methods we will explore later on, try to achieve the best of '\n",
      "                'both worlds. Stay tuned if you want to learn more about '\n",
      "                'this.\\n'\n",
      "                '\\n'\n",
      "                'Conclusion\\n'\n",
      "                'Congratulations for making it all the way to the end of this '\n",
      "                'article! You should now have a solid understanding of '\n",
      "                'Q-learning and DQN. This knowledge will be valuable when we '\n",
      "                'explore even more advanced algorithms in the future. In the '\n",
      "                'next part of this article, I will present an implementation '\n",
      "                'of DQN. The implementation will be built further upon the '\n",
      "                'educative RL framework we are developing for this blog (see '\n",
      "                'the REINFORCE-implementation).',\n",
      "                'Hi and welcome to the first part of a series on Reinforcement '\n",
      "                'Learning.\\n'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'If you somehow ended up here without having heard of '\n",
      "                'Reinforcement Learning (RL) before, then let me summarize it '\n",
      "                'as follows: “RL is a general framework for training an '\n",
      "                'artificial intelligence model to solve a certain task or '\n",
      "                'goal” … or in layman’s terms, we make AI do cool things!\\n'\n",
      "                '\\n'\n",
      "                'The goal of this blog series is to learn about RL and '\n",
      "                'simultaneously explore some of the more recent research later '\n",
      "                'on. We will start from the very basics and work our way '\n",
      "                'towards more advanced topics. Even if you have almost no '\n",
      "                'prior programming and/or mathematics knowledge, you should be '\n",
      "                'able to follow along pretty smoothly.\\n'\n",
      "                '\\n'\n",
      "                'The first mini-series will be split into four parts:\\n'\n",
      "                '\\n'\n",
      "                'Part 1: What is Reinforcement learning?\\n'\n",
      "                'Part 2: RL terminology and formal concepts\\n'\n",
      "                'Part 3: The REINFORCE algorithm\\n'\n",
      "                'Part 4: Implementing the REINFORCE algorithm\\n'\n",
      "                'At the same time, this mini-series will be the introduction '\n",
      "                'to future posts with increasing complexity. Feel free to skip '\n",
      "                'to the next part if you are already familiar with the '\n",
      "                'content.\\n'\n",
      "                '\\n'\n",
      "                'Note: If you are reading this on a smartphone browser, you '\n",
      "                'might not be able to view the subscripts. You can download '\n",
      "                'the Medium app to mitigate this.\\n'\n",
      "                '\\n'\n",
      "                'That sounds cool! … but what can I do with RL?\\n'\n",
      "                'Reinforcement learning is a framework to learn any task. In '\n",
      "                'theory, RL can solve any problem that is phrased as a Markov '\n",
      "                'Decision Process. We will explain what that means later on. '\n",
      "                'For now, let’s have a look at some successful applications. '\n",
      "                'If you enjoy these examples, please be sure to also check out '\n",
      "                'the work from the original authors.\\n'\n",
      "                '\\n'\n",
      "                'General\\n'\n",
      "                'One of the videos I love showing is the hide and seek video '\n",
      "                'from OpenAI. The video is a nice example of how RL can help '\n",
      "                'us with finding novel solutions to problems, without '\n",
      "                'explicitly programming tactics or solution methods.\\n'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'Games\\n'\n",
      "                'One of the early successes of Deep RL (which is combining RL '\n",
      "                'with neural networks), was the ability to learn how to play '\n",
      "                'Atari games, straight from pixels. Later on, researchers took '\n",
      "                'it upon themselves to not only evaluate RL on the '\n",
      "                '(relatively) simple Atari games, but rather evaluate it on '\n",
      "                'the hardest competitive games out there. The assumption here '\n",
      "                'is, that if RL can solve these complex games, it can also '\n",
      "                'generalize to challenging real-world settings. As an example, '\n",
      "                'this is Deepmind’s AlphaStar taking on a pro-gamer in the '\n",
      "                'game StarCraft 2.\\n'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'Robotics\\n'\n",
      "                'Solving tasks in simulations and video games is one thing, '\n",
      "                'but what about real life? Another popular field where RL is '\n",
      "                'often applied (or at least holds great promise), is robotics. '\n",
      "                'Robotics are significantly harder than simulations for '\n",
      "                'various reasons. Think for example about the time it takes to '\n",
      "                'repeatedly make a robot try out a certain action. Or think '\n",
      "                'about the safety requirements involved for robotics. In the '\n",
      "                'example below, you can see how the ANYmal robot from the '\n",
      "                'Robotics System Lab in Zürich learned to recover from a '\n",
      "                'fall.\\n'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'Real world examples\\n'\n",
      "                'RL can be applied to many other domains than the ones I just '\n",
      "                'mentioned. Advertising, finance, healthcare, … just to name a '\n",
      "                'few. As a final example, I present a goal-oriented chatbot, '\n",
      "                'trained to negotiate about sales (source: '\n",
      "                'https://siddharthverma314.github.io/research/chai-acl-2022/ '\n",
      "                ').\\n'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'RL: The basics\\n'\n",
      "                'A description of the RL framework is as follows: We have an '\n",
      "                'agent that tries to solve a task in a certain environment. '\n",
      "                'The concept of agent should be taken very broadly here, an '\n",
      "                'agent can be a robot, a chatbot, a virtual character, etc. . '\n",
      "                'At every timestep t, the agent needs to choose an action a. '\n",
      "                'After this action it might receive a reward r and we get a '\n",
      "                'new observation of its state s. The new state can be '\n",
      "                'determined both by the action of the agent and also by the '\n",
      "                'environment the agent is operating in.\\n'\n",
      "                '\\n'\n",
      "                'The RL problem is trying to maximize the cumulative reward '\n",
      "                'the agent gets over time.\\n'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'Imagine our agent is a monkey and the task we want the monkey '\n",
      "                'to solve, is to pick up as many bananas as possible. At every '\n",
      "                'timestep, the monkey needs to decide to take an action. The '\n",
      "                'actions could be to step towards the tree, grab something, '\n",
      "                'climb, … Perhaps the reward at every timestep can be defined '\n",
      "                'as the number of bananas the monkey got at that timestep. '\n",
      "                'After every action, the monkey will also be in a new state. '\n",
      "                'Maybe we define the state of the monkey as its position in '\n",
      "                'the world. So when the monkey takes a step, the state at the '\n",
      "                'next timestep would be the coordinates of the monkey at the '\n",
      "                'next timestep. We are now searching for the optimal behavior, '\n",
      "                'the best sequence of actions the monkey can take, to maximize '\n",
      "                'the cumulative number of bananas it will get.\\n'\n",
      "                '\\n'\n",
      "                'How does RL fit in the bigger picture?\\n'\n",
      "                'You might look at this framework and think: “Hey, isn’t that '\n",
      "                'exactly what people are already studying in the field of …?”. '\n",
      "                'And in fact, you might be right.\\n'\n",
      "                '\\n'\n",
      "                'In other domains like engineering or mathematics, people have '\n",
      "                'often been studying the same problems with different names '\n",
      "                'and methods. Or in fields like neuroscience and psychology, '\n",
      "                'some similarities can be found in the way our brain “rewards” '\n",
      "                'us by releasing dopamine.\\n'\n",
      "                '\\n'\n",
      "                'The likely reason for this intersection of domains, is that '\n",
      "                'reinforcement learning is the study of a fundamental problem. '\n",
      "                'It is essentially the science of decision taking. In these '\n",
      "                'series, we will be looking at it from the umbrella of '\n",
      "                'computer science and machine learning.\\n'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'This general applicability is also what makes RL so '\n",
      "                'interesting to me personally. RL is one of the potential '\n",
      "                'technologies that could get us closer to general AI: an AI '\n",
      "                'system that can solve any task, in contrast to a narrow set '\n",
      "                'of tasks. There are also other technologies (e.g. big '\n",
      "                'language models, graph neural networks, …) that are making '\n",
      "                'some strides in this regard, but the problem statement of RL '\n",
      "                'in particular seems to be the most ambitious.\\n'\n",
      "                '\\n'\n",
      "                'Another way of situating RL, is by looking at how it compares '\n",
      "                'to other learning paradigms. Within the field of machine '\n",
      "                'learning, people often distinguish between Supervised '\n",
      "                'learning, Unsupervised learning and Reinforcement learning.\\n'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'When it comes to Supervised learning, we are essentially '\n",
      "                'trying to learn a function, a mapping from X to Y. Our '\n",
      "                'dataset consists of samples X and during training we provide '\n",
      "                'the AI system with labels Y of what these samples should map '\n",
      "                'to. Our AI model is successful when it correctly predicts a '\n",
      "                'label y given an (unseen) sample x.\\n'\n",
      "                '\\n'\n",
      "                'As an example, say we have a dataset consisting of cat and '\n",
      "                'dog images. Each sample (image) has a label, stating whether '\n",
      "                'the image contains a “dog” or a “cat”. The goal of our '\n",
      "                'supervised model, is now to learn how to map a dog-image to '\n",
      "                'the label “dog” and a cat-image to the label “cat”. After it '\n",
      "                'has learned this mapping, the hope is that this AI model can '\n",
      "                'repeat this process for new images that it has not seen '\n",
      "                'before.\\n'\n",
      "                '\\n'\n",
      "                'In an Unsupervised learning context, we no longer provide any '\n",
      "                'labels Y. So the task for the AI system now becomes learning '\n",
      "                'some general statistics about the dataset. We could for '\n",
      "                'example give an AI the task to generate a new sample, similar '\n",
      "                'to the ones seen in the dataset. In this case the model would '\n",
      "                'be considered successful if it manages to correctly learn the '\n",
      "                'interesting characteristics of a dataset.\\n'\n",
      "                '\\n'\n",
      "                'Reinforcement Learning is rather different from the previous '\n",
      "                'paradigms. In the case of RL, we consider an agent that is '\n",
      "                'actively interacting with an environment. Through its '\n",
      "                'interactions, it is possible that it may influence the '\n",
      "                'environment that it operates in. The “dataset” we need to '\n",
      "                'consider here, are the actions our agent took and the '\n",
      "                'accumulated rewards it got by taking those actions. An added '\n",
      "                'difficulty here is that our dataset is non-static. Say that '\n",
      "                'our agent acts in a certain way, we can then collect some '\n",
      "                'data of the actions our agent took and we can try to optimize '\n",
      "                'these (e.g. do more of the actions that led to a successful '\n",
      "                'result). But as a result of this optimization, we have now '\n",
      "                'changed the behavior of this agent, and thus we will need to '\n",
      "                'collect new data to see how well our agent fares now.\\n'\n",
      "                '\\n'\n",
      "                'If RL is so great, then why isn’t everyone using RL?\\n'\n",
      "                'After reading all this, you might be wondering why people '\n",
      "                'aren’t using RL to solve all imaginable problems. The truth '\n",
      "                'is that even though the field has made a lot of advancements '\n",
      "                'in the last few years, there are still a few fundamental '\n",
      "                'problems to be solved. Progress is being made all the time, '\n",
      "                'but to give you an idea of what you might encounter, I’ll '\n",
      "                'list a few common ones.\\n'\n",
      "                '\\n'\n",
      "                'Sample (in-)efficiency\\n'\n",
      "                'It is generally known that RL is very sample-inefficient. We '\n",
      "                'regard a “sample” as an interaction with the environment. RL '\n",
      "                'needs a lot of samples/interactions to be able to solve a '\n",
      "                'task. In this sense, RL is very inefficient compared to '\n",
      "                'humans, for example, it doesn’t take a human dozens of hours '\n",
      "                'to learn how to play an Atari game.\\n'\n",
      "                '\\n'\n",
      "                'This sample-efficiency can in part be explained by the fact '\n",
      "                'that humans can leverage a lot of their previous knowledge '\n",
      "                '(priors) when they encounter a new task. A human can for '\n",
      "                'example reuse some of the knowledge and skills of previous '\n",
      "                'games and/or concepts they already acquired from other '\n",
      "                'experiences throughout their life. An RL-agent in contrast, '\n",
      "                'starts the learning process without any assumptions.\\n'\n",
      "                '\\n'\n",
      "                'Another thing to mention is that leveraging knowledge from '\n",
      "                'previous tasks is also an active research topic. I’ll just '\n",
      "                'put one example (out of many) here to give you an idea.\\n'\n",
      "                '\\n'\n",
      "                'Deepmind Gato\\n'\n",
      "                'Google Jump-Start RL\\n'\n",
      "                '\\n'\n",
      "                'The exploration-exploitation trade-off\\n'\n",
      "                'While the previous problem sounds more like an engineering '\n",
      "                'effort (it’s not), the exploration-exploitation trade-off '\n",
      "                'seems more fundamental. Whenever we train an RL-agent, the '\n",
      "                'agent will need some time to explore, it needs to take some '\n",
      "                'actions that it hasn’t taken before, in order to discover how '\n",
      "                'to solve the problem. On the other hand, we can’t let the '\n",
      "                'agent always take random actions, because these random '\n",
      "                'actions might lead to nothing. Sometimes we want the agent to '\n",
      "                'leverage what it has already learned to try and optimize '\n",
      "                'further. This is the exploration-exploitation trade-off, we '\n",
      "                'want an automated way to strike a good balance between '\n",
      "                'letting the agent explore and taking actions for which it '\n",
      "                'already knows what they will lead to.\\n'\n",
      "                '\\n'\n",
      "                'For a lot problems, it is quite possible that the agent gets '\n",
      "                'stuck in a local optimum.\\n'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'The exploration-exploitation trade-off sounds very much '\n",
      "                'tractable at first, but it turns out to be one of the hardest '\n",
      "                'problems for RL to solve. To give you an idea about how hard '\n",
      "                'this problem is: The problem was originally considered by the '\n",
      "                'scientists of the allied forces, but was suggested to be '\n",
      "                'dropped over to Germany, because it was deemed so intractable '\n",
      "                'that they wanted the German scientists to also waste their '\n",
      "                'time on it.\\n'\n",
      "                '\\n'\n",
      "                'No silver bullet for this problem has been found and there '\n",
      "                'are many people working on various solutions. I will leave a '\n",
      "                'link here to a previously proposed solution called '\n",
      "                '“curiosity-driven exploration” which I found particularly '\n",
      "                'interesting.\\n'\n",
      "                '\\n'\n",
      "                'Curiosity-driven exploration by Self-supervised Prediction\\n'\n",
      "                '\\n'\n",
      "                'The sparse-reward problem\\n'\n",
      "                'Another rather fundamental problem, is the so called '\n",
      "                'Sparse-reward problem. As the name implies, this problem '\n",
      "                'occurs when our RL-agent receives so little rewards, that it '\n",
      "                'actually gets no feedback on how it should improve.\\n'\n",
      "                '\\n'\n",
      "                'Imagine for example this mountain car. The agent needs to '\n",
      "                'move the car left and right, such that it gets enough '\n",
      "                'momentum to reach the top. Initially though, the agent '\n",
      "                'doesn’t know that it needs to move the car back and forth to '\n",
      "                'reach the top. If we only give our agent a reward (a positive '\n",
      "                'feedback signal) when we have reached the flag, it might not '\n",
      "                'ever get a positive feedback signal, simply because it might '\n",
      "                'never reach the flag by taking random actions (exploration).\\n'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'Something commonly done to counteract this problem, is by '\n",
      "                '“reward shaping”. We will modify the reward such that the '\n",
      "                'agent gets more feedback signals to learn from. In case of '\n",
      "                'the mountain car, we could for example also give the agent a '\n",
      "                'positive reward based on the speed or altitude it achieves. '\n",
      "                'However, reward shaping is not a scalable solution. Luckily '\n",
      "                'other solutions are being sought after.\\n'\n",
      "                '\\n'\n",
      "                'I’m leaving another example here which I think is an '\n",
      "                'interesting (but not generally applicable approach) for '\n",
      "                'counteracting sparse rewards.\\n'\n",
      "                '\\n'\n",
      "                'Hindsight Experience Replay\\n'\n",
      "                '\\n'\n",
      "                'The aforementioned problems are just some prominent examples, '\n",
      "                'but there are in fact many more problems that are being '\n",
      "                'looked into by some of the brightest minds out there. The '\n",
      "                'good news is that regular and steady progress is being made.\\n'\n",
      "                '\\n'\n",
      "                'Conclusion\\n'\n",
      "                'Phew, that was a lot of information in a very short period, '\n",
      "                'but you made it through! This first introduction should give '\n",
      "                'you a good idea of what RL is all about.']],\n",
      " 'embeddings': None,\n",
      " 'ids': [['id1', 'id5']],\n",
      " 'metadatas': [[{'source': 'reinforcement learning DQN part 1.txt'},\n",
      "                {'source': 'reinforcement learning and introduction part '\n",
      "                           '1.txt'}]],\n",
      " 'uris': None}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collections have a few useful convenience methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = collection.peek() # returns a list of the first 10 items in the collection\n",
    "col_num = collection.count() # returns the number of items in the collection\n",
    "# collection.modify(name=\"new_name\") # Rename the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': ['id0', 'id1', 'id2', 'id3', 'id4', 'id5'], 'embeddings': [[-0.06983184069395065, -0.06972957402467728, -0.09745623916387558, 0.05306597054004669, -0.0515860840678215, -0.015564018860459328, 0.016165276989340782, -0.009612586349248886, -0.06355269253253937, 0.01641022600233555, 0.027449922636151314, -0.10778404027223587, 0.06536437571048737, -0.006359830964356661, 0.11691190302371979, 0.05337958037853241, 0.010832883417606354, 0.04060027748346329, -0.05769278109073639, -0.05000103637576103, 0.04764392971992493, -0.020905539393424988, 0.009941269643604755, -0.0009177944157272577, 0.005550786387175322, -0.014407220296561718, 0.03488421067595482, 0.00801693182438612, -0.015840968117117882, -0.023544764146208763, 0.06425171345472336, -0.0007424064096994698, 0.04046451672911644, 0.03580328822135925, 0.04099864140152931, 0.050649888813495636, 0.005459611304104328, 0.0006583072245121002, 0.025315532460808754, -0.00743169104680419, -0.024391919374465942, -0.05828355625271797, -0.06552297621965408, -0.03209194168448448, 0.040599655359983444, -0.014982599765062332, 0.0034258500672876835, 0.044368620961904526, 0.07505949586629868, -0.03544149920344353, -0.05987362563610077, -0.007260330952703953, 0.025376426056027412, -0.0023637223057448864, -0.10544351488351822, -0.014684638008475304, -0.008903645910322666, -0.06602760404348373, 0.04392382130026817, -0.10934733599424362, 0.014408838003873825, -0.033870093524456024, 0.005169359967112541, 0.014469658024609089, -0.041638411581516266, -0.049831923097372055, -0.08488760888576508, 0.01869926042854786, 0.11407723277807236, -0.08897727727890015, 0.000543477653991431, -0.07592938840389252, -0.10511674731969833, -0.00310908374376595, -0.010701912455260754, -0.010663643479347229, 0.09226628392934799, -0.07042089849710464, -0.006643187254667282, 0.01649080403149128, -0.04593927785754204, 0.010982208885252476, 0.026705747470259666, 0.048047132790088654, 0.03743350878357887, 0.019636349752545357, 0.08405192196369171, 0.11047614365816116, -0.012887276709079742, -0.004216813948005438, 0.03691808879375458, 0.03533901274204254, 0.027770115062594414, -0.00624436279758811, 0.07533195614814758, 0.053096067160367966, 0.05490162596106529, -0.033984340727329254, 0.009022388607263565, 0.09334629029035568, 0.017017710953950882, -0.06261766701936722, -0.07023970782756805, 0.002418175106868148, -0.022538509219884872, 0.01758703961968422, 0.026965733617544174, 0.07259386777877808, 0.012364731170237064, 0.014900833368301392, -0.04081176966428757, 0.007617552764713764, -0.09423422813415527, -0.0038576959632337093, 0.021910475566983223, 0.11931713670492172, -0.10254830121994019, 0.028554849326610565, -0.007470282260328531, 0.03278515487909317, 0.05401016026735306, 0.049163736402988434, -0.0273887999355793, 0.03251432627439499, 0.007739754859358072, -0.03687110170722008, 0.0002977751719299704, 7.531420223709228e-33, 0.011885893531143665, 0.020155521109700203, 0.03793717175722122, 0.0019094252493232489, -0.009645240381360054, 0.10512484610080719, 0.07984466850757599, 0.015636691823601723, -0.018456682562828064, -0.06945618987083435, -0.07432118058204651, 0.12119786441326141, -0.06551698595285416, 0.07327345013618469, -0.07540370523929596, -0.11470136046409607, -0.020515447482466698, 0.027453472837805748, 0.1368662416934967, -0.04567712917923927, -0.0026038039941340685, 0.009242461062967777, 0.044541358947753906, 0.009920668788254261, -0.013951986096799374, 0.07822077721357346, -0.006546606309711933, -0.01913786493241787, 0.09564606100320816, 0.021688664332032204, 0.040277160704135895, 0.0300698634237051, -0.04112224280834198, 0.012659535743296146, -0.03366982936859131, 0.0149207953363657, -0.044368013739585876, -0.04824109375476837, -0.043169476091861725, 0.004044496454298496, -0.016057347878813744, 0.06573601067066193, 0.0011623069876804948, -0.05441637337207794, -0.09551230072975159, 0.0333184078335762, -0.01365678571164608, -0.009589499793946743, -0.004500626586377621, 0.008239641785621643, -0.054728832095861435, 0.03178143501281738, -0.030653972178697586, -0.06424044072628021, -0.006695794407278299, -0.09371031820774078, -0.015822745859622955, -0.0554208867251873, -0.012646923772990704, -0.051298290491104126, 0.00857149250805378, 0.028866799548268318, 0.05453924089670181, 0.027790118008852005, -0.010466325096786022, 0.02605554461479187, 0.03216871991753578, 0.02259380742907524, -0.022602824494242668, 0.006929495837539434, -0.0340379923582077, 0.10796579718589783, 0.03504234552383423, 0.003919393289834261, 0.04370272904634476, 0.0024804959539324045, 0.021015657112002373, -0.04258950054645538, 0.014858168549835682, -0.029967334121465683, 0.005820455029606819, -0.03727763518691063, 0.02637219987809658, 0.004663099534809589, -0.014142927713692188, -0.005638361442834139, -0.01766619086265564, -0.0918751060962677, 0.04108186811208725, -0.050265539437532425, -0.07146409153938293, -0.08037018775939941, -0.07550090551376343, 0.021723469719290733, -0.03496180847287178, -7.286845399709204e-33, 0.009986928664147854, -0.03951248154044151, 0.031213659793138504, 0.07736208289861679, 0.01241944171488285, -0.05834399536252022, -0.03652281314134598, -0.015198086388409138, 0.025370316579937935, 0.03609132021665573, -0.04775712266564369, -0.04118446260690689, 0.08727362751960754, -0.03486780822277069, -0.0046448130160570145, -0.03624531254172325, -0.025004612281918526, 0.037833306938409805, -0.013911943882703781, -0.0035093307960778475, -0.09267202764749527, 0.04847879335284233, -0.1285172700881958, -0.04597482457756996, 0.039396002888679504, -0.010255508124828339, -0.04113926365971565, -0.01594504900276661, 0.06573422998189926, 0.0314013697206974, 0.008276371285319328, 0.028786586597561836, -0.010031912475824356, -0.05382696911692619, -0.029833054170012474, 0.02291678637266159, 0.028503794223070145, -0.02167796902358532, 0.0276622474193573, 0.047935016453266144, 0.06750015914440155, -0.008675377815961838, -0.1153636947274208, -0.05777030438184738, 0.034134216606616974, -0.040573570877313614, -0.06487138569355011, 0.1328047662973404, 0.06632168591022491, -0.00822453573346138, 0.03193967416882515, 0.05460602790117264, -0.03512118011713028, -0.08066673576831818, 0.004183164332062006, 0.02584792673587799, 0.04303641617298126, 0.08255680650472641, -0.002683584578335285, -0.06771116703748703, -0.08525989204645157, -0.0808568224310875, 0.011972288601100445, 0.04603901877999306, 0.0008911309996619821, -0.004155280999839306, -0.03531055524945259, 0.03776104003190994, -0.031972236931324005, -0.0661800280213356, 0.04882632568478584, -0.044785916805267334, 0.058326903730630875, -0.054440975189208984, 0.002783177187666297, -0.03568070009350777, -0.024456795305013657, -0.029094483703374863, 0.05284433811903, 0.027858031913638115, 0.015565495938062668, -0.016753485426306725, 0.1215791255235672, 0.1183285266160965, 0.06549143046140671, 0.023812109604477882, 0.07858933508396149, 0.02433326095342636, 0.024141903966665268, 0.030001921579241753, -0.08999283611774445, 0.02637486159801483, 0.037787389010190964, 0.11997127532958984, 0.07023368775844574, -5.6013185201209126e-08, -0.004960131831467152, -0.017417732626199722, 0.0431026853621006, 0.11408970504999161, 0.022968029603362083, 0.1254846602678299, -0.016432663425803185, -0.04004563018679619, -0.012941432185471058, 0.018167339265346527, 0.03748156130313873, -0.014707298949360847, -0.06788342446088791, -0.06907201558351517, -0.04663791134953499, 0.12911322712898254, 0.021892456337809563, 0.0037653411272913218, 0.059185657650232315, -0.04242236167192459, 0.039741091430187225, -0.006215349305421114, 0.0539468489587307, -0.03930274769663811, -0.014709300361573696, -0.05548395961523056, -0.0013634016504511237, 0.09221784770488739, -0.053883910179138184, -0.04516468197107315, -0.018746813759207726, -0.06287497282028198, 0.09753924608230591, -0.01619151420891285, 0.04262657091021538, 0.013012966141104698, -0.08518892526626587, -0.026423539966344833, -0.011051742359995842, -0.02473677322268486, -0.09829215705394745, 0.025503162294626236, -0.03312386944890022, -0.04426925256848335, 0.04404039680957794, -0.008285812102258205, -0.07882798463106155, -0.027066171169281006, 0.015564880333840847, -0.02735557220876217, -0.037836529314517975, -0.036783814430236816, 0.0518374927341938, 0.015701958909630775, 0.09405581653118134, 0.10656555742025375, -0.09564264863729477, -0.03279908373951912, 0.00844500120729208, 0.020655998960137367, 0.016915665939450264, -0.00046185479732230306, -0.03795035555958748, -0.039756424725055695], [-0.1746644377708435, -0.0286662969738245, 0.020995715633034706, 0.024822935461997986, -0.015803059563040733, 0.07658044248819351, -0.0065614343620836735, 0.0010164488339796662, 0.003307355334982276, 0.03455674275755882, -0.04309146851301193, 0.09054107964038849, 0.03266545385122299, 0.07092161476612091, -0.02730625867843628, 0.03993455693125725, 0.01578792743384838, -0.013313574716448784, -0.051549021154642105, -0.0399448424577713, 0.01453592348843813, -0.03458799794316292, 0.03833533450961113, -0.003940415568649769, 0.020292341709136963, 0.016505515202879906, -0.004358355887234211, -0.00964765902608633, 0.02822262980043888, 0.004517030902206898, 0.013645030558109283, 0.02496897242963314, 0.086848184466362, -0.058342959731817245, -0.0958513393998146, 0.03335870802402496, -0.053479600697755814, -0.025725584477186203, -0.049665242433547974, 0.08733194321393967, -0.04988903924822807, -0.0014687766088172793, -0.021448882296681404, 0.006447157356888056, 0.09475214779376984, 0.004257221706211567, -0.03319839388132095, -0.0024614890571683645, -0.05676152557134628, -0.0568840317428112, -0.002690606052055955, -0.042127374559640884, 0.0703621655702591, 0.01808207295835018, 0.05992662534117699, 0.021555397659540176, 0.008066557347774506, 0.030905358493328094, -0.039936523884534836, -0.04385741427540779, 0.025948811322450638, -0.027761122211813927, -0.06441247463226318, 0.011155765503644943, -0.07535048574209213, -0.04572755843400955, -0.00896270852535963, 0.05701359733939171, 0.09911993891000748, -0.028773484751582146, -0.13314542174339294, 0.0035451131407171488, -0.013096002861857414, -0.04175486043095589, 0.016226112842559814, -0.05907246097922325, 0.046948812901973724, 0.049118198454380035, -0.038163576275110245, -0.05571942776441574, 0.04522952809929848, -0.04192785546183586, -0.03617898002266884, -0.036426253616809845, 0.01679106429219246, -0.032705940306186676, -0.03246399760246277, 0.006055820733308792, 0.020281698554754257, -0.036561932414770126, 0.03719368204474449, 0.026657484471797943, 0.006317954510450363, -0.017315708100795746, 0.10084306448698044, 0.026987314224243164, -0.05797360837459564, -0.03866963088512421, 0.010249707847833633, 0.024414530023932457, 0.07085315883159637, 0.038593463599681854, -0.08078725636005402, 0.01271913480013609, 0.06552091985940933, -0.03923416882753372, -0.0011215993436053395, 0.03820270672440529, 0.06193076819181442, -0.06571625173091888, -0.04376339167356491, 0.023610826581716537, -0.00590010080486536, 0.013737206347286701, -0.04045296087861061, 0.04628157243132591, 0.04525193199515343, -0.007102013565599918, -0.01886468380689621, 0.03274918720126152, -0.03517625480890274, -0.08122344315052032, -0.08491890877485275, -0.016942841932177544, 0.06046583503484726, -0.01620618626475334, -0.04632175713777542, 1.9915252842280042e-33, 0.03743290528655052, -0.00012867907935287803, -0.02309291623532772, -0.0022620190866291523, 0.05651941895484924, -0.0397026427090168, 0.1301870048046112, -0.06047680974006653, 0.052237097173929214, -0.07582612335681915, 0.0770384818315506, 0.04874829947948456, -0.046643733978271484, 0.006834204774349928, -0.007646087557077408, -0.060057997703552246, -0.15532465279102325, -0.07247133553028107, 0.05545320361852646, -0.03700033575296402, 0.04856843128800392, -0.008350281044840813, -0.00737344566732645, -0.0013588322326540947, 0.008126694709062576, 0.07659243047237396, 0.06262795627117157, 0.03854077309370041, 0.019922440871596336, 0.0006594769656658173, -0.02059357985854149, -0.010234919376671314, -0.10800276696681976, 0.0011449328158050776, -0.018291611224412918, 0.08516460657119751, -0.03200584277510643, -0.019288966432213783, -0.01233656145632267, -0.0037744108121842146, -0.0058352155610919, -0.017699331045150757, 0.0012902705930173397, -0.03410765901207924, -0.028585363179445267, -0.0031293833162635565, -0.004816197324544191, -0.04869939759373665, -0.015531713142991066, -0.06706704944372177, -0.018596233800053596, -0.03147977218031883, -0.002169879851862788, -0.0976877361536026, 0.030506068840622902, 0.010400799103081226, 0.011466001160442829, 0.07123000919818878, -0.09777862578630447, 0.02843419276177883, 0.11179620027542114, -0.022783230990171432, -0.004139200318604708, -0.006825851742178202, -0.04241323471069336, 0.10378096252679825, -0.012284194119274616, 0.006250372156500816, 0.021457742899656296, -0.004083845764398575, 0.040656138211488724, 0.0517507903277874, 0.0020420304499566555, -0.054526373744010925, 0.10900074988603592, -0.008073084987699986, 0.018835846334695816, -0.04257082939147949, -0.008804354816675186, -0.018967971205711365, -0.0353231206536293, 0.02150251716375351, -0.04174909368157387, -0.004298386629670858, -0.005562690086662769, -0.09730730950832367, 0.032558243721723557, -0.06479138880968094, -0.00847859401255846, 0.04794797673821449, -0.076778344810009, -0.04782797023653984, 0.025351203978061676, 0.15364712476730347, 0.04595500975847244, -3.8974789878199615e-33, -0.011719339527189732, 0.002668471774086356, -0.025568252429366112, 0.05403359979391098, -0.04179973527789116, -0.03191036731004715, -0.10242132842540741, 0.029853031039237976, 0.06625622510910034, -0.07754333317279816, -0.04388319328427315, 0.06737696379423141, 0.05680013447999954, 0.08842974901199341, -0.025536997243762016, -0.018985003232955933, -0.014073085971176624, -0.0566694512963295, -0.04775692895054817, 0.0654061883687973, 0.01706714741885662, 0.037139225751161575, -0.13703899085521698, 0.02805786021053791, 0.09186189621686935, 0.023434126749634743, 0.048923738300800323, 0.03957180306315422, -0.011709950864315033, -0.01983741857111454, -0.09745585173368454, -0.17567294836044312, -0.00990474782884121, -0.060606203973293304, -0.011698801070451736, 0.07716567814350128, 0.09947472810745239, -0.0002180899609811604, -0.04079747572541237, 0.025357762351632118, 0.04685712233185768, -0.10918877273797989, 0.0320710614323616, -0.016359947621822357, 0.0028514028526842594, -0.03574782982468605, -0.08415525406599045, 0.03985583409667015, -0.010931461118161678, 0.02281486615538597, 0.037106312811374664, 0.01967528834939003, 0.02953071892261505, 0.019681382924318314, -0.0670260637998581, 0.029475772753357887, -0.016499696299433708, 0.11210969090461731, 0.07656136900186539, 0.03806189075112343, -0.04270680248737335, 0.04970446228981018, -0.10696285218000412, 0.08072281628847122, -0.039451587945222855, -0.010118444450199604, -0.008540529757738113, 0.0011017967481166124, 0.04809923842549324, -0.0448714941740036, 0.0576707199215889, 0.04848159849643707, -0.001940314075909555, 0.017096417024731636, 0.04122433811426163, -0.02826082892715931, -0.014281852170825005, -0.032514940947294235, -0.06389890611171722, -0.034422170370817184, 0.03662724047899246, 0.04067060723900795, 0.02821475639939308, 0.01870458759367466, 0.006031685508787632, 0.08077715337276459, 0.06791488826274872, 0.02456728368997574, 0.004371442832052708, -0.07229416072368622, -0.0005923830321989954, -0.04067383334040642, -0.03638452664017677, 0.001207894878461957, -0.08136789500713348, -5.2445514597820875e-08, 0.008681230247020721, -0.03120511770248413, 0.06636565178632736, -0.019116394221782684, 0.14461828768253326, 0.02321562170982361, 0.008297944441437721, 0.011821804568171501, -0.09506580978631973, 0.0655013844370842, 0.0359574630856514, 0.13737215101718903, 0.004977372009307146, -0.04464947059750557, -0.014707876369357109, 0.05938571318984032, -0.010617581196129322, -0.05392974615097046, 0.018165525048971176, -0.002516612410545349, 0.08673832565546036, 0.008175186812877655, 0.01334894634783268, -0.060909826308488846, 0.02962803840637207, -0.013556936755776405, -0.006654819939285517, 0.022868309170007706, 0.06038712337613106, -0.0340830497443676, -0.0007152213947847486, 0.062183622270822525, 0.07454974949359894, -0.012582825496792793, 0.012480277568101883, 0.051507193595170975, 0.024330083280801773, -0.08953807502985, 0.030352100729942322, -0.048328697681427, -0.07363751530647278, -0.004053704906255007, 0.03942224755883217, -0.021087417379021645, -0.009100333787500858, 0.011952069588005543, -0.10697440057992935, -0.054723430424928665, 0.0360281839966774, -0.005464437883347273, 0.029613956809043884, -0.03813246637582779, -0.004017896018922329, -0.029887013137340546, 0.05709080770611763, 0.007744142785668373, 0.0591045543551445, -0.09271116554737091, 0.05168578401207924, 0.03284498304128647, 0.028198832646012306, 0.06012088060379028, 0.025348151102662086, 0.08204342424869537], [-0.11938374489545822, -0.011259947903454304, 0.015469560399651527, -0.003319990122690797, -0.05277243256568909, 0.0050654783844947815, -0.0011035595089197159, 0.024731537327170372, -0.1007835641503334, 0.04894816130399704, -0.0832124650478363, -0.06667296588420868, 0.061193209141492844, 0.01793580688536167, 0.06252425163984299, 0.08048065751791, -0.002954458585008979, 0.03798721358180046, -0.06026645377278328, -0.13416491448879242, 0.004574502352625132, -0.0284400824457407, 0.058490004390478134, 0.015438791364431381, -0.03607692942023277, 0.034671884030103683, 0.055346205830574036, 0.025724656879901886, 0.04569769650697708, 0.0013765193289145827, 0.06772539764642715, -0.09153399616479874, 0.039507780224084854, -0.03564797341823578, -0.08684614300727844, 0.10414747148752213, -0.07642455399036407, -0.09329143911600113, -0.043338555842638016, -0.0059540471993386745, -0.09693286567926407, 0.0069077759981155396, 0.006813374347984791, -0.040350787341594696, 0.07817645370960236, -0.05035340040922165, -0.011305649764835835, -0.024081779643893242, 0.015934377908706665, -0.08434368669986725, 0.013131883926689625, -0.06806328892707825, 0.02545865997672081, -0.061880920082330704, 0.007552805356681347, -0.019619490951299667, 0.10359890013933182, -0.023399416357278824, 0.0033835056237876415, -0.10099592059850693, -0.009542111307382584, -0.06188647449016571, -0.027044590562582016, -0.00438440078869462, -0.0912415087223053, -0.018524380400776863, -0.04823029413819313, 0.11797364801168442, 0.09532193094491959, -0.029631856828927994, -0.09622452408075333, -0.03034641593694687, 0.01470138132572174, 0.006779374089092016, 0.048121627420186996, -0.061323132365942, -0.014146042056381702, 0.003403659677132964, 0.05130830779671669, -0.026098724454641342, -0.0035930073354393244, -0.020129183307290077, -0.020599020645022392, 0.056760188192129135, 0.021293386816978455, -0.08259066939353943, 0.05846270173788071, 0.07923020422458649, 0.09379199892282486, 0.0808463916182518, 0.1417560577392578, 0.05402516946196556, 0.033912669867277145, -0.0033945732284337282, 0.05086364224553108, 0.050802234560251236, -0.06592199206352234, -0.13742271065711975, -0.07387612015008926, 0.050480302423238754, 0.032284799963235855, 0.004422989208251238, 0.008151755668222904, 0.009449784643948078, -0.01487059611827135, -0.035400040447711945, -0.003629008773714304, -0.007303324528038502, 0.01901816762983799, -0.08367230743169785, -0.0037193784955888987, -0.01322497334331274, -0.006296795792877674, 0.019366703927516937, -0.02236356772482395, 0.040109019726514816, 0.06903530657291412, -0.02921394445002079, -0.025245051831007004, 0.07898800820112228, 0.024515492841601372, -0.05814891308546066, -0.03615891560912132, 0.03171251714229584, 0.06558316200971603, -0.020420905202627182, -0.01651160418987274, 4.37871719149699e-33, 0.03479805588722229, 0.03870408609509468, -0.04686268791556358, 0.04889494553208351, 0.029912684112787247, 0.004233749583363533, 0.11464039981365204, 0.006624280009418726, -0.05322021618485451, -0.0681275725364685, 0.06232163682579994, 0.03929649293422699, -0.0604785792529583, 0.10476171225309372, 0.047737155109643936, -0.10777126997709274, -0.03640179708600044, 0.04761187732219696, 0.03722527623176575, -0.024616505950689316, -0.040046051144599915, -0.050661660730838776, 0.018472421914339066, -0.008913939818739891, 0.01188149768859148, 0.03629215806722641, 0.04067402333021164, 0.0008362410007975996, 0.023077096790075302, -0.02242228575050831, 0.02504800073802471, -0.030336126685142517, -0.061289962381124496, 0.07262684404850006, -0.0010437722085043788, 0.06762398779392242, -0.008872628211975098, 0.021532613784074783, 0.014564082026481628, 0.011602746322751045, 0.016311800107359886, -0.01615045964717865, 0.049612801522016525, 0.0015413160435855389, -0.004948154557496309, 0.02295818366110325, 0.038490068167448044, -0.0478588342666626, -0.04557625204324722, -0.011387435719370842, 0.00656898133456707, 0.026495318859815598, 0.017090553417801857, -0.037223439663648605, -0.03288610652089119, 0.04031454026699066, -0.029434364289045334, 0.033653635531663895, 0.002157376380637288, -0.011835481971502304, 0.06203269958496094, -0.011630125343799591, 0.019075756892561913, 0.051541123539209366, -0.019972754642367363, 0.08435876667499542, -0.037870630621910095, 0.043412815779447556, 0.05202110856771469, 0.026571298018097878, -0.024394886568188667, 0.013325824402272701, 0.06657379865646362, -0.03186178207397461, 0.04251647740602493, -0.005469091702252626, 0.04549901932477951, -0.07678910344839096, 0.024700962007045746, -0.023261774331331253, -0.031356971710920334, 0.07272942364215851, 0.01320428866893053, -0.02673295885324478, 0.005720555316656828, -0.06977806985378265, -0.014926213771104813, 0.012632835656404495, 0.00815766490995884, 0.0327611118555069, -0.08150749653577805, -0.06549707800149918, 0.047379352152347565, 0.07808343321084976, 0.05573761835694313, -5.4902224053063126e-33, 0.06166549026966095, -0.008090442977845669, -0.018146319314837456, -0.05089939385652542, -0.014521652832627296, -0.009255991317331791, -0.08426427841186523, -0.06956183910369873, 0.06660301983356476, -0.050232890993356705, -0.03006790205836296, 0.04064057394862175, 0.0510026291012764, 0.048928629606962204, -0.07993577420711517, -0.07107605040073395, -0.017798030748963356, 0.036191366612911224, 0.04176069051027298, 0.002368414541706443, -0.0177528765052557, 0.059935469180345535, -0.10853098332881927, 0.03421028330922127, 0.014821183867752552, -0.02508101984858513, -0.06819561868906021, 0.039145391434431076, 0.044707126915454865, 0.05427208170294762, -0.05644446238875389, -0.047803573310375214, 0.022555144503712654, -0.03800230473279953, -0.012632640078663826, 0.04996638372540474, 0.11457385867834091, 0.009658390656113625, -0.06221676990389824, 0.054121509194374084, 0.0565335750579834, -0.04548200964927673, -0.014442384243011475, 0.0017246954375877976, 0.03976663202047348, -0.01247168704867363, -0.09492059051990509, 0.04150926321744919, -0.09798579663038254, -0.0231202132999897, -0.039912380278110504, 0.010353568941354752, -0.028989652171730995, -0.08842041343450546, -0.03437558189034462, -0.04793202504515648, 0.05848943442106247, 0.00199161097407341, 0.020365728065371513, 0.007276515476405621, -0.15747328102588654, 0.005851110443472862, -0.02369179017841816, 0.04092960059642792, -0.07053112238645554, -0.009103154763579369, -0.02224106341600418, 0.010485752485692501, -0.01027010940015316, -0.01978975348174572, -0.024940602481365204, 0.09063486009836197, 0.04327511414885521, -0.04205852746963501, 0.07526881992816925, -0.0633196085691452, -0.037308890372514725, -0.028303466737270355, -0.008247553370893002, -0.001208538538776338, 0.06286229938268661, 0.02673100121319294, 0.08097770065069199, 0.04472909867763519, -0.08652187138795853, 0.07827658951282501, -0.0043483697809278965, 0.07099557667970657, 0.03657831251621246, -0.051030971109867096, -0.06705930829048157, 0.014711505733430386, 0.02317803166806698, -0.010997110046446323, -0.0401933491230011, -6.384038186979524e-08, -0.011758522130548954, -0.016528373584151268, 0.0828535333275795, 0.042154621332883835, 0.04923957586288452, 0.05356879159808159, -0.028745662420988083, -0.01964154653251171, -0.011333925649523735, -0.015112525783479214, 0.05016574636101723, 0.029848232865333557, 0.013460717163980007, -0.05915283039212227, -0.034947313368320465, 0.10119384527206421, -0.019501518458127975, -0.010118943639099598, -0.014680505730211735, -0.03213230147957802, 0.07239218801259995, 8.353339944733307e-05, 0.042597007006406784, -0.03807958960533142, 0.026521652936935425, -0.10029710829257965, 0.024950528517365456, 0.016767999157309532, 0.04147204011678696, -0.00123938440810889, 0.047822196036577225, -0.03862953186035156, 0.0559520423412323, 0.04205005615949631, 0.09768284112215042, 0.03346752002835274, 0.06295174360275269, -0.0741562694311142, 0.03443906456232071, 0.04575006663799286, -0.06157601624727249, 0.059214621782302856, 0.03716273605823517, -0.06359007209539413, 0.0121453320607543, -0.0009734979830682278, -0.1196160838007927, -0.07440046966075897, 0.012597816064953804, -0.11132040619850159, 0.022287940606474876, -0.03250527381896973, -0.06109865754842758, 0.054768502712249756, 0.07473497837781906, 0.046216029673814774, -0.012673883698880672, -0.023299533873796463, 0.005145504605025053, 0.10101816803216934, -0.019806629046797752, 0.06387259811162949, -0.0038199317641556263, 0.004076294135302305], [-0.2041444331407547, -0.08783330768346786, 0.017198022454977036, -0.008652301505208015, 0.039868373423814774, 0.08115921914577484, 0.027448076754808426, -0.029941828921437263, 0.03890117257833481, 0.018862226977944374, -0.013091027736663818, 0.03422588109970093, 0.08324902504682541, 0.03604242578148842, -0.0011653994442895055, 0.045127660036087036, 0.03229420632123947, 0.034367021173238754, -0.07307817786931992, -0.08438996970653534, 0.03774787485599518, 0.016496384516358376, 0.07250428944826126, 0.02275489643216133, 0.021305367350578308, 0.01860813796520233, -0.009624059312045574, -0.012703693471848965, 0.019765689969062805, -0.001362795359455049, 0.08110924065113068, -0.06328995525836945, 0.09184098243713379, -0.04755198583006859, -0.08322415500879288, 0.10662007331848145, -0.1703406572341919, -0.05905541777610779, -0.03674013167619705, 0.04473978653550148, -0.038089558482170105, 0.025155777111649513, -0.014545194804668427, 0.02239392139017582, 0.038405440747737885, -0.02810746245086193, -0.030325157567858696, -0.0289655402302742, -0.042734429240226746, -0.05265594273805618, -0.004392136819660664, -0.003337090602144599, 0.0035250051878392696, -0.02256413921713829, -0.004824694246053696, 0.02824663557112217, 0.03853275626897812, -0.008977414108812809, 0.006385141517966986, -0.08837106823921204, -0.00016259575204458088, -0.07731527090072632, -0.026445921510457993, 0.02215961366891861, -0.015440677292644978, -0.006985850632190704, -0.07760359346866608, 0.09721362590789795, 0.03740682825446129, 0.06010078638792038, -0.08007044345140457, 0.036321233958005905, 0.01634671725332737, -0.008921876549720764, 0.07939566671848297, -0.029744327068328857, 0.01384820882230997, -0.008053027093410492, -0.00917357299476862, -0.026008667424321175, -0.0456826277077198, -0.09627076238393784, 0.01730329729616642, 0.025799818336963654, 0.04760061949491501, -0.07624794542789459, 0.0334947295486927, 0.015928160399198532, 0.1336647868156433, 0.02787725068628788, 0.06811175495386124, 0.030859723687171936, 0.0019559012725949287, -0.012694456614553928, 0.010606182739138603, 0.04344521090388298, -0.07590918987989426, -0.11534223705530167, -0.08352632075548172, 0.04729084670543671, 0.06348021328449249, 0.08953683823347092, 0.012812397442758083, -0.014474551193416119, -0.035678498446941376, -0.07985208928585052, -0.047071412205696106, 0.04127424955368042, 0.04560622572898865, -0.07308027148246765, 0.04730381816625595, 0.05506812036037445, 0.018245765939354897, 0.00624755909666419, -0.09771403670310974, 0.06769669055938721, 0.03633906692266464, -0.07180619239807129, 0.020419519394636154, 0.07192902266979218, 0.009675467386841774, -0.0241222083568573, -0.07529343664646149, 0.011419564485549927, 0.03258780762553215, -0.028366021811962128, -0.04452075809240341, 2.7683365833025046e-33, 0.029329190030694008, 0.02736675553023815, -0.03678238019347191, -0.0363335981965065, -0.0295675378292799, -0.05659055709838867, 0.049468815326690674, -0.022732146084308624, -0.05080616846680641, -0.04299575835466385, 0.02835005521774292, 0.07852857559919357, -0.009641663171350956, 0.09520608931779861, 0.022248370572924614, -0.06524854153394699, -0.06996776163578033, 0.046662572771310806, 0.021994948387145996, -0.03114645928144455, 0.0022733197547495365, 0.020038330927491188, 0.0049705044366419315, -0.029045388102531433, -0.02283395826816559, 0.025497477501630783, 0.019814249128103256, -0.027572505176067352, 0.004828498233109713, 0.037716418504714966, -0.011808612383902073, 0.030764643102884293, -0.04714659973978996, -0.002401124220341444, 0.02403348498046398, 0.05389880761504173, -0.053177688270807266, -0.021939000114798546, 0.009934311732649803, 0.028219949454069138, 0.04870569705963135, -0.04155907779932022, -0.024400707334280014, 0.05198819562792778, -0.06975609064102173, -0.11396889388561249, -0.015171964652836323, -0.04775256663560867, -0.04306192696094513, 0.0037773209623992443, -0.009006967768073082, 0.016015460714697838, 0.014433148317039013, -0.0642734095454216, -0.04968760162591934, 0.018717417493462563, -0.05006171017885208, 0.06001480296254158, -0.0497114360332489, -0.02164379507303238, 0.052194371819496155, -0.07037509977817535, 0.05557343736290932, 0.03071781061589718, -0.03405939042568207, 0.0692162960767746, -0.06758677959442139, 0.015134199522435665, 0.023418888449668884, 0.03690774366259575, -0.00014036285574547946, 0.0661846473813057, 0.03213634714484215, -0.014988159760832787, 0.054487403482198715, 0.01013187412172556, 0.03887546807527542, -0.056351564824581146, -0.011520727537572384, 0.041331060230731964, -0.05892886593937874, 0.012383864261209965, -0.046507205814123154, 0.00666813924908638, -0.016401922330260277, -0.045005012303590775, 0.04309467971324921, -0.11656881123781204, -0.009532124735414982, 0.06413251906633377, -0.06328632682561874, -0.09477502852678299, 0.03574175387620926, 0.041128624230623245, 0.12353404611349106, -5.092182385702624e-33, -0.030669912695884705, 0.009488042443990707, -0.025766810402274132, -0.02629493735730648, -0.054956309497356415, -0.05468307435512543, -0.09021712094545364, -0.009376179426908493, 0.017810482531785965, 0.01201464980840683, -0.09024844318628311, 0.04763329029083252, -0.04135606810450554, 0.02457418665289879, -0.027054933831095695, -0.08140265941619873, 0.016156574711203575, 0.05211706459522247, -0.020001640543341637, -0.0005516152596101165, 0.006552575156092644, 0.08126046508550644, -0.0415382981300354, 0.08820809423923492, 0.059855397790670395, 0.0423802025616169, -0.032895278185606, 0.08263684064149857, 0.03691416233778, 0.04190599545836449, -0.08450845628976822, -0.03606975078582764, -0.055638447403907776, 0.03125807270407677, -0.03993823751807213, 0.08887650072574615, 0.060128096491098404, 0.05938997119665146, -0.030436336994171143, 0.02916787937283516, 0.06377984583377838, -0.07742921262979507, 0.022738222032785416, -0.020965907722711563, 0.08128861337900162, -0.06257928162813187, 0.002568415831774473, -0.018346304073929787, -0.03008795902132988, -0.03787541389465332, -0.036225154995918274, 0.030196920037269592, -0.03702976927161217, 0.0030786634888499975, -0.07264543324708939, 0.010627690702676773, 0.0057744961231946945, 0.00496707484126091, 0.049115344882011414, 0.017146160826086998, -0.12462488561868668, 0.06117716431617737, -0.0447051040828228, 0.021425317972898483, -0.004602855537086725, -0.03220027685165405, 0.009595225565135479, 0.0006266880664043128, 0.1111968457698822, -0.0442904531955719, 0.024194110184907913, 0.04412626475095749, 0.042712196707725525, -0.004301253240555525, 0.049558449536561966, -0.019152700901031494, -0.031068572774529457, 0.011269544251263142, -0.08742671459913254, -0.07410864531993866, 0.03738278150558472, 0.02756209298968315, 0.06645219773054123, 0.0455748587846756, -0.023035965859889984, -0.003750150091946125, 0.010483910329639912, 0.03685910999774933, -0.019790044054389, -0.05568866431713104, -0.06659430265426636, 0.0016387869836762547, 0.004995480179786682, 0.011187594383955002, -0.08523909747600555, -6.72349358410429e-08, -0.015831705182790756, 0.04994218423962593, 0.03137305751442909, 0.029439682140946388, 0.11851032078266144, 0.0032808082178235054, -0.015054731629788876, -0.04548945650458336, -0.04953732341527939, -0.03449913486838341, 0.051386184990406036, 0.04033602029085159, 0.040806930512189865, -0.1168646365404129, -0.050307437777519226, 0.1086931899189949, -0.06438998132944107, 0.007543284446001053, -0.018522441387176514, 0.014234955422580242, 0.07001923769712448, 0.004178379196673632, 0.042373038828372955, 0.007963129319250584, -0.0069116526283323765, -0.036796435713768005, 0.034613873809576035, 0.08529295027256012, 0.06558449566364288, -0.03574178367853165, 0.02127370610833168, 0.009308494627475739, 0.04831169545650482, 0.051186494529247284, 0.07741003483533859, 0.025756269693374634, 0.10860677063465118, -0.09005878120660782, 0.05091169476509094, 0.018757715821266174, -0.025608424097299576, 0.015653248876333237, 0.026091311126947403, -0.00559931481257081, 0.02825997956097126, 0.03355308249592781, -0.10669592022895813, -0.06382950395345688, 0.06063494458794594, -0.06409760564565659, -0.010959100909531116, -0.015597001649439335, -0.05200279876589775, 0.05412665754556656, 0.041634950786828995, -0.015016689896583557, 0.052579615265131, -0.07645260542631149, 0.003958635497838259, 0.050513505935668945, 0.007434767205268145, 0.10673543065786362, -0.030232150107622147, -0.04484035447239876], [-0.1465541273355484, -0.026460859924554825, 0.05484423413872719, 0.03237215057015419, -0.0017560158157721162, 0.07704626768827438, 0.030550964176654816, 0.0034038410522043705, 0.012543653137981892, 0.061653029173612595, -0.08355557173490524, -0.017486881464719772, 0.05767541751265526, 0.05875924602150917, -0.02010585367679596, 0.06080585718154907, -0.007400416769087315, -0.038870617747306824, -0.013438668102025986, -0.10251101106405258, 0.05025729909539223, 0.004524624906480312, 0.04067762568593025, 0.005370030645281076, -0.02147183008491993, 0.046923160552978516, 0.0191020630300045, 0.041159115731716156, 0.04233371093869209, 0.02594130113720894, 0.0965777263045311, -0.04420115426182747, 0.08046592026948929, -0.0731206089258194, -0.06960640102624893, 0.09547185897827148, -0.08901381492614746, -0.08397746086120605, -0.02565079554915428, 0.020449282601475716, -0.036910269409418106, 0.006629176903516054, -0.012379408814013004, 0.037078049033880234, 0.08505698293447495, -0.04291894659399986, -0.05543404072523117, -0.03292008489370346, -0.02957030199468136, -0.07546043395996094, 0.0022031019907444715, 0.014980396255850792, -0.004106262698769569, 0.03679727390408516, -0.014035784639418125, 0.011614607647061348, -0.01992298848927021, -0.02225814200937748, -0.008551615290343761, -0.11585724353790283, 0.024344349279999733, -0.09104838967323303, -0.02443472109735012, 0.02007364109158516, -0.021093444898724556, 0.005640936549752951, -0.07943665236234665, 0.07987737655639648, 0.09427488595247269, -0.04021499678492546, -0.09406746923923492, -0.0076650818809866905, 0.024321602657437325, -0.06134678050875664, 0.041771162301301956, -0.06296014040708542, -0.025864941999316216, -0.03228694573044777, -0.0076169283129274845, 0.020943177863955498, -0.017368026077747345, -0.05054468289017677, -0.05342483893036842, 0.034569449722766876, 0.02499796263873577, -0.04315521568059921, 0.03560708835721016, 0.014154625125229359, 0.09831520915031433, -0.001385223469696939, 0.05378582328557968, -0.03246769681572914, 0.051934756338596344, -0.02048061229288578, 0.014633135870099068, 0.03192947432398796, -0.02394936978816986, -0.09623685479164124, -0.021059677004814148, 0.06683645397424698, 0.08120613545179367, 0.029794124886393547, -0.0013051967835053802, -0.02766035497188568, -0.03714883700013161, -0.04744565114378929, -0.043059613555669785, 0.006233002059161663, 0.07637904584407806, -0.08254409581422806, -0.044428750872612, -0.0033040023408830166, 0.012331327423453331, 0.06704173982143402, -0.037996549159288406, 0.07728638499975204, 0.06051249802112579, -0.0524011105298996, -0.0071810949593782425, 0.032020993530750275, 0.011156326159834862, -0.06553338468074799, -0.023759037256240845, -0.04248693212866783, 0.0701134130358696, -0.0706600546836853, -0.020336396992206573, 3.0791874318408197e-33, 0.021016258746385574, -0.01816667430102825, -0.0017375919269397855, 0.026857538148760796, 0.030108941718935966, 0.00628639105707407, 0.10119543969631195, -0.01573789305984974, -0.002668913919478655, -0.008184894919395447, 0.02666427753865719, 0.11067653447389603, -0.0051198904402554035, 0.07771535217761993, -0.009402471594512463, -0.08286300301551819, -0.0729733482003212, -0.025715336203575134, 0.024021554738283157, -0.06502291560173035, 0.014167726039886475, 0.02075006254017353, 0.03427280858159065, 0.010574432089924812, 0.023006698116660118, 0.06311056017875671, 0.03488187491893768, -0.018358781933784485, -0.012171192094683647, -0.0038480146322399378, 0.005931005347520113, 0.08229850977659225, -0.11912865936756134, 0.013317005708813667, -0.0035432700533419847, 0.06931930035352707, -0.023785069584846497, 0.008796455338597298, 0.02201388031244278, 0.006963983178138733, -0.03803085535764694, -0.03428671509027481, -0.0020897029899060726, 0.043635040521621704, -0.014157452620565891, -0.06335000693798065, 0.01090692076832056, -0.052719227969646454, -0.018593676388263702, 0.016599299386143684, -0.031051108613610268, -0.006446931976824999, 0.0264792051166296, -0.09548982232809067, -0.01826235093176365, 0.050701938569545746, -0.025229373946785927, 0.04999823123216629, -0.05918123573064804, -0.01583671011030674, 0.09354102611541748, -0.04622675105929375, 0.012473911046981812, 0.04206622764468193, -0.044622015208005905, 0.06828857958316803, -0.028050554916262627, 0.022939467802643776, 0.030271047726273537, 0.05269938334822655, 0.036139387637376785, 0.0820658951997757, 0.08846871554851532, -0.023549163714051247, 0.0998162105679512, 0.002063451102003455, 0.042456671595573425, -0.07385352998971939, -0.07514764368534088, 0.03517590090632439, -0.06080804392695427, -0.0386953204870224, -0.03959137201309204, 0.021577566862106323, 0.015068207867443562, -0.06235865131020546, -0.011961705982685089, -0.147544726729393, -0.0014735215809196234, 0.03701134771108627, -0.07091325521469116, -0.046269312500953674, 0.06875401735305786, 0.07880326360464096, 0.0984797552227974, -5.256822491820778e-33, 0.0433151051402092, -0.03635900467634201, -0.040316447615623474, -0.02988186478614807, -0.012715339660644531, -0.04564320296049118, -0.10751594603061676, 0.0368100069463253, 0.032704152166843414, -0.038771308958530426, -0.10100457817316055, 0.1225813627243042, 0.028817320242524147, 0.028019236400723457, -0.0437149703502655, -0.08345307409763336, 0.09040517359972, -0.031706929206848145, -0.0097986264154315, 0.009942264296114445, -0.023702818900346756, 0.0637841746211052, -0.09109413623809814, 0.025126731023192406, 0.04716481268405914, 0.07647180557250977, 0.03588568791747093, 0.049045782536268234, 0.004326944705098867, 0.036076195538043976, -0.07941821962594986, -0.08948838710784912, 0.0012385845184326172, -0.0028023773338645697, -0.00796565879136324, 0.057814013212919235, 0.1361401528120041, -0.031917471438646317, -0.07349207997322083, -0.0017733483109623194, 0.05404387041926384, -0.10409026592969894, 0.06097165495157242, -0.027997173368930817, 0.0204109288752079, -0.059700142592191696, -0.07949628680944443, 0.03790135309100151, -0.054658304899930954, -0.04671724885702133, 0.03849529102444649, 0.03000340610742569, -0.07405237853527069, -0.05559099093079567, -0.04539010301232338, 0.010754914954304695, -0.026235025376081467, 0.02501603215932846, 0.03916309028863907, 0.04930213838815689, -0.08928608149290085, 0.002552747493609786, -0.03865170478820801, 0.0925549790263176, -0.06619628518819809, -0.01913047768175602, -0.04574966803193092, -0.0006900600856170058, 0.08452705293893814, -0.041182421147823334, -0.01622331514954567, 0.021342402324080467, 0.04810846224427223, -0.038461215794086456, 0.09287898242473602, -0.05708612874150276, -0.09208156168460846, -0.025077812373638153, -0.03655177727341652, -0.07252031564712524, 0.02631196193397045, 0.0008142748265527189, 0.012456411495804787, 0.048663992434740067, -0.04666167125105858, 0.040497444570064545, -0.005307269282639027, 0.041616082191467285, -0.0006103700143285096, -0.03910374641418457, -0.03678658604621887, 0.011659751646220684, -0.004149472340941429, 0.0716521143913269, -0.057704705744981766, -6.795799123437973e-08, -0.0033920565620064735, -0.013115677051246166, 0.06421120464801788, -0.022131076082587242, 0.055196329951286316, 0.02746068872511387, -0.005508122965693474, -0.03284167870879173, -0.017172083258628845, 0.032043274492025375, 0.06606956571340561, 0.038724035024642944, 0.03473208099603653, -0.032237567007541656, -0.053157154470682144, 0.10468054562807083, 0.02380530908703804, -0.011799382045865059, -0.011138186790049076, -0.006237152963876724, 0.06942421197891235, -0.02157973125576973, 0.025870777666568756, -0.00305653247050941, 0.012753921560943127, -0.012280604802072048, -0.020308837294578552, 0.024695726111531258, 0.0155285419896245, -0.023949675261974335, 0.043311409652233124, 0.011815805919468403, 0.05687312036752701, 0.0754389688372612, 0.0032198482658714056, 0.04391852766275406, 0.07933194190263748, -0.09832887351512909, 0.01363933365792036, 0.012962302193045616, -0.05272839963436127, 0.024334322661161423, 0.021394157782197, -0.030409635975956917, -0.07738470286130905, 0.008697282522916794, -0.08106022328138351, -0.07908989489078522, 0.07900922000408173, -0.07519518584012985, 0.024516738951206207, -0.039866507053375244, -0.07278640568256378, 0.008875095285475254, 0.06552782654762268, 0.014616510830819607, 0.06531855463981628, -0.08629048615694046, 0.07129410654306412, 0.05342310294508934, 0.0014454040210694075, 0.05037468299269676, -0.04048289731144905, 0.004181876312941313], [-0.11362778395414352, -0.04881398752331734, 0.04484492167830467, 0.020668260753154755, -0.04159668833017349, 0.07448402792215347, -0.03010508418083191, 0.016756579279899597, -0.05542619153857231, 0.032942138612270355, -0.056462179869413376, 0.015486114658415318, 0.04074455052614212, 0.023090267553925514, -0.05265730619430542, 0.055289942771196365, 0.026153041049838066, 0.0012498261639848351, -0.05773862823843956, -0.10302501171827316, 0.040053900331258774, -0.013002450577914715, 0.017170552164316177, -0.011214212514460087, -0.023818334564566612, 0.06990329176187515, 0.05230994522571564, 0.0324404239654541, 0.018462182953953743, 0.02495955489575863, 0.08188437670469284, -0.06280627846717834, 0.06265922635793686, -0.0446556881070137, -0.07933901250362396, 0.0795721784234047, -0.06965436041355133, -0.03690244257450104, 0.03632955998182297, -0.001296982285566628, -0.02084413543343544, -0.02829838916659355, 0.03162115067243576, -0.0001291508524445817, 0.08178959041833878, -0.00035628204932436347, -0.05807894468307495, -0.042733289301395416, 0.01794523559510708, -0.08393749594688416, -0.02805464155972004, -0.028934646397829056, 0.0033178776502609253, 0.021002918481826782, 0.014553669840097427, 0.013467772863805294, -0.03787080943584442, -0.021550381556153297, 0.03253251314163208, -0.08351615816354752, 0.04957897961139679, -0.11473658680915833, -0.014373259618878365, -0.041818536818027496, -0.03805871307849884, -0.020208772271871567, -0.022465810179710388, 0.07222812622785568, 0.0992107093334198, -0.04882914200425148, -0.063894122838974, 0.011746762320399284, 0.0396626852452755, 0.0059500765055418015, 0.0314243920147419, -0.038860078901052475, 0.029568642377853394, -0.029958512634038925, -0.0125013068318367, 0.00021781795658171177, -0.03652196377515793, -0.026414746418595314, -0.009913763962686062, 0.06182913854718208, -0.04599687457084656, -0.024898752570152283, 0.05407784879207611, 0.014848828315734863, 0.06634128838777542, 0.01309436745941639, 0.08362863212823868, 0.013124589808285236, 0.008493591099977493, -0.0112308943644166, 0.055543966591358185, 0.06434474140405655, -0.037184759974479675, -0.08809231221675873, -0.02043977566063404, 0.06331122666597366, 0.12032779306173325, 0.025800589472055435, -0.012168061919510365, -0.04857022687792778, -0.03171047568321228, -0.02172289602458477, -0.021632296964526176, 0.019055405631661415, 0.026592129841446877, -0.08670102059841156, -0.025604521855711937, 0.0020214153919368982, -0.012878838926553726, 0.026869092136621475, -0.012474463321268559, 0.032535020262002945, 0.06565828621387482, -0.06659393757581711, 0.016020579263567924, 0.06844215095043182, 0.05706867203116417, -0.05055525153875351, -0.027419866994023323, 0.02219947800040245, 0.0973193421959877, -0.09644926339387894, -0.0027949316427111626, 6.108578351586915e-33, -0.02089257538318634, 0.03859721124172211, 0.02447088249027729, 0.0798841267824173, 0.013379944488406181, 0.0017567949835211039, 0.0967685878276825, 0.02130112051963806, 0.00471021281555295, -0.00681693060323596, 0.034809842705726624, 0.05966409668326378, -0.0720633864402771, 0.04773255065083504, 0.014294612221419811, -0.12170702964067459, -0.010176658630371094, -0.03750641271471977, 0.04711974784731865, -0.01575314812362194, 0.00047021283535286784, -0.02399514988064766, 0.05552702397108078, -0.07341654598712921, 0.03424502909183502, 0.07726215571165085, 0.08562499284744263, -0.06768076866865158, 0.015192152932286263, -0.0077242678962647915, 0.007665305398404598, 0.04836210981011391, -0.15165436267852783, -0.0043424335308372974, -0.008625680580735207, 0.08124326169490814, 0.02353532239794731, -0.019985880702733994, 0.008783205412328243, 0.01264073047786951, -0.030799739062786102, 0.045445870608091354, -0.009623629041016102, -0.0071833450347185135, -0.036593589931726456, -0.01000828668475151, 0.0713716670870781, -0.055566027760505676, 0.010564855299890041, -0.02625010721385479, -0.03776409104466438, -0.050574079155921936, -0.03308665752410889, -0.09211866557598114, -0.00398889509961009, 0.08116158097982407, -0.06837035715579987, 0.02691560611128807, -0.033613670617341995, -0.0075346799567341805, 0.06314930319786072, -0.031025884672999382, 0.0020981996785849333, 0.08067216724157333, -0.028938177973031998, 0.08846928179264069, -0.028504351153969765, 0.0369875393807888, 0.09203893691301346, -0.00639862148091197, -0.00882688071578741, 0.03526923432946205, 0.11665086448192596, -0.054772477596998215, 0.09999118000268936, 0.016833337023854256, 0.05853809043765068, -0.1037687435746193, -0.04000617191195488, 0.020233631134033203, -0.05036965757608414, 0.02940581738948822, -0.01777428388595581, -0.01703512668609619, 0.0015459536807611585, -0.050238385796546936, 0.03454107791185379, -0.011350642889738083, 0.014850982464849949, -0.043244827538728714, -0.03264358267188072, -0.04755876213312149, 0.030942628160119057, 0.10663295537233353, 0.0287152249366045, -7.164035627407046e-33, 0.05195535346865654, -0.013898412697017193, -0.02492164634168148, -0.04945121705532074, 0.0332651324570179, 0.0034366436302661896, -0.1321355402469635, 0.013649712316691875, 0.04475413262844086, 0.0016442440683022141, -0.044975247234106064, 0.04002263769507408, 0.026419775560498238, 0.049921151250600815, -0.0622522309422493, -0.04864916577935219, 0.05014950782060623, -0.036531466990709305, 0.03424198180437088, 0.019463030621409416, -0.059481389820575714, 0.04661859571933746, -0.12729503214359283, 0.014959747903048992, 0.051771651953458786, 0.021532079204916954, 0.04516156390309334, 0.06300420314073563, -0.038615647703409195, 0.06533703953027725, 0.00822364166378975, -0.09922593086957932, 0.06754106283187866, -0.021754048764705658, -0.0610620453953743, 0.08809482306241989, 0.10336050391197205, -0.014186185784637928, -0.026233848184347153, -0.023644905537366867, 0.0751374140381813, -0.06945618987083435, 0.0487632229924202, -0.13324429094791412, -0.02426559291779995, -0.08532093465328217, -0.0583004392683506, 0.07828465849161148, -0.09713131189346313, 0.011055780574679375, -0.0010894750012084842, 0.0027446262538433075, -0.06793641299009323, -0.11597616225481033, -0.017059477046132088, 0.01281803473830223, 0.007065571844577789, 0.05300695821642876, 0.039342645555734634, -0.006065825466066599, -0.12129168212413788, 0.00024688232224434614, -0.04591875523328781, 0.022582819685339928, -0.04945291578769684, -0.029270334169268608, -0.014769623056054115, -0.017714397981762886, 0.03176121786236763, -0.027789033949375153, -0.07996831089258194, 0.08852019906044006, 0.06971315294504166, -0.025847168639302254, 0.08671065419912338, -0.09972425550222397, -0.03961735591292381, 0.019025225192308426, -0.04045781493186951, -0.046119574457407, 0.013106155209243298, -0.01804298162460327, 0.029785623773932457, 0.05140738934278488, -0.014436442404985428, 0.0632929652929306, 0.030048955231904984, 0.07628440111875534, 0.025887506082654, -0.0545719750225544, -0.08765517920255661, 0.006203354801982641, 0.060615330934524536, -0.007073872722685337, -0.07087522000074387, -6.461866064455535e-08, -0.04189907759428024, -0.02383640967309475, 0.05091071128845215, 0.021578021347522736, 0.0423547737300396, 0.0294320210814476, -0.03600334748625755, -0.027063561603426933, -0.039999913424253464, -0.014724258333444595, 0.036052364856004715, 0.021463751792907715, 0.024120481684803963, -0.028373776003718376, -0.01550036109983921, 0.0998498722910881, 0.02703595533967018, -0.08072381466627121, -0.0065856436267495155, -0.03426911681890488, 0.08745195716619492, -0.052289944142103195, 0.005651995539665222, -0.042710963636636734, 0.026645107194781303, -0.043741658329963684, 0.0053185666911304, 0.031282149255275726, -0.0190722793340683, -0.03507566824555397, -0.00100443116389215, -0.0070920055732131, 0.029403679072856903, 0.04573959857225418, 0.055244412273168564, 0.014891366474330425, 0.0888994112610817, -0.05914340540766716, -0.009526287205517292, -0.020357713103294373, -0.0353378988802433, 0.04158688336610794, 0.06687872856855392, -0.023784277960658073, -0.014544862322509289, -0.01268273126333952, -0.09932795912027359, -0.11163291335105896, 0.07550503313541412, -0.07183481752872467, 0.06487690657377243, 0.0076247043907642365, -0.027437595650553703, 0.021135946735739708, 0.07440956681966782, 0.05800019949674606, 0.05421743169426918, -0.04799005016684532, 0.015078775584697723, 0.05973287299275398, 0.002510505961254239, 0.05014168471097946, -0.06110936775803566, 0.03312963992357254]], 'metadatas': [{'source': 'applying RL to build a binanace trading bot.txt'}, {'source': 'reinforcement learning DQN part 1.txt'}, {'source': 'reinforcement learning and introduction part 4.txt'}, {'source': 'reinforcement learning and introduction part 3.txt'}, {'source': 'reinforcement learning and introduction part 2.txt'}, {'source': 'reinforcement learning and introduction part 1.txt'}], 'documents': ['Our objective is to make a trading bot that trade cryptocurrency using state-of the-art reinforcment learning. To create our RL agents we will use the following technologies:\\n\\nPython\\nReinforcment Learning\\nOPenAI gym\\nBinance\\nYou don’t need any background in ML to understand the following articles, knowledge of Python will be enough. However, if a part is unclear do not hesitate to contact me.\\n\\nHere are the different parts of the creation of our bot, each part will be one article:\\n\\nWe will use Binance data to generate our data that we will customize for our need. All you need is a Binance account, you can create one by clicking here. It is free and one of best trading platform to find cryptocurrency data.\\n\\nAll of the code for this article will be available on my GitHub.\\n\\nSteps for generating data:\\n\\nAdd our Binance API keys\\nPulling data from Binance\\nStandardize our data as we wish\\nSave it in a csv file\\nAdd our Binance API keys\\nAfter cloning the repository, we will need to add our binance keys. We can generate them on the Binance site. Then we just have to add them to keys.py.\\n\\napiKey = “”\\nsecretKey = “”\\nAfter the adding:\\n\\napiKey = “glebfxHdBtDR3RWt3hVRtHmvQaYduT3WodIRdKCOg51qwQsRWbn3aeS298GINGaM”\\nsecretKey = “NSBJOldWu9xXARDCcJRdQ5C0uVth3Qx5bY4y9YyKJubvz7HPcbJL3Racao9xEo1o”\\nThat’s it ! We are connected to the API.\\n\\nPulling data from Binance\\nFor all the steps below we will works on main.py.\\nThe first step is to import a bunch of things that you may be familiar with, we will explain them after:\\n\\nfrom binance.client import Client\\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\\nfrom matplotlib import pyplot\\nimport pandas as pd\\nimport keys\\nimport os\\nIn our case we want BTCUSDT (Bitcoin/Tether) but this script can create data for any pair available on Binance. We want the data since 2 year ago, but we can change this if wanted. The data will be for every minutes so it’s approximately 1 million rows of data.\\n\\n#Changes these values to obtain the desired csv\\npair = os.getenv(“PAIR”, “BTCUSDT”)\\nsince = os.getenv(“SINCE”, “2 year ago UTC”)\\nTo connect to our Binance account we use Client(apikey, secretkey), then we will have our dataframe df that we could work on.\\n\\n#Connect to the Binance client\\nclient = Client(keys.apiKey, keys.secretKey)\\n# Get the data from Binance\\ndf = client.get_historical_klines(pair, Client.KLINE_INTERVAL_1MINUTE, since)\\nWe want to normalize our data (make him between 0 and 1). However we still want to know the “Real Open” and the “Real CLose” it will be usefull to evaluate our bot.\\n\\n# Store the open and close values in a pandas dataframe\\nreal_values = []\\nfor i in df:\\n    real_values.append([i[1], i[3]])\\nreal_df = pd.DataFrame(real_values, columns=[“Real open”, “Real close”])\\nNormalizing data is easy with sklearn, after this all our data are between 0 and 1, exept for the Real Open and the Real Close of course.\\n\\n# Normalize the data\\nnormalizer = MinMaxScaler().fit(df)\\ndf = normalizer.transform(df)\\nWe then transform our data to a pandas array and drop useless columns “Open Time”, “Close Time” and “Ignore”.\\n\\n# Transform the data to a pandas array\\ndf = pd.DataFrame(df,\\ncolumns=[\\n“Open time”, “Open”, “High”, “Low”, “Close”, “Volume”,\\n“Close time”, “Quote asset volume”, “Number of trades”,\\n“Taker buy base asset volume”,\\n“Taker buy quote asset volume”, “Ignore”\\n])\\n# Drop useless columns\\ndf.drop([“Open time”, “Close time”, “Ignore”], axis=1, inplace=True)\\nPlot the data ! It will give us a good idea if our generation is successful.\\n\\n#Plot the data\\ndf.plot()\\nWe then add our 2 dataframes. We also need to drop the last timesteps, because it is possibly not finished.\\n\\n# Add the real open and the real close to df\\ndf = pd.concat([df, real_df], axis=1)\\n# Remove the last timestep\\ndf = df[:-1]\\nOur csv file will be in our folder datafile and at the end we will show the plot.\\n\\n# Add to the csv file\\ndf.to_csv(“datafile/” + pair + “.csv”)\\n#Show the plot\\npyplot.show()\\nTest our script\\nRunning can take a bit of time, for 2 years of data I needed 15 minutes.\\n\\nRunning main.py give us a plot:\\n\\n\\nBut above all it gives us a precious csv file in the folder datafile that we will use for our trading agent.\\n\\nOpenAI’s gym is by far the best packages to create a custom reinforcement learning environment. It comes with some pre-built environnments, but it also allow us to create complex custom environments. An environment contains all the necessary functionality to run an agent and allow it to learn.\\n\\nOur goal is to recreate Binance with the same fees, the same data, and let our agent learn from these. If you do not have a Binance account you can create one by clicking here. A common mistake is to make environments who do not match reality and are to easy for our agents to learn, it’s what we will try to avoid here.\\n\\nIt’s really hard to make money on the stock market/cryptocurrency market. If our bot make +10% per month, it’s probably not sustainable or just pure luck. Being able to trade and keeping our money with paying our fees is already really impressive, let’s do our best !\\n\\nAll of the code for this article is available on my GitHub.\\n\\nThe custom environment\\nFirst let import what we will need for our env, we will explain them after:\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport gym\\nimport random\\nfrom gym import spaces\\nimport static\\nA custom environment is a class that inherits from gym.env.\\n\\nclass CryptoEnv(gym.Env):\\ndef __init__(self, df, title=None):\\nself.df = df\\nself.reward_range = (-static.MAX_ACCOUNT_BALANCE,\\nstatic.MAX_ACCOUNT_BALANCE)\\nself.total_fees = 0\\nself.total_volume_traded = 0\\nself.crypto_held = 0\\nself.bnb_usdt_held = static.BNBUSDTHELD\\nself.bnb_usdt_held_start = static.BNBUSDTHELD\\nself.episode = 1\\n# Graph to render\\nself.graph_reward = []\\nself.graph_profit = []\\nself.graph_benchmark = []\\n# Action space from -1 to 1, -1 is short, 1 is buy\\nself.action_space = spaces.Box(low=-1,\\nhigh=1,\\nshape=(1, ),\\ndtype=np.float16)\\n# Observation space contains only the actual price for the     moment\\nself.observation_space = spaces.Box(low=0,\\nhigh=1,\\nshape=(10, 5),\\ndtype=np.float16)\\ndf : The dataframe that we have created in the past article\\n\\nreward_range : Not really usefull but needed, let’s make it be between 2 huge numbers (static is the file where we stored all of our constants). Let’s make it between -10 millions and +10 millions.\\n\\ntotal_fees : Keep track of the total fees paid\\n\\ntotal_volumes_traded : Keep track of the the total trading volume\\n\\ncrypto_held : Keep track of the crypto held (Bitcoin in our case)\\n\\nbnb_usdt_held, bnb_usdt_start : To track our USDT ( We can easily change the pair)\\n\\nepisode : The number of our current episode (start at 1)\\n\\ngraph_reward, graph_profit, graph_benchmark are used to render the result.\\n\\naction_space : We set our action space between -1 and 1. 1 means using 100% of our USDT to buy BTC, 0 means doing nothing, -1 means selling all of our BTC for USDT.\\n\\nThe method reset()\\ndef reset(self):\\nself.balance = static.INITIAL_ACCOUNT_BALANCE\\nself.net_worth = static.INITIAL_ACCOUNT_BALANCE + static.BNBUSDTHELD\\nself.max_net_worth = static.INITIAL_ACCOUNT_BALANCE\\nstatic.BNBUSDTHELD\\nself.total_fees = 0\\nself.total_volume_traded = 0\\nself.crypto_held = 0\\nself.bnb_usdt_held = static.BNBUSDTHELD\\nself.episode_reward = 0\\n# Set the current step to a random point within the data frame\\n# Weights of the current step follow the square function\\nstart = list(range(4, len(self.df.loc[:, ‘Open’].values) —    static.MAX_STEPS)) + self.df.index[0]\\nweights = [i for i in start]\\nself.current_step = random.choices(start, weights)[0]\\nself.start_step = self.current_step\\nreturn self._next_observation()\\nWe decide to start our current_step to a random point in our dataframe. But not totally random, we choose it so that the older the data the least it is choosed at a starting point. Make sense right ? What append yesterday has more impact then what append 2 years ago.\\n\\nThe method _next_observation_()\\ndef _next_observation(self):\\n# Get the data for the last 5 timestep\\nframe = np.array([\\nself.df.loc[self.current_step — 4:self.current_step, ‘Open’],\\nself.df.loc[self.current_step — 4:self.current_step, ‘High’],\\nself.df.loc[self.current_step — 4:self.current_step, ‘Low’],\\nself.df.loc[self.current_step — 4:self.current_step, ‘Close’],\\nself.df.loc[self.current_step — 4:self.current_step, ‘Volume’],\\nself.df.loc[self.current_step -\\n4:self.current_step, ‘Quote asset volume’],\\nself.df.loc[self.current_step -\\n4:self.current_step, ‘Number of trades’],\\nself.df.loc[self.current_step -\\n4:self.current_step, ‘Taker buy base asset volume’],\\nself.df.loc[self.current_step -\\n4:self.current_step, ‘Taker buy quote asset volume’]\\n])\\n# We append additional data\\nobs = np.append(frame, [[self.balance /  static.MAX_ACCOUNT_BALANCE,\\nself.net_worth / self.max_net_worth,\\nself.crypto_held / static.MAX_CRYPTO,\\nself.bnb_usdt_held / self.bnb_usdt_held_start,\\n0]],\\naxis=0)\\nreturn obs\\nWe take here the data we want our agent to know before making the decision to buy or sell. We decide to give the 5 last timeframes, so our agent will know the Open of the last 5 timesteps (he will also know other things like the Close, the Volume,…). We also pass him our current balance, net worth and the amount of crypto held.\\n\\n\\nThe method _take_action()\\ndef _take_action(self, action):\\n# Set the current price to a random price between open and close\\ncurrent_price = random.uniform(\\nself.df.loc[self.current_step, ‘Real open’],\\nself.df.loc[self.current_step, ‘Real close’])\\nif action[0] > 0:\\n# Buy\\ncrypto_bought = self.balance * action[0] / current_price\\nself.bnb_usdt_held -= crypto_bought * current_price *  static.MAKER_FEE\\nself.total_fees += crypto_bought * current_price * static.MAKER_FEE\\nself.total_volume_traded += crypto_bought * current_price\\nself.balance -= crypto_bought * current_price\\nself.crypto_held += crypto_bought\\nif action[0] < 0:\\n# Sell\\ncrypto_sold = -self.crypto_held * action[0]\\nself.bnb_usdt_held -= crypto_sold * current_price * static.TAKER_FEE\\nself.total_fees += crypto_sold * current_price * static.TAKER_FEE\\nself.total_volume_traded += crypto_sold * current_price\\nself.balance += crypto_sold * current_price\\nself.crypto_held -= crypto_sold\\nself.net_worth = self.balance + self.crypto_held * current_price + self.bnb_usdt_held\\nif self.net_worth > self.max_net_worth:\\nself.max_net_worth = self.net_worth\\nThis method make us buy or sell depending on the action taken and calculate the new net_worth.\\n\\nThe metod step()\\ndef step(self, action, end=True):\\n# Execute one time step within the environment\\nself._take_action(action)\\nself.current_step += 1\\n# Calculus of the reward\\nprofit = self.net_worth — (static.INITIAL_ACCOUNT_BALANCE +\\nstatic.BNBUSDTHELD)\\nprofit_percent = profit / (static.INITIAL_ACCOUNT_BALANCE +\\nstatic.BNBUSDTHELD) * 100\\nbenchmark_profit = (self.df.loc[self.current_step, ‘Real open’]   / self.df.loc[self.start_step, ‘Real open’] - 1) * 100\\ndiff = profit_percent — benchmark_profit\\nreward = np.sign(diff) * (diff)**2\\n# A single episode can last a maximum of MAX_STEPS steps\\nif self.current_step >= static.MAX_STEPS + self.start_step:\\nend = True\\nelse:\\nend = False\\ndone = self.net_worth <= 0 or self.bnb_usdt_held <= 0 or end\\nif done and end:\\nself.episode_reward = reward\\nself._render_episode()\\nself.graph_profit.append(profit_percent)\\nself.graph_benchmark.append(benchmark_profit)\\nself.graph_reward.append(reward)\\nself.episode += 1\\nobs = self._next_observation()\\n# {} needed because gym wants 4 args\\nreturn obs, reward, done, {}\\nThis method calculate our reward. The choice of the formula is crucial for our bot, it depends to the profit we have made (easily understandable) but it also depends on the benchmark profit. It is made that way because it is easy for our agent to make +1% when the BTC make +10% and it is hard to keep our money when BTC goes -10%. We have to reward the agent when he choose the best solution not when he make money.\\n\\nRender the choice of our agent\\nWe have make 2 method that render, one render a summary of our balance, crypto held and profit for each step and one render at the end of each episode. We also plot a graph to have a a better visualisation.\\n\\nConclusion\\nWe have made a environment close to the Binance site, we did not forget fees that can be change on the static.py file. The reward function can be tested and changed if we find a better one but this one will do the work for the moment.\\n\\nOur objective is to train an agent by making him on the binance environment created in the last article. Let’s see if he can make money trading Bitcoin. All of the code for this article is available on my GitHub.\\n\\nIn this article we will be using OpenAI’s gym and PPO agent from the stable-baseline library. We already have a env.py which contains our RL environment, a static.py file which contains all of our constants like fees paid and a .csv file which contains the past Bitcoin data that we have created in this article.\\n\\nWe will create a file main.py that we will run and that will train our agent.\\n\\nThe main.py file\\nFirst let import what we will need for our env, we will explain them after:\\n\\nfrom stable_baselines.common.policies import MlpPolicy\\nfrom stable_baselines.common.vec_env import DummyVecEnv\\nfrom stable_baselines import PPO2\\nfrom env import CryptoEnv\\nimport pandas as pd\\nimport os\\nThen we read the data that we have put on a data folder:\\n\\ndf = pd.read_csv(‘data/BTCUSDT.csv’, index_col=0)\\nWe have imported our environment created (CryptoEnv), the way to instantiate it is not really instinctive:\\n\\nenv = DummyVecEnv([lambda: CryptoEnv(df)])\\nWe create our agent that will try his best to trade on our environment. We can change parameters like gamma or the learning rate to have better results.\\n\\n# Instanciate the agent\\nmodel = PPO2(MlpPolicy, env, gamma=1, learning_rate=0.01, verbose=0)\\nWe then train the agent during 5000000 timesteps:\\n\\n# Train the agent\\ntotal_timesteps = int(os.getenv(‘TOTAL_TIMESTEPS’, 500000))\\nmodel.learn(total_timesteps)\\nAnd let’s render if we succeed to increase our reward over time:\\n\\n# Render the graph of rewards\\nenv.render(graph=True)\\nAfter training we need to check if he can predict the market:\\n\\n# Trained agent performence\\nobs = env.reset()\\nenv.render()\\nfor i in range(100000):\\naction, _states = model.predict(obs)\\nobs, rewards, done, info = env.step(action)\\nenv.render(print_step=True)\\nRun main.py !\\nLet’s see the result for a short training:\\n\\n\\nSo after a few training, what i see is that the agent is doing very poorly at first because he is trading WAY TOO MUCH and he is paying the trading fees each time.\\n\\nAfter ~50 episodes he understand that trading comes with a cost. After ~1000 episodes he barely trade like he know that he will just loose money most of the time.\\n\\nIt’s a great news ! That means we have succeeded to create an environment close to the real one. Surpassing the 0% profit is the real challenge, for the moment we have only recreated the environment.', 'Hi and welcome to the intro series on Reinforcement Learning, today’s topic will be about the DQN algorithm!\\n\\n\\nThis blog post is part of a longer series about Reinforcement Learning (RL). If you are completely unfamiliar with RL, I suggest you read my previous blog posts first.\\n\\nPreviously we talked about policy gradient algorithms. Today we will have a look at a different family of RL algorithms: Q-learning algorithms. And more specifically, we will focus on the vanilla DQN-algorithm.\\n\\nThe topics for today’s blog post are:\\n\\nHistorical significance of DQN\\nWhat is Q-Learning?\\nDQN Explained\\nQ-Learning VS. Policy Gradients\\nThere will also be a part 2 for today’s blog post, which will include a basic implementation of DQN.\\n\\nNote: If you are reading this on a smartphone browser, you might not be able to view the subscripts. You can download the Medium app to mitigate this.\\n\\nHistorical significance of DQN\\nThe algorithm we will discuss today is called DQN. You might wonder why we would discuss such an old algorithm at this point, and you’d be right about the fact that DQN is in fact quite old by now (the paper was published in 2013, an age ago in the software industry!). But despite its age, DQN carries some historical significance as the original paper by Mnih et al. was the first paper to successfully combine deep learning (the use of deep neural networks) with reinforcement learning. Furthermore, some of the ideas from this paper are still valuable today.\\n\\nThe paper specifically showed how a reinforcement learning agent can learn how to play Atari games, directly from pixels. In other words, the algorithm directly takes images as input, just like a human would see them, and outputs which actions to take to successfully play these games.\\n\\n\\nAlthough the idea of Q-learning was not new at the time, Q-learning hadn’t been directly applied before to high-dimensional inputs (like images). Of course, the algorithm was not perfect yet. DQN had many similar problems as the ones we have today, such as sample inefficiency, but it heralded a new age for reinforcement learning, the age of Deep Reinforcement Learning.\\n\\n\\nIn the picture above, you can see a performance comparison of various versions of DQN that have come out since the original. Even to this day, these algorithms remain a popular choice for model-free, off-policy RL.\\n\\nWhat is Q-Learning?\\nBefore we dive into the specifics of DQN, let’s first talk about Q-learning in general.\\n\\nIn the previous blog posts we focussed on policy gradient algorithms. Policy gradient algorithms try to optimise the policy/behaviour of an agent by directly changing the policy. This, however, is not the case for Q-learning algorithms. Q-learning algorithms try to exploit the fact that: if we know the Q-function, we can construct an optimal policy by selecting the action at every state that has the highest Q-value.\\n\\nLet’s back up for a bit and go over some of the terminology. We are still operating in the typical RL framework: we have an agent taking actions in an environment at every discrete timestep. The agent starts in a certain state (or receives an observation of the state). After taking an action, the agent receives a new observation, a (possibly negative) reward, and the agent will be in a new state or receives a new observation.\\n\\n\\nThe goal of reinforcement learning is to learn a policy (a behaviour) that maximizes the agent’s expected cumulative reward. Or in other words, we want the agent to behave in a way such that the reward it receives over time is maximal.\\n\\nIn previous blog posts, we have talked about the value function:\\n\\n\\nThe value function tells us how valuable it is to be in state s when following policy π. It is equal to the summed reward of the trajectory that is expected to follow from state s.\\n\\n\\nA closely related function, is the Q-function:\\n\\n\\nThe Q-function tells us how valuable it is to be in state s and take action a while following policy π.\\n\\n\\nNow let’s say that we denote the Q-function for an optimal policy (a policy that maximises the cumulative reward) as Q*. The optimal Q-function needs to adhere to one important property, the Bellman equation:\\n\\n\\nThe Bellman equation tells us that, for the optimal policy, the value of taking action a while in state s, is equal to the immediate reward r, plus the value of taking the best possible action in the next state (multiplied by discount factor gamma). In other words, the Bellman equation is a recursive formula, the value of taking an action is equal to the reward you get for that action plus following the optimal policy from then on.\\n\\nThe idea behind Q-learning is that we can learn or approximate the optimal Q-function by iteratively updating our estimate of it based on the Bellman equation.\\n\\nIf you haven’t quite caught on to the idea yet, don’t worry, things will probably become more clear in the next section. There we will look at this idea from a more practical point of view.\\n\\nDQN Explained\\nSo, our goal is to find a good approximation of the Q-function. If we can do this, we can make our agent behave optimally by selecting the action that produces the highest Q-value (gives the highest reward) at every state.\\n\\nA rough outline of the algorithm would look like this:\\n\\nInitialize Q-function approximation\\nRepeat:\\n    Collect experience\\n    Update Q-function approximation\\nAs you can tell, we repeatedly switch between collecting new experience and updating our approximation of the q-function. We switch because crucially the states and action we will see, also depend on the policy/behaviour of our agent. We may only encounter certain states after we have already learned certain strategies.\\n\\nQ-function representation\\nIn order to make an estimate of the Q-function, we will need to represent it somehow. In earlier versions of Q-learning, people tried to keep track of a table that contained the exact reward for every state and action that the agent took. In practice this is often infeasible, especially when our state is highly dimensional such as when we use images for representing the state (remember, images are just matrices/tensors with pixel intensity values). One of the innovations the original DQN paper brought, was using a neural network to approximate the Q-function.\\n\\n\\nIn theory the neural network should take a state and an action as input and output the corresponding Q-value. In practice, we will only use the state as input and output a Q-value for each possible action. This makes the learning process a bit easier and saves some computation costs.\\n\\nInitially the neural network weights will be initialised with random values, over time we update the values for a more accurate estimate based on the collected experience.\\n\\nReplay memory\\nAs is mentioned above, we continuously collect new experience. An advantage of DQN is that it is an off-policy algorithm, meaning that it can learn from collected experience, even if that experience was collected with an older version of the policy.\\n\\n\\nCollecting experience happens by letting our agent operate in the environment and storing so-called transitions in a replay buffer/replay memory. A transition refers to the combination of a state, action, next state, reward and an indication whether the episode was over after this transition.\\n\\nDuring the learning process the buffer will be filled with transitions, while we update our q-function estimate based on the already collected transitions. Over time, older transitions will be replaced with new ones once the maximum capacity of the memory is reached.\\n\\nSome variations of DQN use prioritised replay memory, where transitions that are more significant for achieving high reward are more often sampled from the memory.\\n\\nQ-function update\\nFor updating our q-function estimate, we make use of the Bellman equation. We make a prediction of the state from our transition and for the next state in our transition. We then calculate the gradient based on the difference between our Q-value estimate for state sₜ and the received reward plus the maximum Q-value prediction of the next state sₜ₊₁(multiplied by a discount factor).\\n\\n\\nNote that, in order to do the gradient update for the q-function, we need to mask out the predicted values for the actions that weren’t actually taken in the sampled transition.\\n\\nFor example, if we fetch a (non-terminal) transition from our replay memory that contains the following values: state s₇, action a₃, next state s₈ and reward 10. Our Q-value estimate for (s₇, a₃) might be 20 and the maximum Q-value estimate (the highest value for any of the actions) for s₈ may be 8. If we use the mean-squared error, the loss would be 400–324. We get 400 by taking the second power of the estimate for s₇ and a₃ (20²) and we get 324 by taking the second power of the estimate for the highest Q-value for s₈ plus the actual observed reward ((8+10)²).\\n\\nThe loss we just calculated is only valid for action a₃ and not for the other Q-values produced by our neural network, so we want to mask out those other values/gradients, such that we don’t influence the variables in the neural network used to estimate the Q-values of the other actions.\\n\\nEpsilon-greedy policy\\nWe are now close to having all the ingredients needed for our algorithm. One thing that is still missing, is a way for deciding which actions our agent should take during the process of collecting experience. This is the classical exploration-exploitation trade-off. On one hand, we can let our agent take random actions to discover new strategies (exploration), but if we only act randomly, we don’t use the knowledge we have already gained from previous experience. On the other hand, if we only use what we have already learned in the past (exploitation), we won’t discover any new strategies. Hence, the trade-off.\\n\\nSome algorithms deal with this trade-off in a natural way. For example, some policy gradient algorithms output a distribution over actions and from time to time actions get selected that are not optimal according to the algorithm. DQN on the other hand, deals with this trade-off very explicitly by using a so-called epsilon-greedy strategy. Despite the fancy name, the idea is rather simple. We will define a value epsilon ϵ. Every time we need to choose an action, we will generate a new value. If that value is greater than epsilon ϵ, we will select the action that is optimal according to our Q-function (the action with the highest Q-value should give the highest reward). If the generated value is smaller than epsilon ϵ on the other hand, we will select a random value.\\n\\nDuring the course of the training, we can decrease the value of epsilon ϵ, such that our algorithm will do more exploitation over time.\\n\\nPutting it all together\\nAlright, with all this knowledge in mind, I can now present the algorithm as it was described by the original paper. Let’s take a look and go over it.\\n\\n\\nIn the first lines, we initialise our empty replay memory with capacity N and an initially random Q-function. The capacity N refers to the maximum number of transitions that will be stored in this memory.\\n\\nWe then start from an initial state (referred to as a “sequence” in the algorithm). The original authors also talk about “preprocessed sequences”, the preprocessed part refers to the fact that some states/sequences might need some preprocessing before they are fed into the neural network. For example, in the case of a game, they stack several images of the game together, because a single image is not enough to determine how certain objects are moving (see image).\\n\\n\\nNext, we perform an action based on the epsilon-greedy strategy (either random or based on the Q-function estimate), and we store all the information we get from taking that action. The stored information contains the state, the action taken, the reward and the next state.\\n\\nWe then go on to sample some of the transitions from our replay memory and use the formula described above to calculate the loss. Note that for a terminal state (a transition where the episode is ended), solely the reward is used as a target instead.\\n\\nWe then perform a gradient step (update the weights of our neural network) and we repeat this until we are satisfied with the results. That’s it! You now have a solid understanding of the basics of DQN.\\n\\nQ-Learning VS. Policy Gradients\\nAs a short final note, I want to comment a bit on the difference between Q-learning and the algorithms we previously mentioned (Policy Gradient algorithms).\\n\\nPolicy Gradient algorithms are sometimes regarded as being more stable. They are principled because we are directly optimising the policy. This is not the case for Q-learning, in the case of Q-learning we are constructing a policy by selecting an action for every state based on our estimate of the Q-function.\\n\\nThe advantage of Q-learning is that it can be used in an off-policy way. Since we store transitions in a buffer, we can later on reuse transitions for which the actions haven’t necessarily been generated by the latest policy. This makes Q-learning more sample-efficient, since we need to interact less with the environment.\\n\\nBoth methods come with their own trade-offs, and some of the methods we will explore later on, try to achieve the best of both worlds. Stay tuned if you want to learn more about this.\\n\\nConclusion\\nCongratulations for making it all the way to the end of this article! You should now have a solid understanding of Q-learning and DQN. This knowledge will be valuable when we explore even more advanced algorithms in the future. In the next part of this article, I will present an implementation of DQN. The implementation will be built further upon the educative RL framework we are developing for this blog (see the REINFORCE-implementation).', 'Welcome back to the final part of this introduction series to Reinforcement Learning!\\n\\n\\nIn this final part, we are going to make our hands dirty and implement the REINFORCE algorithm that we talked about in part 3. After this you will have a working algorithm that you can use to solve some simple tasks. On top of that, you should have a proper understanding of Policy Gradient algorithms, which you can use to tackle more complex problems and algorithms.\\n\\nThe content of today looks like this:\\n\\nDevelopment Environment Setup\\nThe RL Environment Abstraction\\nImplementing REINFORCE\\nConclusion\\nLet’s dive in!\\n\\nDevelopment Environment Setup\\nThe first thing every developer needs is a development environment. I don’t want to enforce any specific setup or tools, but in case you are a beginner, I will just walk you through to the tools I personally use.\\n\\nIDE (Code editor)\\nMy choice of code editor is Visual Studio Code. It is a lightweight editor which you can use for multiple programming languages. The editor comes out of the box with very minimal tooling, but allows you to add more functionality through extensions.\\n\\nDownload VS Code here\\n\\n\\nPython\\nThe programming language we will be using is Python, a high-level programming language. By high-level we mean that the language takes care of a lot of things for you like memory management. It is also an interpreted and dynamically typed language. You can simply download and install Python like any other application. Personally I use version 3.8 for maximum compatibility with several libraries, but for this tutorial you can just go on and install the latest available version.\\n\\nDownload Python here\\n\\nVirtual environment setup (Optional, recommended)\\nFor this tutorial we will be using some libraries (code that we import to use in our own project). You typically install these libraries through a package manager. In the case of Python, the default package manager is called pip. We will use pip to install some packages. It is however a good idea to create a so-called virtual environment. A virtual environment isolates the packages you installed from other (virtual) environments. Having this isolation is often a good idea, because it allows you to install multiple versions of a package at the same time (but in different environments).\\n\\nThere are multiple benefits of creating a virtual environment and also multiple ways to do it. For now we will just look at one particular way. We start by opening a terminal. Navigate to the folder where you intend the code to be and type the following commands:\\n\\n# For Windows\\npy -m venv env\\n# For Mac OS/Linux\\npython3 -m venv env\\nThis tells Python (version 3.X) to create a new virtual environment named env. Next up, we will need to activate this environment by using:\\n\\n# For Windows\\n\\\\env\\\\Scripts\\\\activate\\n# For Mac OS/Linux\\nsource env/bin/activate\\nThat’s it! Commands you enter will from now on be executed in your virtual environment. To deactivate the environment, simply type deactivate. To activate the environment again, simply type the command mentioned above. Do make sure you are in the same folder as where you have created the environment (your project folder).\\n\\nPyTorch\\nIn the next step of our setup, we will install a deep learning framework. There are many frameworks available, but the industry standards at the time of writing are either TensorFlow or PyTorch. We will be using the latter one although nothing prevents you from opting for TensorFlow (or other frameworks) instead.\\n\\n\\nIn previous posts we talked about neural networks, gradients etc. PyTorch will make abstractions of a lot of these things for us and make our life easier by providing ready to use implementations for these concepts.\\n\\nIn order to install PyTorch, you will need to open a terminal and enter the commands provided to you on this website:\\n\\nhttps://pytorch.org/get-started/locally/\\n\\nYou should select Stable, your operating system, Pip, Python and Default.\\n\\nAdditional packages\\nSome smaller additional pip packages we will use are:\\n\\nrandomname\\ntensorboard\\nYou can just use “pip install nameofpackage” to install these.\\n\\nThe RL Environment Abstraction\\nIn order to train our RL algorithm to solve “any” problem, we are going to make an abstraction of the environment. For this we will use the popular framework called Gym by OpenAI:\\n\\nhttps://github.com/openai/gym\\n\\nYou can install the Gym framework like any other pip package from the link above. Crucially, Gym provides us with several abstractions that we can apply to any environment and problem:\\n\\nYou can create an environment with gym.make(“environment-name”) function\\nAfter creating the environment, you can use the env.step(action) to take an action and go to the next state. This function will provide you with the observation of the next state, the reward granted, a boolean indicating whether the episode ended, and some optional metadata\\nThe env.reset() function will reset the environment to a starting state (and return the values of this starting state)\\nFinally, the env.close() function will clean up any resources allocated by the environment\\nWe will go even one step further than using vanilla Gym. We will write our own Environment class, which contains some additional variables and helper functions, which will mostly help us later on when implementing more complex algorithms.\\n\\nImplementing Reinforce\\nWe will now start writing some code. First we will need a class that can represent our policy (Neural network).\\n\\nNeural Network\\n\\nnet/variable_type.py\\nWe start by defining a small enumeration. We will use this Enum to distinguish between the different types neural networks we could expect.\\n\\nDiscrete represents a discrete output, or more precisely a distribution of probabilities over a fixed number of dimensions.\\nContinuous represents a continuous value.\\nGaussian represents a gaussian distribution output, meaning we output a mean and standard deviation.\\n\\nnet/dnn.py\\nOur next class represents our neural network. We start with a fairly complex constructor (the __init__ function). Even though there is a lot of code, all this constructor does is construct a neural network, given some configuration. The most crucial parameters are dim_in, dim_out and layer_sizes. They represent the input dimension size, output dimension size and the number of neurons in the hidden layers, respectfully.\\n\\nNext we have a forward function, which is typical in PyTorch. This function does a forward pass through our neural network (see the resources here if you want to learn more about how PyTorch handles neural networks).\\n\\nLastly we have a sample function. This function performs a forward pass through the network, constructs a distribution (categorical or gaussian) and then takes a sample from it. Optionally you can pass a boolean called deterministic, in which case either the mean of the gaussian will be returned, or the index of the dimension where the categorical distribution is highest.\\n\\nEnvironment\\nAs mentioned earlier, we will also create our own abstraction of an environment, on top of the Gym abstraction. We do this to make experimentation a bit easier and such that we have to write less code to try out a different environment. Later on this abstraction will also help us with implementing some more complicated algorithms that require us to overwrite some functions (for example in case we need a custom reward function).\\n\\n\\ncommon/step.py\\nWe start with a simple Step class, which represents a step in the environment. Essentially the Step class is just a data container class.\\n\\n\\ncommon/trajectory.py\\nOur Trajectory class holds an array of Steps. Besides this, it contains some helper functions to convert the values these steps contain to Tensors. A Tensor is a data object from PyTorch which will make our computations more efficient. It also contains a helper function to compute the score of a trajectory and the length.\\n\\n\\nenvironments/action_type.py\\nWe define an ActionType enum, to make a distinction between environments that require a discrete action space or a continuous action space.\\n\\n\\nenvironments/environment.py\\nThe next class is our actual environment abstraction. We define some variables like the dimensions of the state space (note: currently only vectors are supported), the dimensions of the action space and the type of actions, along with the usual Gym abstractions. We also allow the user to pass some functions as parameters. They allow a user to override the reward-function or how to determine when an episode is over.\\n\\nThe two most important functions are create_trajectory and step. The former will output a trajectory in the environment, the latter takes a single step in the environment. Both might be useful under certain circumstances.\\n\\nIf you want to use this class in tandem with one of the environments provided by OpenAI Gym, you can do so with the following class, which inherits from the Environment class.\\n\\n\\nenvironments/gym_environment.py\\nIn order to use this with one of the environments, for example the Pendulum-environment, you can do it similar to this example.\\n\\n\\nenvironments/pendulum.py\\nBaseline calculation\\nAs we have seen in the previous blogpost, we will also need to calculate the reward-to-go. I’ve made this code part of a file called “advantages.py”, since this value should in theory be similar to the advantage and we might want to reuse it later for other algorithms.\\n\\n\\ncommon/advantages.py\\nThe file also mentions a buffer, which we will later use for other algorithms, but you can safely ignore it for now. The reward-to-go calculation takes a trajectory and a discount_factor as input. You might want to tweak the discount factor depending on how (un)important future rewards are for the problem you are solving.\\n\\nLogging\\nTo make sure that we see some output and can track how our RL agent is doing, we will write a class that performs some evaluation.\\n\\n\\nloggers/model_tracker.py\\nAs you can see, we use the Tensorboard library in this class. This will allow us to track the progress of our algorithm through a nice dashboard with some graphs.\\n\\nThe crux of the logger is in the _evaluate function. Here we make use of the Environment class to create several trajectories. In order to create those trajectories, we pass in a parameter called get_action. The get_action parameter is a function that takes a state as input, and produces an action as output. This may seem very generic, and it is, but this is exactly the way we want it to allow maximum flexibility in what we want to evaluate.\\n\\nTypically the get_action parameter will be a function that uses our RL agent to determine what the next action (given the state) should be.\\n\\nReinforce algorithm\\nWe then arrive at the implementation of the actual algorithm. In this version we only sample one trajectory before every gradient update. It is definitely not wrong to sample multiple trajectories before doing a gradient update.\\n\\n\\nreinforce/reinforce.py\\nThe most important part of this file is in the train function. First we initialise our environment and logger. We then sample a trajectory, for which we calculate the log probabilities of the taken actions, as well as the reward-to-go.\\n\\nAs you can see, we use some to_tensor functions, which are converting our array outputs to PyTorch Tensors for speedy calculations. In addition we call the to_device function which allows us to do some of the calculations on our graphics card instead of the CPU. For our purposes, these are just implementation details.\\n\\nNext, we calculate the loss (target) by multiplying the log probabilities of the actions by the reward-to-go, as we have seen in the previous blog post. By default, PyTorch accumulates gradients, so we first call the zero_grad function to reset them. We then calculate the gradient of our loss with the backward function and update the weights with the step function.\\n\\nIn addition, this class contains some functions to save and load a trained model, and to sample an action from the model or sample an action deterministically.\\n\\nPutting it all together\\n\\nrun_reinforce.py\\nWe now have all the ingredients to run our algorithm. I created a small main function that instantiates our Reinforce algorithm and passes in a custom environment called EasyTraversalDiscrete. We train the model using the default number of iterations and finally run our model!\\n\\nFor completeness, I will show what the directory structure of our project will look like. As you can see the project contains some extra files that we haven’t discussed yet, for now you can see these as a teaser for the following tutorials.\\n\\n\\nConclusion\\nIf you made it all the way here, congratulations, you have implemented your first RL algorithm and are probably well equipped to do your own research to explore more methods and algorithms.\\n\\nThe last part of this tutorial was probably not so easy to follow though, I have exclusively included images such that you would also think about what we are doing in the code, instead of blindly copying the code. If you have any questions though, feel free to leave a comment or to reach out to me personally.\\n\\nLastly, the files in this project are ordered in such a way, such that they allow for extending the code to other algorithms as well. This will be it for now however, so thank you for following this first series and see you in the next one!', 'Welcome back to the third part of this introduction series to Reinforcement Learning!\\n\\n\\nIn part 2 we explained some of the basic concepts of RL. We will be using these concepts today to discuss an RL algorithm: the REINFORCE algorithm. REINFORCE is conceptually simple, but it gives you a solid basis for understanding and implementing more advanced algorithms. Specifically, REINFORCE is part of the Policy Gradient family of algorithms.\\n\\nThe blog post will be ordered like this:\\n\\nThe REINFORCE Algorithm\\nPseudo-code\\nThe Credit Assignment Problem\\nOn-policy vs. Off-policy RL\\nPolicy Gradient Baselines\\nNote: If you are reading this on a smartphone browser, you might not be able to view the subscripts. You can download the Medium app to mitigate this.\\n\\nThe REINFORCE Algorithm\\nREINFORCE is a policy gradient algorithm. Remember that the policy was a function that defines the behaviour of our agent. We will define our policy as a parametrised function:\\n\\n\\nHere, π is our policy with parameters θ. The policy takes a state sₜ as input and produces an action distribution as output (the probabilities that our agent would perform a certain action aₜ). In this case we talk about a stochastic policy, since our policy outputs probabilities instead of an action directly.\\n\\nPolicy gradient algorithms attempt to improve the policy directly, by changing the parameters of the policy function, such that the policy produces better results.\\n\\n\\nThis is also what the REINFORCE algorithm is doing. The main idea is this: We start with a random policy (so our agent will take random actions). Next, we let our agent operate in the environment according to this policy. This will produce a trajectory (a series of states and actions that the agent took). If the agent got a high reward during this trajectory, we are going to update our policy such that the trajectory our policy produced is more likely to happen next time. Vice versa, if our agent performed poorly, we are going to make the selected trajectories less likely to be selected. We will repeat this process until we (hopefully) get to a good policy.\\n\\nPseudo-code\\nThe process we just described, would look like this:\\n\\ninitialise policy P with random parameters θ\\nrepeat until agent performs well:\\n    generate trajectory τ (s₀, a₀, s₁, a₁, s₂, ...) with reward R \\n    for each state sₜ in trajectory τ:\\n        calculate reward-to-go Gₜ\\n    calculate policy gradient L(θ) using G\\n    update policy parameters θ using L(θ)\\nLet’s break this pseudo-code down into more digestable bits.\\n\\ninitialise policy P with parameters θ\\nWe will first initialise our policy P and our parameters θ.\\n\\nIn the second part of this series, we mentioned that we will represent our policy by a neural network. However, this doesn’t necessarily need to be the case, for policy gradient algorithms we can use any differentiable learning method. A differentiable learning method, in simpler terms, means that we know a way of calculating how to update the parameters of the policy. In this blog post, I will still opt for a neural network.\\n\\n\\nA neural network takes some input in the first layer and produces some output at the end. The connections (weights) between the neurons are our parameters. They influence the output by strengthening or weakening the signal produced by the previous layer. Initially these connections will be random. So our policy will initially give us random actions.\\n\\nrepeat until agent performs well:\\nThe next line in our pseudo-code we can somewhat ignore for now. It just means that we will repeat the next steps until we are happy with how our agent is performing.\\n\\ngenerate trajectory τ (s₀, a₀, s₁, a₁, s₂, ...) with reward R\\nHere we generate a trajectory τ. This means we will now let the agent interact with the environment. We will start in a starting state s₀ (determined by the environment) and then let our agent act according to our policy P with the parameters θ at that point in time. So we will take state s₀, provide it as input to our neural network (our policy) and get a distribution of actions out of it. We can then select an action a₀ by sampling from that distribution. We execute the selected action and arrive at a new state s₁. We repeat this process until we reach a terminal state, the end of the episode (determined by the environment). That is how we get trajectory τ.\\n\\n\\nIf you wonder how we can get a distribution from a neural network, then consider that many distributions are defined by just a few parameters. Take a Gaussian (normal) distribution for example, a normal distribution is defined by a mean and a standard deviation. Our network can take a state as input and produce two scalars (a mean and standard deviation) as output. From this we can construct our distribution and sample an action from it.\\n\\nNote that we don’t necessarily need to make the neural net to output both a mean and a standard deviation. It is fairly common practice to assume a certain standard deviation and let the neural network only predict a mean value.\\n\\nfor each state sₜ in trajectory τ:\\n    calculate reward-to-go Gₜ\\nLet’s look at the next two lines together, because the first line simply means we will repeat the process for every timestep of our trajectory. The line that follows introduces a new concept, the reward-to-go. The reward-to-go is the reward obtained during the trajectory from a certain state onwards.\\n\\n\\nSo for the first state of the trajectory for example, this accounts for the total reward of the trajectory. We will however also take a discount factor into account. A discount factor makes us value rewards in the future less than how we value immediate rewards. This gives us the following formula for the reward-to-go:\\n\\n\\nγ is the discount factor, which has a value between 0 and 1. Rₜ is the reward at timestep t. If we care a lot about future rewards, we could give γ a value of 0.99 for example. If we mostly care about immediate rewards, we could give it a lower value, like 0.7.\\n\\ncalculate policy gradient L(θ) using G\\nWe have arrived at our policy gradient. You might be wondering what a gradient actually is. A gradient is a mathematical way of determining how a change in the parameters affects the outcome of a function. If you happen to remember derivatives from high school, the idea is somewhat the same. Except, we will be dealing with a function with multiple variables.\\n\\n\\nWe want to know how we should update our parameters θ of our policy P, such that the trajectories which have a high reward-to-go Gₜ associated with them, are more likely to happen and vice versa.\\n\\nYou can imagine the policy, the neural network, like a big machine with a lot of knobs to turn. We want to calculate how we should turn those knobs, in order to make the good trajectories more likely to happen. The method that tells us how we should turn those knobs, is the gradient.\\n\\nLet’s say we have a function J that tells us what return (total reward of a trajectory) we can expect for a given policy:\\n\\n\\nR is the return for a trajectory τ. The formula tells us that for a policy π the expected return is equal to the return of the trajectory we expect to follow, given policy π. We want to maximize the value of J(π). So we will try to calculate the gradient of J. This brings us to the next formula:\\n\\n\\nThe ∇ symbol here stands for the gradient. It will tell us how to change the values of θ to increase the value of J. Specifically, the gradient gives us an array of values (one for each parameter in θ). You can think of these values as a direction, pointing towards the place where the outcome of function J would be higher. If we add those values to our parameters θ, the value of J would maximally increase.\\n\\nUnfortunately, we cannot do any calculus with the above formula directly. We will derive an equivalent formula which we can actually use to do our calculations. The full mathematical derivation would take us a bit too far, but if you are interested in a clear explanation on how to derive the gradient, I invite you to read this webpage by OpenAI: Deriving the simplest policy gradient\\n\\nThe calculation of our policy gradient would look like this:\\n\\n\\nThis formula tells us how to calculate the gradient of J. The right-hand side starts with an Expectation-symbol (E). The expectation represents the trajectory τ we expect to get when following policy π (the average trajectory if you will). We cannot exactly calculate which trajectory would be the expected trajectory, because this would require us to calculate all possible trajectories for our policy. Luckily, we can sample one or more trajectories and these should lie close to the expected trajectory.\\n\\nAfter sampling, we apply the formula between brackets. π(aₜ ∣ sₜ) represents the probability of action aₜ under policy π given state sₜ. We will however use the log of this probability. Taking the log will make the gradient calculation process a bit more stable as it allows us to sum the gradients for each timestep instead of having to multiply them. Multiplication might make the gradients very small or big, causing numerical instability. You can check the derivation link to see how taking the log is actually equivalent.\\n\\nFinally, we calculate the gradient for this function, giving us a direction. We multiply this direction with Gₜ, the reward-to-go. If Gₜ was high, we step in the direction of the gradient, making action aₜ more probable. If Gₜ was low, we step in the opposite direction (or we take a relatively smaller step compared to a trajectory with high Gₜ).\\n\\nThe gradient calculation itself is usually taken care of by the programming library we use (for example Tensorflow or Pytorch). If you are interested in how to do the calculation of the policy gradient by hand, both for discrete action spaces and continuous action spaces, then I recommend this excellent post that goes a little deeper into the mathematics: RL Policy Gradients explained by Jonathan Hui\\n\\nupdate policy parameters θ using L(θ)\\nWe arrive at the final line of our algorithm. In this step, we will use the gradient we have just calculated to update the weights of our neural network (the policy).\\n\\nOur gradient is a vector, containing a value for each parameter in θ. So now we will simply update our weights in the following way:\\n\\n\\nIf you are still somewhat confused about how a neural network works and how we can update these weights, I recommend this video explaining backpropagation. Backpropagation is the algorithm that allows calculating and applying these gradients.\\n\\n\\nNote that in the video the narrator talks about a loss function. Strictly speaking we are not computing the gradient of a loss function, even though the calculation is somewhat similar. We are calculating a policy gradient. A loss function (as shown in the video) is a direct indicator of how well your neural network is doing on learning a dataset. A high value for the loss function means your network is making a lot of mistakes. This does not hold true for the policy gradient, there is no indication of how successful our agent is. Another difference is that we are not trying to minimize our policy gradient, but we are trying to maximize it (gradient ascent instead of descent). An easy trick to achieve this, is by performing gradient descent on the negative of our policy gradient.\\n\\nThe Credit Assignment Problem\\nYou might be looking at the explanation above and wonder “how does REINFORCE filter out the good actions from the bad actions?”. Our agent could take a very bad action, but in the end still end up with a score that was relatively good. In such case, we would reinforce that behaviour, making the bad action more likely to happen. Vice versa, our agent might take a very good action that didn’t lead to a good score. In that case the behaviour that led to the good action would be discouraged.\\n\\nThe problem that we are describing is the so-called credit assignment problem: Our algorithm cannot determine which actions were good actions or bad actions throughout the course of a trajectory. Does this mean that REINFORCE won’t work? No. What will happen is that during the good trajectory, on average, there should be more good actions than bad actions. Vice versa for bad trajectories. So after we have gone through enough iterations, our model will start averaging out these trajectories and should eventually grow a bias towards the good actions.\\n\\n\\nA consequence of the credit assignment problem is that our algorithm is more sample-inefficient. We will need to evaluate more trajectories to start filtering out bad behaviours from good ones. There is a lot of research that tries to tackle specifically this problem. As an example, researchers have attempted to use a separate neural network that learns to infer immediate rewards from actions in trajectories. (Link to paper)\\n\\nOn-policy vs. Off-policy RL\\nAnother characteristic of the REINFORCE algorithm is that it’s an on-policy algorithm. On-policy means that the policy gets updated, based on the experience collected from that same policy. This is exactly what happens in REINFORCE. Each iteration, our agent behaves according to the latest version of the policy. Afterwards we update the policy based on what we learned from these interactions.\\n\\nThis doesn’t necessarily need to be the case. Off-policy RL algorithms may update their policy based on what they learned from previous or other policies. Take a look at the image below for an example.\\n\\n\\nIn the on-policy case the agent is constantly interacting with the environment, and as soon as new experience has been collected, the policy gets updated and the agent behaves accordingly.\\n\\nIn the off-policy case we have an agent (or multiple agents) acting according to a policy, we call this policy the behaviour-policy. The experience they collect gets saved in a buffer. We then use that collected experience to train a policy for action selection. So in this case, our buffer can contain experience from older versions of our policy, or even a completely different policy. The methodology we just described is often seen in Q-learning algorithms.\\n\\nOff-policy methods have the potential to be more sample-effective, since the agent might need to interact less with the environment. They are also less prone to get stuck in local minima. The reason for this is that in the on-policy case, the agent might have created such a strong bias towards a certain behaviour, that small deviations from this behaviour are no longer good enough to discover new (and improved) strategies. This is why more sophisticated on-policy methods often try to incorporate a mechanism that encourages the agent to do more exploration.\\n\\nIn the most extreme case of off-policy RL, we only let our agent interact with the environment once. So the experience we collected thus far is similar to a limited dataset of example interactions. This is called offline RL and it is one of the more prominent research directions for RL in the last few years. The reason for the popularity of offline RL is simply because of its practicality and applicability in the real world. Offline RL is very hard however, because the agent that collected the training data was behaving according to an unknown policy. If you want to learn more about this offline RL, I invite you to check out the incredible work done by Sergey Levine et al.: Offline RL paper\\n\\nPolicy Gradient Baselines\\nWe need to talk about one more thing before we can end this part of the blog. You see, we can improve our version of the REINFORCE algorithm in one more subtle way.\\n\\nIn our current version of the algorithm, we apply gradients in such a way that trajectories become more or less likely, proportional with the return (cumulative reward) that these trajectories gave us. How much more or less we make a trajectory is all relative. In mathematical terms, the magnitude of our gradient is completely dependent on the return of a trajectory.\\n\\n\\nIn the picture above, you can see we get three scores: 100, 0 and 100. So what our algorithm will do, is update the policy accordingly. This might sound desirable at first, but actually it is not. Because the last trajectory is similar to the first trajectory and once again we make it more likely. After a while, this might induce such a big change in behaviour that it might have an averse effect on the average outcome of the game. If we update our policy parameters too much in a specific direction, they might start to show unwanted behaviours.\\n\\nSo what can we do about this? Intuitively, we want our gradient to update not just according to the score, but we want to update it when our agent did better or worse than expected. This is something we can achieve by introducing a baseline.\\n\\n\\nThe formula shown here is exactly the same as our policy gradient, with one difference: We subtracted V(sₜ) from our reward-to-go Gₜ. Remember that the value function is an estimator for the value of being in a certain state and behaving according to a certain policy. So V is a good baselines in this case, because by subtracting V from Gₜ, we will now only update our policy “if we did better or worse than expected”. In practice we don’t know the exact value of V. So usually during the course of our algorithm we additionally train a neural network that estimates V.\\n\\nSo why do baselines actually improve the training process? Mathematically speaking, adding an appropriate baselines will reduce the variance of our samples. In simple terms, our policy will have a clearer learning signal, which will make the training faster and more stable. We will talk more about baselines in a later post.\\n\\nConclusion\\nWe have once again explored a lot of topics in today’s post. Congratulations on making it this far! You should now have a solid understanding of basic policy gradient algorithms. Additionally, this post will prepare you to get your hands dirty in our next post. If this post seemed very theoretical to you, or you learn more quickly through code, then stick around for the final post of this intro series!', 'Welcome back to part 2 of the intro to reinforcement learning series!\\n\\n\\nIn part 1 we described what reinforcement learning (RL) is, what you can do with it and some of the challenges of RL. This time around, we will explain some of the main concepts of RL and introduce some terminology and notation. This will be useful for the next part, where we discuss a simple RL algorithm.\\n\\nSpecifically we will talk about:\\n\\nStates\\nActions\\nReward functions\\nEnvironment dynamics (Markov Decision Processes)\\nPolicies\\nTrajectories and return\\nValue function & Q-function\\nAs always, if you’re already familiar with the content, feel free to head on to the next part.\\n\\nNote: If you are reading this on a smartphone browser, you might not be able to view the subscripts. You can download the Medium app to mitigate this.\\n\\nThe RL framework\\nLet’s start with a quick recap. The RL framework involves an agent that tries to solve a task in a certain environment. At every timestep t, the agent needs to choose an action a. After this action it might receive a reward r and we get a new observation of its state s. The new state can be determined both by the action of the agent and also by the environment the agent is operating in. In RL we want to maximize the future accumulated reward.\\n\\nFor the sake of explanation, we will use the game of Pong to further explain these concepts.\\n\\n\\nPong is an easy game to explain: There are two players, represented by the green and the red paddles in the image above. In our example, we will consider the player who operates the green paddle to be our agent. The goal of Pong is to bounce back the ball, past the other players’ paddle. When either player manages to bounce the ball past the other player, they get a point and the ball starts moving from the centre of the field again. The player with the highest score wins.\\n\\nHere is a 1 hour long gameplay video if you are still confused.\\n\\nHow do the different terms of the RL framework apply when we want to train an RL agent to play Pong?\\n\\nStates\\nLet’s start with the state. A state is usually a vector, matrix or other tensor. If you are not familiar with these terms, don’t worry, they are essentially various ways of organising numbers. You can check the image below as an example.\\n\\n\\nThe state should describe the relevant information we need in order to decide which action to take at timestep t. We will denote the state at timestep t as sₜ .\\n\\nIn the case of Pong, a good description of the state can be a vector (an array of numbers) containing the position of the players’ paddle, the position of the ball and the angular velocities of the ball. This would be enough for our agent to decide whether it’s best to move the paddle up or down. Because from this information it can check its own position, the position of the ball and where the ball is moving towards.\\n\\nNote that it wouldn’t suffice to only encode the position of the ball and the player, because we also need some information that indicates in which direction the ball is moving.\\n\\nAnother option would be to directly use the frames/pixels of the game to represent the state. The state is then a matrix or tensor representing the light intensities/colours of the pixels. This will be harder for the AI algorithm to learn, but it is definitely possible. It will be harder, because now it not only needs to learn how to play the game, but it also needs to learn how to extract the relevant information from these pixels. It will need to learn that for example the blue pixels in the middle represent a ball and it will need to learn that the location of this ball is relevant in solving this problem.\\n\\nNote: We also encounter the same problem as the one we encountered when we used a state vector, namely: one static image is not enough to show in which direction the ball is moving. As a solution, the state is often represented as a stack of the last few frames.\\n\\n\\nAs a final remark, even though we are using the term “state”, the term “observation” is more appropriate. The difference is that a state is assumed to give a full description of the state of the world, while the term observation refers to a (possibly partial) observation of the state. For our purposes, we will use these terms interchangeably.\\n\\nActions\\nIn Pong we want our agent to be able to move the paddle up and down.\\n\\n\\nThere are various ways of representing this action, but first there is an important distinction to make about actions in RL in general. We will make a distinction between discrete action spaces and continuous action spaces.\\n\\nIn the case of a discrete action space, our actions are discrete, this means that they can take on only a certain set of values. For example in the game of Pong, we could define our actions to be “Up”, “Down” and “No move”. This means that our paddle can move up, down or stand still at every timestep, but only by a predefined amount. Moving up could mean that we move up by 10 pixels every time for example.\\n\\n\\nTo represent these three actions, we could use a vector (row of numbers), where each dimension (column) of the vector indicates which action to take. Normally the values in the vector are still continuous and represent “a categorical distribution” of which action the agent should take. You can disregard this for now and just look at this as “the dimension in the vector where the number is 1, is the action our agent will take”.\\n\\n\\nIn the case of a continuous action space, our actions are continuous, meaning that they can take on any value. So if we look at the game Pong, now our action might not just represent the paddle moving up our down, but we might also specify by how much the paddle should move up or down. In this case, a vector with a single dimension (a scalar) will suffice. The scalar can have a positive value for moving up, 0 for not moving and a negative value for moving down.\\n\\nReward function\\nThe reward function tells us after every action how much reward the agent got for taking that action. Or more precise: the reward function takes a state Sₜ and an action Aₜ as input at every timestep t, and outputs a reward Rₜ . The reward Rₜ is a scalar (number) that indicates how well the agent did.\\n\\n\\nIn Pong, we could opt for giving a reward of 1 every time the agent manages to make the ball go past the other players’ paddle, 0 when nobody scores, and -1 when he misses the ball causing the opponent to score.\\n\\nFormally, the reward function has the following signature:\\n\\n\\nWhere S represents the set of states, A the set of actions and ℝ the set of real numbers.\\n\\nYou should be careful when choosing how you define the reward the agent receives, as this can highly affect its behaviour. For example, it can produce unforeseen consequences or if your agent sees too little rewards (sparse reward problem), it might fail to learn anything at all.\\n\\nEnvironment dynamics\\nThe environment dynamics answer the following question: given a state and an action, what will be the next state? Typically, we don’t need to model these environment dynamics ourselves. Our RL algorithm will need to figure them out on its own. Even more so, for many problems the environment dynamics are unknown.\\n\\nWe will briefly talk about the idea of environment dynamics nonetheless, because the only requirement for us to be able to apply RL to a problem, is that we are in theory able to define the problem as a Markov Decision Process (MDP).\\n\\nA Markov-what you ask? You will see this term come up in a lot of the literature around RL and the reason for this is that MDP’s are a way of formalising problem statements. They will also help us explain some of the algorithms we will use later on. Let’s talk about MDP’s for a moment.\\n\\nMarkov Decision Process\\nAn MDP is a discrete-time stochastic control process that has the Markov property. This is a mouth-full, but it’s not so complicated. Let’s break it down.\\n\\nWe are describing a process in which we have a set of states. These are the states that our agent can be in during the process of solving a problem. The process is modelled in discrete time, which roughly means that everything that happens is represented as a separate point in time. In other words, you cannot be “in between” two states, either you are in a state or you are not. A point in time is then represented by a number and the next point in time would be that number +1.\\n\\nWe also define a transition distribution:\\n\\n\\nA transition means we are going from one state to another. So this formula tells us the probability of going to a next state sₜ₊₁, given our current state sₜ and taking an action aₜ. That is the “stochastic” part of our definition, stochastic simply means that we are dealing with probabilities. So sometimes our agent might decide to take an action, but because of the environment, there might not be a guarantee that he ends up in the state the agent intended.\\n\\nLast but not least, our control process needs to have the Markov property. Markov refers to its inventor, Andrej Markov. The property tells us that any next state only depends on the current state. When a process has the Markov property, the next state does not depend on any other states except for the current one. So our agent might have seen a million states before, but still it won’t matter to determine the next state, only the current state does.\\n\\nThe diagram below shows an example of such a process. The circles containing S₀-S₂ represent states, while the pink circles represent actions that can be taken in those states. S₀ is green, because it represents a starting state. Most processes also have terminal states (states that mark the end of the process), but they are not drawn here.\\n\\n\\nIf you are in state S₂ at timestep t for example, you can take actions A₀ or A₁. If you opt for action A₀, then you have a 26% chance of ending up in state S₀ at time t+1, while you have a 74% chance of ending back up in state S₂ at t+1. Notice that it doesn’t matter which states we have previously visited to determine our next state.\\n\\nGreat, that’s all there’s to it, now you know what an MDP is! Now what would an MDP look like for the game Pong? We have already discussed what the states and actions of our agent can be. The states describe the positions of the paddles and the ball, as well as the speed of the ball. The actions we can take are move up or down.\\n\\nIn the case of Pong we are dealing with an environment that is fully observed, we have access to all information in this game. This is not always true, sometimes our environment can only be partially observed. Think for example of a game of cards, where you cannot see the cards your opponent is holding.\\n\\n\\nOur environment, as we currently described it, is still stochastic however. The reason for this is mostly due to our opponent. Say that our paddle is at position X1 while our opponent is at position X2. Even though we decide how our paddle will move and the physics of the moving ball might be deterministic, our opponents’ paddle might move randomly or probabilistically.\\n\\nPolicies\\nAlright, we have a lot of terms to cover in today’s post, next up are policies. When we talk about a policy, we are referring to the “behaviour” of our agent. Given a state, what should be the next action the agent takes? This is the crux of our algorithm.\\n\\nSimilar to what we did for actions, we will distinguish between deterministic policies and stochastic policies. We can write a deterministic policy as:\\n\\n\\nIn this formula, aₜ is an action at timestep t, sₜ is the state at timestep t, and μ is our policy function with parameters θ. You can simply look at this formula as a mathematical function that receives some input (the state) and then outputs a corresponding action for the agent to take. The parameters θ influence the output of the function, so we want to change these such that our agent behaves in a way that optimally solves the problem. We will see in the next blog post how we do that, don’t worry about them for now.\\n\\nA stochastic policy on the other hand, would look like this:\\n\\n\\nThis formula is very similar to the deterministic policy. aₜ is an action at timestep t, sₜ is the state at timestep t, and π is our policy function with parameters θ. It is a convention to use π for stochastic policies. The difference with the deterministic policy, is that we select action aₜ with a certain probability, but it could very well be the case that the next time we are in a similar state, we select a different action instead.\\n\\nNeural networks\\nYou might wonder, these policies look nice, but what function do we use to model them?\\n\\nIn practice, we will often use a neural network (NN)for these functions. Remember, we are trying to create a self-learning system after all. We will very briefly explain what a neural network is and how it works.\\n\\n\\nA neural network (NN) is a learning structure loosely based on the human brain. It consists of neurons (the cyan dots in the picture) and synapses (the blue wires between the blue dots). When you look at the NN, you can see that it has various “columns” of blue dots, we will call those layers. The first layer (blue dots with green edge) we call the “input layer”. The final layer (a single neuron with blue edge) we call the output layer.\\n\\nA NN works by taking some numbers as input, starting at the -you guessed it- input layer. This input is similar to an electrical signal arriving at the brain, and passing through it. The synapses (connections) reinforce or weaken the signal. Then, the signal from various neurons gets combined, right before it arrives at a new neuron in the next layer. If the signal is strong enough, the neuron will “fire” and propagate the signal further, if it is not strong enough, it will not propagate the signal. This process continues, until the signal arrives at the output layer and thus gives us an output. So in our case, the state will be the input to the network, and the number (or numbers) it outputs will be the action of our agent.\\n\\nInitially the NN is wired randomly, so the signal will also be strengthened and weakened randomly, and thus produce a random output/action. The trick is now for us to change the connections in such a way that the neural network produces an output that is closer to the one we were expecting. You can look at the NN as a big machine with a lot of knobs, and now we need to figure out how to turn the knobs until it produces satisfactory output. We can do this using mathematics, through an algorithm called backpropagation. But we will omit details for now.\\n\\nRemember the parameters we were referring to when talking about policies? In the case of a neural network, they are in fact referring to these connections! Changing the connections is our way of changing the output the policy produces.\\n\\nWe have covered a lot of ground already and we are nearly ready to complete this article full of definitions and notations. I promise you, they will all be useful later on when we get to the actual coding of these algorithms!\\n\\nTrajectories and returns\\nTrajectories are easy to understand once you know about states and actions. A trajectory is simply referring to a certain sequence of states and actions:\\n\\n\\n\\nAt the end of a trajectory, our agent will have accumulated a certain amount of rewards. This accumulated reward is what we call the return.\\n\\nWe distinguish between finite-horizon and infinite-horizon returns. A finite horizon means that our trajectory will consist only of a limited amount of timesteps, while an infinite-horizon means we will calculate the return for infinite timesteps.\\n\\n\\nThis is the formula for the finite-horizon return. We have a trajectory τ for which we calculate the return R(τ). In the formula, we take the sum of every rₜ for timestep 0 until horizon T.\\n\\n\\nThe formula for the infinite-horizon return is quite similar, except we are now summing over a theoretical infinite number of timesteps and we introduced a new variable γ. The γ (gamma) is the discount factor, its value is always between 0 and 1 (inclusive). The lower the discount factor, the less valuable future rewards will be.\\n\\nThe discount factor is both mathematically convenient and it has an intuitive interpretation as well. It is convenient because for problems with infinite timesteps, we know that it will converge to a finite value. On the other hand, it is also intuitive to think that it’s usually better to get rewards rather sooner than later. It is for example better to get cash now than later, because due to inflation the cash might be worth less later.\\n\\nValue function and Q-function\\nIn RL we are looking to find a policy (a behaviour) for our agent such that it will receive the highest possible cumulative reward. In order to discover this policy, there are some other useful functions we can define, like the Value function. The value function looks like this:\\n\\n\\nYou can read this formula as: “The value of being in state s, while acting under policy π, is equal to the reward we get of the trajectory τ that we expect to follow from policy π, given that state s is the starting state.” Or in short, it roughly tells us how good it is to be in a certain state. The “E”-symbol in this formula stands for “expectation”. It is the trajectory we expect to get from policy π, the average trajectory if you will.\\n\\n\\nAs an example, have a look at the above image. In the top image, our agent is very far from the ball, while in the bottom image, our agent is pretty close to the ball. If our agent has a policy that makes him move towards the ball, then it is highly likely that the value of state Y is higher than the value of state X. The reason is that in state X, our agent might not be able to make it to the ball in time, thus losing him the point.\\n\\nAnother function that turns out to be very useful is the Q-function. It looks like this:\\n\\n\\nAs you can tell, it looks rather similar to the value function, and it is in fact similar, with one subtle difference. The function says :“The value of being in state s and taking action a, while acting under policy π, is equal to the reward we get of the trajectory τ that we expect to follow from policy π, given that state s is the starting state and the first action we take is a.” Or in short, it tells us how good is it to be in a state and to take a certain action.\\n\\n\\nBoth of these functions are often used in RL algorithms. They have whole families of algorithms associated with them. For example, Q-learning algorithms are based on exploiting the fact that if you know the Q-value for all possible actions in a state, then you can decide which action is best to take. You will get an optimal policy simply by picking the action that has the highest value for the Q-function.\\n\\nUnfortunately, in most cases we can’t know the Value function or Q-function exactly, so in practice we will estimate/learn them using learning algorithms (like a neural network).\\n\\nConclusion\\nThis was a long post full of terminology, but you made it! Having talked about all these concepts, will give you a solid basis to start implementing your own algorithm. In the next part we will talk about the REINFORCE algorithm. The REINFORCE algorithm is conceptually simple, but lies at the heart of the policy gradient-family of algorithms. Thank you for reading and see you in the next part!', 'Hi and welcome to the first part of a series on Reinforcement Learning.\\n\\n\\nIf you somehow ended up here without having heard of Reinforcement Learning (RL) before, then let me summarize it as follows: “RL is a general framework for training an artificial intelligence model to solve a certain task or goal” … or in layman’s terms, we make AI do cool things!\\n\\nThe goal of this blog series is to learn about RL and simultaneously explore some of the more recent research later on. We will start from the very basics and work our way towards more advanced topics. Even if you have almost no prior programming and/or mathematics knowledge, you should be able to follow along pretty smoothly.\\n\\nThe first mini-series will be split into four parts:\\n\\nPart 1: What is Reinforcement learning?\\nPart 2: RL terminology and formal concepts\\nPart 3: The REINFORCE algorithm\\nPart 4: Implementing the REINFORCE algorithm\\nAt the same time, this mini-series will be the introduction to future posts with increasing complexity. Feel free to skip to the next part if you are already familiar with the content.\\n\\nNote: If you are reading this on a smartphone browser, you might not be able to view the subscripts. You can download the Medium app to mitigate this.\\n\\nThat sounds cool! … but what can I do with RL?\\nReinforcement learning is a framework to learn any task. In theory, RL can solve any problem that is phrased as a Markov Decision Process. We will explain what that means later on. For now, let’s have a look at some successful applications. If you enjoy these examples, please be sure to also check out the work from the original authors.\\n\\nGeneral\\nOne of the videos I love showing is the hide and seek video from OpenAI. The video is a nice example of how RL can help us with finding novel solutions to problems, without explicitly programming tactics or solution methods.\\n\\n\\nGames\\nOne of the early successes of Deep RL (which is combining RL with neural networks), was the ability to learn how to play Atari games, straight from pixels. Later on, researchers took it upon themselves to not only evaluate RL on the (relatively) simple Atari games, but rather evaluate it on the hardest competitive games out there. The assumption here is, that if RL can solve these complex games, it can also generalize to challenging real-world settings. As an example, this is Deepmind’s AlphaStar taking on a pro-gamer in the game StarCraft 2.\\n\\n\\nRobotics\\nSolving tasks in simulations and video games is one thing, but what about real life? Another popular field where RL is often applied (or at least holds great promise), is robotics. Robotics are significantly harder than simulations for various reasons. Think for example about the time it takes to repeatedly make a robot try out a certain action. Or think about the safety requirements involved for robotics. In the example below, you can see how the ANYmal robot from the Robotics System Lab in Zürich learned to recover from a fall.\\n\\n\\nReal world examples\\nRL can be applied to many other domains than the ones I just mentioned. Advertising, finance, healthcare, … just to name a few. As a final example, I present a goal-oriented chatbot, trained to negotiate about sales (source: https://siddharthverma314.github.io/research/chai-acl-2022/ ).\\n\\n\\nRL: The basics\\nA description of the RL framework is as follows: We have an agent that tries to solve a task in a certain environment. The concept of agent should be taken very broadly here, an agent can be a robot, a chatbot, a virtual character, etc. . At every timestep t, the agent needs to choose an action a. After this action it might receive a reward r and we get a new observation of its state s. The new state can be determined both by the action of the agent and also by the environment the agent is operating in.\\n\\nThe RL problem is trying to maximize the cumulative reward the agent gets over time.\\n\\n\\nImagine our agent is a monkey and the task we want the monkey to solve, is to pick up as many bananas as possible. At every timestep, the monkey needs to decide to take an action. The actions could be to step towards the tree, grab something, climb, … Perhaps the reward at every timestep can be defined as the number of bananas the monkey got at that timestep. After every action, the monkey will also be in a new state. Maybe we define the state of the monkey as its position in the world. So when the monkey takes a step, the state at the next timestep would be the coordinates of the monkey at the next timestep. We are now searching for the optimal behavior, the best sequence of actions the monkey can take, to maximize the cumulative number of bananas it will get.\\n\\nHow does RL fit in the bigger picture?\\nYou might look at this framework and think: “Hey, isn’t that exactly what people are already studying in the field of …?”. And in fact, you might be right.\\n\\nIn other domains like engineering or mathematics, people have often been studying the same problems with different names and methods. Or in fields like neuroscience and psychology, some similarities can be found in the way our brain “rewards” us by releasing dopamine.\\n\\nThe likely reason for this intersection of domains, is that reinforcement learning is the study of a fundamental problem. It is essentially the science of decision taking. In these series, we will be looking at it from the umbrella of computer science and machine learning.\\n\\n\\nThis general applicability is also what makes RL so interesting to me personally. RL is one of the potential technologies that could get us closer to general AI: an AI system that can solve any task, in contrast to a narrow set of tasks. There are also other technologies (e.g. big language models, graph neural networks, …) that are making some strides in this regard, but the problem statement of RL in particular seems to be the most ambitious.\\n\\nAnother way of situating RL, is by looking at how it compares to other learning paradigms. Within the field of machine learning, people often distinguish between Supervised learning, Unsupervised learning and Reinforcement learning.\\n\\n\\nWhen it comes to Supervised learning, we are essentially trying to learn a function, a mapping from X to Y. Our dataset consists of samples X and during training we provide the AI system with labels Y of what these samples should map to. Our AI model is successful when it correctly predicts a label y given an (unseen) sample x.\\n\\nAs an example, say we have a dataset consisting of cat and dog images. Each sample (image) has a label, stating whether the image contains a “dog” or a “cat”. The goal of our supervised model, is now to learn how to map a dog-image to the label “dog” and a cat-image to the label “cat”. After it has learned this mapping, the hope is that this AI model can repeat this process for new images that it has not seen before.\\n\\nIn an Unsupervised learning context, we no longer provide any labels Y. So the task for the AI system now becomes learning some general statistics about the dataset. We could for example give an AI the task to generate a new sample, similar to the ones seen in the dataset. In this case the model would be considered successful if it manages to correctly learn the interesting characteristics of a dataset.\\n\\nReinforcement Learning is rather different from the previous paradigms. In the case of RL, we consider an agent that is actively interacting with an environment. Through its interactions, it is possible that it may influence the environment that it operates in. The “dataset” we need to consider here, are the actions our agent took and the accumulated rewards it got by taking those actions. An added difficulty here is that our dataset is non-static. Say that our agent acts in a certain way, we can then collect some data of the actions our agent took and we can try to optimize these (e.g. do more of the actions that led to a successful result). But as a result of this optimization, we have now changed the behavior of this agent, and thus we will need to collect new data to see how well our agent fares now.\\n\\nIf RL is so great, then why isn’t everyone using RL?\\nAfter reading all this, you might be wondering why people aren’t using RL to solve all imaginable problems. The truth is that even though the field has made a lot of advancements in the last few years, there are still a few fundamental problems to be solved. Progress is being made all the time, but to give you an idea of what you might encounter, I’ll list a few common ones.\\n\\nSample (in-)efficiency\\nIt is generally known that RL is very sample-inefficient. We regard a “sample” as an interaction with the environment. RL needs a lot of samples/interactions to be able to solve a task. In this sense, RL is very inefficient compared to humans, for example, it doesn’t take a human dozens of hours to learn how to play an Atari game.\\n\\nThis sample-efficiency can in part be explained by the fact that humans can leverage a lot of their previous knowledge (priors) when they encounter a new task. A human can for example reuse some of the knowledge and skills of previous games and/or concepts they already acquired from other experiences throughout their life. An RL-agent in contrast, starts the learning process without any assumptions.\\n\\nAnother thing to mention is that leveraging knowledge from previous tasks is also an active research topic. I’ll just put one example (out of many) here to give you an idea.\\n\\nDeepmind Gato\\nGoogle Jump-Start RL\\n\\nThe exploration-exploitation trade-off\\nWhile the previous problem sounds more like an engineering effort (it’s not), the exploration-exploitation trade-off seems more fundamental. Whenever we train an RL-agent, the agent will need some time to explore, it needs to take some actions that it hasn’t taken before, in order to discover how to solve the problem. On the other hand, we can’t let the agent always take random actions, because these random actions might lead to nothing. Sometimes we want the agent to leverage what it has already learned to try and optimize further. This is the exploration-exploitation trade-off, we want an automated way to strike a good balance between letting the agent explore and taking actions for which it already knows what they will lead to.\\n\\nFor a lot problems, it is quite possible that the agent gets stuck in a local optimum.\\n\\n\\nThe exploration-exploitation trade-off sounds very much tractable at first, but it turns out to be one of the hardest problems for RL to solve. To give you an idea about how hard this problem is: The problem was originally considered by the scientists of the allied forces, but was suggested to be dropped over to Germany, because it was deemed so intractable that they wanted the German scientists to also waste their time on it.\\n\\nNo silver bullet for this problem has been found and there are many people working on various solutions. I will leave a link here to a previously proposed solution called “curiosity-driven exploration” which I found particularly interesting.\\n\\nCuriosity-driven exploration by Self-supervised Prediction\\n\\nThe sparse-reward problem\\nAnother rather fundamental problem, is the so called Sparse-reward problem. As the name implies, this problem occurs when our RL-agent receives so little rewards, that it actually gets no feedback on how it should improve.\\n\\nImagine for example this mountain car. The agent needs to move the car left and right, such that it gets enough momentum to reach the top. Initially though, the agent doesn’t know that it needs to move the car back and forth to reach the top. If we only give our agent a reward (a positive feedback signal) when we have reached the flag, it might not ever get a positive feedback signal, simply because it might never reach the flag by taking random actions (exploration).\\n\\n\\nSomething commonly done to counteract this problem, is by “reward shaping”. We will modify the reward such that the agent gets more feedback signals to learn from. In case of the mountain car, we could for example also give the agent a positive reward based on the speed or altitude it achieves. However, reward shaping is not a scalable solution. Luckily other solutions are being sought after.\\n\\nI’m leaving another example here which I think is an interesting (but not generally applicable approach) for counteracting sparse rewards.\\n\\nHindsight Experience Replay\\n\\nThe aforementioned problems are just some prominent examples, but there are in fact many more problems that are being looked into by some of the brightest minds out there. The good news is that regular and steady progress is being made.\\n\\nConclusion\\nPhew, that was a lot of information in a very short period, but you made it through! This first introduction should give you a good idea of what RL is all about.'], 'data': None, 'uris': None}\n",
      "\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(col_list)\n",
    "print(f'\\n{col_num}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
